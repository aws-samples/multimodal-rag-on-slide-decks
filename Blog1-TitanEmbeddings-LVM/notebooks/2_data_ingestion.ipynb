{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbb582e-d21a-41cf-b45e-e2f462424ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data ingestion\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we download the images corresponding to the slide deck that we uploaded into Amazon S3 in the [1_data_prep.ipynb](./1_data_prep) notebook, convert them into embeddings and then ingest these embeddings into a vector database i.e. [Amazon OpenSearch Service Serverless](https://aws.amazon.com/opensearch-service/features/serverless/).\n",
    "\n",
    "1. We use the [Amazon Titan Multiodal Embeddings](https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-titan-multimodal-embeddings-model-bedrock/) model to convert the images into embeddings.\n",
    "\n",
    "1. The embeddings are then ingested into OpenSearch Service Serverless using the [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) pipeline. The embeddings are uploaded into an S3 bucket and that triggers the OpenSearch Ingestion pipeline which ingests the data into an OpenSearch Serverless index.\n",
    "\n",
    "1. The OpenSearch Service Serverless Collection is created via the AWS CloudFormation stack for this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a62857-6a66-44db-9d92-3df221c6bd21",
   "metadata": {},
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f869e5d-8e4b-4d44-9e2a-4f20b77b92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda5d30-06f7-44a6-9b1d-8ac27fb93cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import codecs\n",
    "import base64\n",
    "import logging\n",
    "import botocore\n",
    "import numpy as np\n",
    "import globals as g\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import upload_to_s3, get_cfn_outputs, download_image_files_from_s3\n",
    "\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9e4e12-1803-419d-ba38-af9d88b025d8",
   "metadata": {},
   "source": [
    "## Step 2. Download the images files from S3 and convert to Base64 \n",
    "\n",
    "Now we download the image files from the S3 bucket. Once downloaded these files are converted into [Base64](https://en.wikipedia.org/wiki/Base64) encoding so that we can create embeddings from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ccdb0-8e2e-47bc-bd90-34ff8cba1964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download images from S3, we would be converting these to embeddings\n",
    "image_files: List = download_image_files_from_s3(g.BUCKET_NAME, g.BUCKET_IMG_PREFIX, g.IMAGE_DIR, g.IMAGE_FILE_EXTN)\n",
    "logger.info(f\"downloaded {len(image_files)} from s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bbfe98-f792-447c-9878-018b2d3ea3c6",
   "metadata": {},
   "source": [
    "Convert jpg files into Base64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ee6d6-0d23-4481-ac3e-d72a2219bc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_file_path: str) -> str:\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        b64_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        b64_image_path = os.path.join(g.B64_ENCODED_IMAGES_DIR, f\"{Path(image_file_path).stem}.b64\")\n",
    "        with open(b64_image_path, \"wb\") as b64_image_file:\n",
    "            b64_image_file.write(bytes(b64_image, 'utf-8'))\n",
    "    return b64_image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4478ad-9673-4ab5-993e-f0437b3926ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(g.B64_ENCODED_IMAGES_DIR, exist_ok=True)\n",
    "file_list: List = glob.glob(os.path.join(g.IMAGE_DIR, f\"*{g.IMAGE_FILE_EXTN}\"))\n",
    "logger.info(f\"there are {len(file_list)} files in the {g.IMAGE_DIR} directory for conversion to base64\")\n",
    "\n",
    "# convert each file to base64 and store the base64 in a new file\n",
    "b64_image_file_list = list(map(encode_image_to_base64, file_list))\n",
    "logger.info(f\"base64 conversion done, there are {len(b64_image_file_list)} base64 encoded files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a2add-dfda-430a-b93a-c3bf44c8c85b",
   "metadata": {},
   "source": [
    "## Step 3. Get embeddings for the base64 encoded images\n",
    "\n",
    "Now we are ready to use Amazon Bedrock via the Amazon Titan Multimodal Embeddings model to conver the base64 version of the images into embeddings. We store these embeddings into a single JSON file which is then uploaded into S3. \n",
    "\n",
    "It is important to note that the embeddings corresponding to all the images are stored in a single file so that they can be ingested into the vector database in a single PUT operation (one bulk ingest call is more effecient than one ingest call for each image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38cd73-0bce-4313-bc2b-d9cc33a3bb79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_multimodal_embeddings(bedrock: botocore.client, image: str) -> np.ndarray:\n",
    "    body = json.dumps(dict(inputImage=image))\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, modelId=g.FMC_MODEL_ID, accept=g.ACCEPT_ENCODING, contentType=g.CONTENT_ENCODING\n",
    "        )\n",
    "        response_body = json.loads(response.get(\"body\").read())\n",
    "        embeddings = np.array([response_body.get(\"embedding\")]).astype(np.float32)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception while image(truncated)={image[:10]}, exception={e}\")\n",
    "        embeddings = None\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a99c9-2aae-4062-9c22-90e2609910a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_list = []\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=g.AWS_REGION, endpoint_url=g.FMC_URL)\n",
    "file_list: List = glob.glob(os.path.join(g.B64_ENCODED_IMAGES_DIR, \"*.b64\"))\n",
    "logger.info(f\"there are {len(file_list)} to convert to embeddings\")\n",
    "for image_file_path in file_list:\n",
    "    logger.info(f\"going to convert {image_file_path} into embeddings\")\n",
    "    \n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        input_image_b64 = image_file.read().decode('utf-8')\n",
    "    \n",
    "    # make a call to Bedrock to get the embeddings corresponding to\n",
    "    # this image's base64 data\n",
    "    st = time.perf_counter()\n",
    "    embeddings = get_multimodal_embeddings(bedrock, input_image_b64)\n",
    "    if embeddings is None:\n",
    "        logger.error(f\"error creating multimodal embeddings for {os.path.basename(image_file_path)}\")\n",
    "        continue\n",
    "    latency = time.perf_counter() - st\n",
    "    logger.info(f\"successfully converted {image_file_path} to embeddings in {latency:.2f} seconds\")\n",
    "    \n",
    "    # convert the data we want to ingest for this image into a JSON, this include the metadata as well\n",
    "    # the metadata can be used later as part of hybrid search from the vector db\n",
    "    data = {\n",
    "        \"image_path\": f\"s3://{g.BUCKET_NAME}/{g.BUCKET_IMG_PREFIX}/{Path(image_file_path).stem}{g.IMAGE_FILE_EXTN}\",\n",
    "        \"metadata\": {\n",
    "          \"slide_filename\": g.SLIDE_DECK,\n",
    "          \"model_id\": g.FMC_MODEL_ID,\n",
    "          \"slide_description\": \"\"\n",
    "        },\n",
    "        \"vector_embedding\": embeddings[0].tolist()\n",
    "      }\n",
    "    \n",
    "    embeddings_list.append(data)\n",
    "    logger.info(f\"appended json data corresponding to {image_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1b54b-7306-436d-8c12-21e0a92fc3b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fpath: str = f\"{Path(g.SLIDE_DECK).stem}.json\"\n",
    "json.dump(embeddings_list, codecs.open(fpath, 'w', encoding='utf-8'), \n",
    "          separators=(',', ':'), \n",
    "          sort_keys=True, \n",
    "          indent=4)\n",
    "logger.info(f\"saved multimodal embeddings for all images in {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b57d2df-d685-43da-a1b4-4212ec4772b2",
   "metadata": {},
   "source": [
    "## Step 4. Create the OpenSearch Service Serverless index\n",
    "\n",
    "**This step is only required until we get support creating an OpenSearch Service Serverless index via AWS CloudFormation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0acd94-ad70-4d98-95ca-fb7ea5eaa05b",
   "metadata": {},
   "source": [
    "Get the name of the OpenSearch Service Serverless collection endpoint and index name from the CloudFormation stack outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8ed38-2512-428f-a53c-b666466d2e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = get_cfn_outputs(g.CFN_STACK_NAME)\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "index_name = outputs['OpenSearchIndexName']\n",
    "logger.info(f\"opensearchhost={host}, index={index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953454d9-d335-4a2a-bbcb-852ea95a1de0",
   "metadata": {},
   "source": [
    "We use the OpenSearch client to create an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b2b2e-3c41-4ccd-8260-eb7caf247d05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, g.AWS_REGION, g.OS_SERVICE)\n",
    "\n",
    "os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56121d-441e-4d08-8375-78380ae380e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "The structure of the index is important. Note the following about the index.\n",
    "\n",
    "1. The index is a (k-NN](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html) index so that we can do a vector similarity search in this index.\n",
    "\n",
    "1. The vector dimension is 1024 which corresponds to the output dimension of the Amazon Titan Multimodal Embeddings model that we are using.\n",
    "\n",
    "1. The index uses the [`Hierarchical Navigable Small World (HNSW)`](https://aws.amazon.com/blogs/big-data/choose-the-k-nn-algorithm-for-your-billion-scale-use-case-with-opensearch/) algorithm for similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d77228-8db8-4302-a3f9-77cba53a7061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_body = \"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"image_path\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "       \"metadata\": { \n",
    "        \"properties\" :\n",
    "          {\n",
    "            \"slide_filename\" : {\n",
    "              \"type\" : \"text\"\n",
    "            },\n",
    "            \"model_id\" : {\n",
    "              \"type\" : \"text\"\n",
    "            },\n",
    "            \"slide_description\":{\n",
    "              \"type\": \"text\"\n",
    "            }\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# We would get an index already exists exception if the index already exists, and that is fine.\n",
    "index_body = json.loads(index_body)\n",
    "try:\n",
    "    response = os_client.indices.create(index_name, body=index_body)\n",
    "    logger.info(f\"response received for the create index -> {response}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"error in creating index={index_name}, exception={e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27818a-8b7c-4cad-8657-cec343f8c815",
   "metadata": {},
   "source": [
    "## Step 5. Upload the embeddings file to S3\n",
    "\n",
    "Now we are all set for ingesting the embeddings file that contains multimodal embeddings for all the slides in our slide deck into OpenSearch Service Serverless.\n",
    "\n",
    "We do this by simply uploading the file in the designated S3 bucket (see CloudFormation template [`template.yaml`](../template.yaml)) and that triggers a run of the OpenSearch Ingestion pipeline which ultimately ingests the data into the OpenSearch Service Serverless index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b3a43-50e1-4569-96b2-fd2bcfaa57fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload_to_s3(f\"{Path(g.SLIDE_DECK).stem}.json\", g.BUCKET_NAME, g.BUCKET_EMB_PREFIX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
