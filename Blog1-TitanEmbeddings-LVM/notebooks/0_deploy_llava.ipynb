{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78017671-e161-41cc-b662-b16a13013ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy LLaVA-v1.5-7B model on Amazon SageMaker\n",
    "\n",
    "***This notebook works best with the `conda_python3` kernel on a `ml.t3.large` machine***.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we download the [LLaVA-v1.5-7B](https://huggingface.co/anymodality/llava-v1.5-7b) and deploy it on SageMaker. We use the `huggingface-pytorch-inference` container and deploy this model on a `ml.g5.xlarge` instance type. \n",
    "\n",
    "The downloaded model files are archived into a `model.tar.gz` file that is uploaded to the default SageMaker S3 bucket. The `inference.py` file is overwritten with a [`llava_inference.py`](./llava_inference.py) file that has code to run inference on an image stored in S3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c5a2e-9856-4006-b424-5137f634709f",
   "metadata": {},
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2a533-1e0d-4aa6-a455-a7e9e0dcf1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340b289-30e4-466d-a36c-4a604cf4c063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import sagemaker\n",
    "import globals as g\n",
    "import requests as req\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "from sagemaker.s3 import S3Uploader\n",
    "from sagemaker import get_execution_role\n",
    "from huggingface_hub import snapshot_download\n",
    "from sagemaker.huggingface.model import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e19b6-b40e-4046-ad16-c04fef30f203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c3e5f-e7a0-40bb-af06-4724da6a47a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5b378-ee60-453b-b410-c372ab863a24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir: str = g.HF_MODEL_ID.split(\"/\")[-1]\n",
    "model_tar_gz_path: str = os.path.join(os.path.dirname(os.getcwd()), f\"model_{model_dir}.tar.gz\")\n",
    "logger.info(f\"HF_MODEL_ID={g.HF_MODEL_ID}, model_dir={model_dir}, model_tar_gz_path={model_tar_gz_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3419b4-9b47-4e40-a985-90b01dc7ab4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Prepare the `model.tar.gz`\n",
    "\n",
    "1. Download the model files from HuggingFace.\n",
    "\n",
    "1. Update the `inference.py` with [`llava_inference.py`](./llava_inference.py)\n",
    "\n",
    "1. Zip the model directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be9a58-a0f4-4489-a041-c68ace7f164e",
   "metadata": {},
   "source": [
    "Download the model files. **This takes about 5 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb9bd4-b287-4627-847f-c29ac48c0c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_path: str = os.path.join(os.path.dirname(os.getcwd()), model_dir)\n",
    "Path(model_path).mkdir(exist_ok=True)\n",
    "# Download model from Hugging Face into model_dir\n",
    "snapshot_download(g.HF_MODEL_ID, local_dir=model_path, local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ddecbd-13e1-40f4-ab41-799b86c45971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the inference script\n",
    "inf_dest: str = os.path.join(model_path, 'code', 'inference.py')\n",
    "shutil.copyfile(\"llava_inference.py\", inf_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da211c93-6772-465d-ba6c-5da473075681",
   "metadata": {},
   "source": [
    "Create a .tar.gz file. **This step takes about 10 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877fefd-0d85-4f7e-add0-992977863f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create SageMaker model.tar.gz artifact\n",
    "!cd {model_path};tar -cf {model_tar_gz_path} --use-compress-program=pigz *;cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1970ca9-2935-4b58-8256-14ed675d08b7",
   "metadata": {},
   "source": [
    "Upload the model.tar.gz to S3. **This steps takes about 3 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd39a8-a2ec-474b-936b-8e507bb994da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# upload model.tar.gz to s3\n",
    "S3Uploader.upload(local_path=model_tar_gz_path, desired_s3_uri=g.S3_MODEL_URI)\n",
    "logger.info(f\"model uploaded to: {g.S3_MODEL_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b9d37-cabd-48a0-8466-8739d978f62f",
   "metadata": {},
   "source": [
    "## Step 3: Deploy the model on SageMaker\n",
    "\n",
    "Here we deploy the model on SageMaker. We use the [HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) class from the SageMaker SDK. **This steps takes about 10 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c801a72-6e60-4e7b-9e7a-6033ada82d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set the env vars for the model\n",
    "config: Dict = dict(HF_TASK=g.HF_TASK)\n",
    "\n",
    "model_data: str = os.path.join(g.S3_MODEL_URI, f\"model_{os.path.basename(g.HF_MODEL_ID)}.tar.gz\")\n",
    "instance_type: str = \"ml.g5.xlarge\"\n",
    "instance_count: int = 1\n",
    "logger.info(f\"going to deploy {g.HF_MODEL_ID} model, model_data={model_data}, instance_type={instance_type}, instance_count={instance_count}\")\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_data,   \n",
    "   role=get_execution_role(),                                  \n",
    "   transformers_version=g.TRANSFORMERS_VERSION,  \n",
    "   pytorch_version=g.PYTORCH_VERSION,            \n",
    "   py_version=g.PYTHON_VERSION,                \n",
    "   model_server_workers=1,\n",
    "   env=config\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(initial_instance_count=instance_count,\n",
    "                                     instance_type=instance_type)\n",
    "logger.info(f\"finished deploying model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e45f06-3344-41f6-acdd-2a7ef13292eb",
   "metadata": {},
   "source": [
    "The [HuggingFaceModel](https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html) encapsulated several defaults, lets examine the parameters for the deployed model to review the model settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f73749-8c41-4336-8e1f-6ea39d8fe938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"model info -> {vars(huggingface_model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd383428-56be-4109-a12a-fc456c675f8b",
   "metadata": {},
   "source": [
    "Save the name of the deployed endpoint so that the other notebooks can create a [`Predictor`](https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html) and use this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d8d30-ed38-4bf9-b894-e1a4590ed432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = Path(g.ENDPOINT_FILENAME).write_text(predictor.endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
