---
title: "Part 1: Talk to your slide deck (Multimodal RAG) using foundation models (FMs) hosted on Amazon Bedrock and Amazon SageMaker"
format:
  html:
    embed-resources: true
    output-file: blog_post.html
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  docx:
    embed-resources: true
    output-file: blog_post.docx
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  gfm: 
    output-file: blog_post.md
---

_Amit Arora_, _Archana Inapudi_, _Manju Prasad_, _Antara Raisa_

With the advent of generative AI, today's foundation models (FMs), such as the large language models (LLMs) Claude 2 and Llama 2 can perform a range of (generative) tasks such as question answering, content creation and others on "text" data. Real world data however, exists in multiple modalities such as "text", "images", "video" and "audio". Take a PowerPoint slide deck for example, It could contain information in the form of text, or embedded in graphs, tables and pictures. In this blog, we present a solution that uses Multimodal FMs such as the [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) model and [LLaVA 1.5](https://arxiv.org/pdf/2310.03744.pdf), to perform similar generative tasks on multimodal data.

## Solution overview

The solution presented provides an implementation for answering questions using information contained in the text and visual elements of a slide deck. The design relies on the concept of Retrieval Augmented Generation (RAG). Traditionally, RAG has been associated with textual data that can be processed by LLMs. In this blog, we extend RAG to include images as well. This provides a powerful search capability to extract contextually relevant content from visual elements like tables and graphs along with text.

There are different ways to design a RAG solution that includes images. We have presented one approach here and will follow-up with an alternate approach in the second blog of this three-part blog series. 

This solution includes the following components:

- [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) model: this FM is used to generate embeddings for the content in the slide deck used in this blog. As a multimodal model, this Titan model can process text, image or a combination as input and generate embeddings. The Titan Multimodal Embeddings model generates vectors (embeddings) of dimension 1024 and is accessed via the Amazon Bedrock service.
- [Large Language-and-Vision Assistant (LLaVA)](https://llava-vl.github.io/): LLaVA is an open source multimodal model for visual and language understanding and is used to interpret the data in the slides, including visual elements such as graphs and tables. We use the 7-billion parameter version [LLaVA 1.5-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) in this solution. 
- [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html): the LLaVA model to model is deployed on a SageMaker endpoint using SageMaker hosting services and we use the resulting endpoint to run inferences against the LLaVA model. We also use SageMaker Notebooks to orchestrate and demonstrate this solution end-to-end.
- [Amazon OpenSearch Service Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html): OpenSearch Service Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. We use OpenSearch Service Serverless as a vector database for storing embeddings generated by the Titan Multimodal Embeddings model. An index created in the OpenSearch Service Serverless collection serves as the vector store for our RAG solution. 
- [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) (OSI): OSI is a fully managed, serverless data collector that delivers data to Amazon OpenSearch Service domains and OpenSearch Serverless collections. In this blog, we are using an OSI pipeline to deliver data to the OpenSearch Serverless vector store. 

## Solution design 
The solution design consists of two parts - Ingestion and User interaction. During ingestion, we process the input slide deck by converting each slide into an image, generate embeddings for these images and then populate the vector data store. These steps are completed prior to the user interaction steps.

In the User interaction phase, a question from the user is converted into embeddings and a similarity search is run on the vector database to find a slide that could potentially contain answers to user question. We then provide this slide (in the form of an image file) to the LLaVA model and the user question as a prompt to generate an answer to the query. All the code for this post is available in the [GitHub](https://github.com/aws-samples/multimodal-rag-on-slide-decks/tree/main/Blog1-TitanEmbeddings-LVM) repo.

### Ingestion steps:

![Ingestion architecture](img/ML-16123-ingestion-design.jpg){#fig-ingestion-design}

1.	Slides are converted to image files (one per slide) in the JPG format and passed to the Titan Multimodal Embeddings model to generate embeddings. In our blog, we use this slide deck titled [Train and deploy Stable Diffusion using AWS Trainium & AWS Inferentia](https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf)  from the AWS Summit in Toronto, June 2023 to demonstrate the solution.

    - The sample deck has 31 slides and thus we generate 31 sets of vector embeddings, each with 1024 dimensions. We add additional metadata fields to these generated vector embeddings and create a JSON file. These additional metadata fields can be used to perform rich search queries using OpenSearch’s powerful search capabilities. 

1.	The generated embeddings are put together in a single JSON file that is uploaded to Amazon S3
1.	Via S3 Event Notification, an event is put on the Amazon Simple Queue Service (SQS) queue.
1.	This event on the SQS queue acts as a trigger to run the OSI pipeline which in turn ingests the data (JSON file) as documents into the OpenSearch Service Serverless index. 
    - Note that the OpenSearch Service Serverless index is configured as the sink for this pipeline and it is created as part of the OpenSearch Service Serverless collection.

### User interaction steps:
 
![User interaction architecture](img/ML-16123-user-interaction-design.jpg){#fig-ingestion-design}

1.	A user submits a question related to the slide deck that has been ingested.
1.	The user input is converted into embeddings using the Titan Multimodal Embeddings model accessed via Bedrock. An OpenSearch vector search is performed using these embeddings. We perform a K-Nearest Neighbor (`k=1`) search to retrieve the most relevant embedding matching the user query. Setting `k=1` retrieves the most relevant slide to the user question.
1.	The metadata of the response from OpenSearch Services Serverless contains a path to the image corresponding to the most relevant slide.
1.	A prompt is created by combining the user question and the image path and provided to LLaVA hosted on SageMaker. The LLaVA model is able to understand the user question and answer it by examining the data in the image.
1.	Result of this inference is returned to the user. 

These steps are discussed in detail in the following sections. See Results section for screenshots and details on the output. 

## Prerequisites

To implement the solution provided in this post, you should have an [AWS account](https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&client_id=signup) and familarity with FMs, Bedrock, SageMaker and OpenSearch Service. 

This solution uses the Titan multimodal embeddings model. Ensure that this model is enabled for use in Amazon Bedrock. In AWS Management Console → Amazon Bedrock, select Model access. If Titan Multimodal Embeddings is enabled, the Access status will state "Access granted" as below.

![User interaction architecture](img/ML-16123-request-model-access.png){#fig-request-model-access}

If the model is not available, enable access to the model by clicking on "Manage Model Access", selecting "Titan Multimodal Embeddings G1" and clicking on Request model access. The model is enabled for use immediately. 

![User interaction architecture](img/ML-16123-request-model-access-2.png){#fig-request-model-access-2}

## Use AWS CloudFormation template to create the solution stack

|AWS Region                |     Link    |
|:------------------------:|:-----------:|
|us-east-1  | [<img src="./img/ML-16123-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=multimodal-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123/template.yml)|
|us-west-2          | [<img src="./img/ML-16123-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=multimodal-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123/template.yml) |


After the stack is created successfully, navigate to the stack's `Outputs` tab on the AWS CloudFormation console and note the values for `MultimodalCollectionEndpoint`, we will use it in the subsequent steps.

![CloudFormation stack outputs](img/ML-16123-cloudformation-outputs.png){#fig-cft-outputs}

The CloudFormation template creates the following resources:

    - SageMaker Notebooks
    - OpenSearch Serverless collection
    - OSI Pipeline
    - S3 bucket
    - SQS Queue

The CloudFormation template creates two IAM roles. Update these roles to apply least-privilege permissions as discussed in Security best practices.
- SMExecutionRole with S3, SageMaker, OpenSearch Service, and Bedrock full access.
- OSPipelineExecutionRole with access to specific SQS and OSI actions.

The CloudFormation template also sets up Event Notification from S3 to SQS. Any objects created in the specified prefix (‘multimodal/osi-embeddings.-json“) will trigger SQS notifications that will be used by the OSI pipeline to hydrate the vector store. 
In addition, the CloudFormation template creates policies required to setup the OpenSearch components. Update these policies to apply least-privilege permissions as discussed in Security best practices.
- Network, Encryption and Data Access policies required for OpenSearch Serverless Collection
- Pipeline configuration required to setup OSI Pipeline with S3-SQS processing as source and OpenSearch Serverless index as sink 

Note that the CloudFormation template name and OpenSearch Service index name are referenced in the SageMaker notebook “3_raginference.ipynb”. If the default names are changed, make sure you update the same in the notebook.

## Testing the solution 

After the prerequisite steps are complete, all resources needed to run this solution have been created. To run the ‘talk to your slide deck’ implementation: 
In AWS Management Console → SageMaker, in left panel under Notebook, select Notebook instances. Click on Open JupyterLab next to the Notebook instance created by the CloudFormation template. 
 In File Browser, traverse to the notebooks folder to see notebooks and supporting files. The notebooks are numbered in sequence of execution. Instructions and comments in each notebook describe the actions performed by that notebook. 
Notebooks 0, 1 and 2 implement the Ingestion Steps described in Solution Design. Notebook 3 implements the User Interaction Steps described in Solution Design. 

 

Notebook 0: Deploying LLaVA
Open “0_deployllava.ipynb” to view the notebook and run all the cells. This notebook deploys the LLaVA-v1.5-7B model to a SageMaker endpoint.  We will use this endpoint in the user interaction steps. 
 Follow the same process for the remaining ipynb files. Remember to run them in sequence of their numbered prefix.

Notebook 1: Data Preparation 
In this notebook we download a publicly available slide deck and convert each slide into the JPG file format and upload these to a S3 bucket.   
Notebook 2: Data Ingestion
In this notebook, we use Titan Multimodal Embeddings model to convert the JPG images created in the previous notebook into vector embedding.  These embeddings and additional metadata are stored in a JSON file and uploaded to S3.  This action triggers the OpenSearch Ingestion pipeline ingest process that processes the file and hydrates an OpenSearch Service Index 

 

  


Here is a sample of the JSON file created (a vector with 4 dimensions is shown below. Titan Multimodal Embeddings model generates 1024 dimensions)
 


## Results 
Here is a compilation of some of the outputs generated by the User Interaction steps of our implementation. The Question column captures the user prompt/query, and Answer column is the textual response generated by LLaVA. Image is the K-Nearest (k=1) slide match returned by the OpenSearch vector search. This slide is interpreted by LLaVA to provide the Answer






Question	Answer	Image
How does Inf2 compare in performance to comparable EC2 instances? I need numbers.	According to the slide deck, Inf2 instances by AWS Inferentia2 offer up to 4x higher throughput and 10x lower latency compared to comparable EC2 instances.	 

As per the AI/ML flywheel, what do the AWS AI/ML services provide?	The AWS AI/ML services provide better $/perfer capabilities, new capabilities, and investment in innovation.	 
Compared to GPT-2, how many more parameters does GPT-3 have? What is the numerical difference between the parameter size of GPT-2 and GPT-3?	According to the slide, GPT-3 has 175 billion parameters, while GPT-2 has 1.5 billion parameters. The numerical difference between the parameter size of GPT-2 and GPT-3 is 173.5 billion.	 
What are quarks in particle physics?	I did not find the answer to this question in the slide deck.	 


Extending the implementation to your slide decks
This ingestion process and user interaction process can be easily extended to your own slides. To use this solution with your slide deck, update the SLIDE_DECK variable in globals.py with the URL to your slide deck. 
  Then run the ingestion steps described in Testing the solution. Test the ingested content by updating questions in ‘3_raginference.ipynb’ and running the notebook.


## Tip
Note that you can use OpenSearch Dashboards to interact with the OpenSearch API to run quick tests on your index and ingested data. 
  

## Cleanup
To avoid incurring future charges, delete the resources. You can do this by deleting the stack from the CloudFormation console.
 

Additionally, delete the SageMaker Inference Endpoint created for LLaVA inferencing.  You can uncomment the Cleanup step in ‘3_raginference.ipynb’ and run the cell or alternatively, using SageMaker → Inference → Endpoints, delete the Endpoint.  

## Conclusion
Enterprises generate new content all the time and slide decks are a common mechanism used to share and disseminate information internally with the organization and externally with customers or at conferences.  Over time, rich information can remain buried and hidden in non-text modalities like graphs and tables in these slide decks. 
You can use this solution and the power of Large Vision Models like LLaVA to discover new information or uncover new perspectives on content in slide decks. 

Future vision
Look out for 2 additional blogs as part of this series. 
Blog 2 will cover another approach you could take when ‘talking to your slide decks’. This approach will generate and store LLaVA inferences and use those stored inferences to respond to user queries. Blog 3 will compare the two approaches.

Portions of this code are released under the Apache 2.0 License as referenced here: https://aws.amazon.com/apache-2-0/


