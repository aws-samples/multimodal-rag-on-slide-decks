{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbb582e-d21a-41cf-b45e-e2f462424ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data ingestion\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.xlarge` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we download the images and text files corresponding to the `pdf file/slide deck` that we uploaded into Amazon S3 in the [1_data_prep.ipynb](./1_data_prep) notebook, get text description from `images` and `text files`, convert them into embeddings and then ingest these embeddings into a vector database i.e. [Amazon OpenSearch Service Serverless](https://aws.amazon.com/opensearch-service/features/serverless/).\n",
    "\n",
    "1. We use the [Anthropic’s Claude 3 Sonnet foundation model](https://aws.amazon.com/about-aws/whats-new/2024/03/anthropics-claude-3-sonnet-model-amazon-bedrock/) available on Bedrock to convert image to text.\n",
    "\n",
    "1. We use the text extracted from each pdf page as is and convert them into embeddings using [Amazon Titan Text Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) and stored in a `text` index. Each image file is first described using `Claude Sonnet` then the embeddings of the text description of that image is stored in an `image index`.\n",
    "\n",
    "1. We use an `entities` field in the `index body metadata` to store entities from both images and texts in their respective `image and text indexes`. The entities from images are extracted using `Claude Sonnet` and entities from texts extracted files using `nltk`. The purpose of extracting these entities is to later use them as a `prefilter` to get only the related documents to any user question.\n",
    "\n",
    "1. We use `Ray` for running Bedrock inference concurrently in an asynchronous manner.\n",
    "\n",
    "1. The embeddings are then ingested into OpenSearch Service Serverless using the [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) pipeline. We ingest the embeddings into an OpenSearch Serverless index via the OpenSearch Ingestion API.\n",
    "\n",
    "1. The OpenSearch Service Serverless Collection is created via the AWS CloudFormation stack for this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a62857-6a66-44db-9d92-3df221c6bd21",
   "metadata": {},
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f869e5d-8e4b-4d44-9e2a-4f20b77b92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install the requirements before running this notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda5d30-06f7-44a6-9b1d-8ac27fb93cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries that are needed to run this notebook\n",
    "import os\n",
    "import re\n",
    "import ray\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import nltk\n",
    "import boto3\n",
    "import base64\n",
    "import logging\n",
    "import requests\n",
    "import botocore\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import opensearchpy\n",
    "import globals as g\n",
    "from pathlib import Path\n",
    "from nltk.tree import Tree\n",
    "from nltk.tag import pos_tag\n",
    "from typing import List, Dict\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, word_tokenize, punkt\n",
    "from requests_auth_aws_sigv4 import AWSSigV4\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_bucket_name, download_image_files_from_s3, get_text_embedding, load_and_merge_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e79b34-a7ee-4bfc-a7e0-109ef54426f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bbced-2937-4ecd-945d-ca380f94a456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcac8a-e241-4710-8499-0e834cf2df67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7153438-f58d-4f63-b04e-1f8a5a02fc71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the merged config file - user config file, and parent config file\n",
    "config = load_and_merge_configs(g.CONFIG_SUBSET_FILE, g.FULL_CONFIG_FILE)\n",
    "logger.info(f\"config file -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e6c2cd-a79b-4e21-837f-9e37a9077689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region: str = boto3.Session().region_name\n",
    "claude_model_id: str = config['model_info']['inference_model_info'].get('model_id')\n",
    "endpoint_url: str = g.BEDROCK_EP_URL.format(region=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9ddf2-d8f5-404a-8fcf-a5efa3f3fad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bucket_name: str = get_bucket_name(config['aws']['cfn_stack_name'])\n",
    "logger.info(f\"Bucket name being used to store extracted images and texts from data: {bucket_name}\")\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d273a8-4d5a-4ede-bc3f-b7d9c8fe6825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba8ccd-7e5e-48c5-93fd-2a98319f33c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = get_cfn_outputs(config['aws']['cfn_stack_name'])\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "text_index_name = outputs['OpenSearchTextIndexName']\n",
    "img_index_name = outputs['OpenSearchImgIndexName']\n",
    "logger.info(f\"opensearchhost={host}, text index={text_index_name}, image index={img_index_name}\")\n",
    "osi_text_endpoint = f\"https://{outputs['OpenSearchPipelineTextEndpoint']}/data/ingest\"\n",
    "osi_img_endpoint = f\"https://{outputs['OpenSearchPipelineImgEndpoint']}/data/ingest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9213968-f87b-4e19-9d4d-99d0ab8f19d6",
   "metadata": {},
   "source": [
    "#### We use the OpenSearch client to create an index.\n",
    "---\n",
    "For the purpose of segregation and ease of understanding, we are initializing two opensearch clients (for each image and text index). You can create/use just one index too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95923df7-fe11-4ace-ba2a-f56edf0ac1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, g.OS_SERVICE)\n",
    "\n",
    "# Represents the OSI client for images\n",
    "img_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "# Represents the OSI client for texts\n",
    "text_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f46cae-f11e-4ed2-9574-438c87a1d932",
   "metadata": {},
   "source": [
    "#### Index Body\n",
    "---\n",
    "Given below is the index body that is stored in the opensearch service. It contains information about:\n",
    "\n",
    "1. **File path**: The path of the text or image file in the index\n",
    "\n",
    "1. **File text**: The texts extracted from the pdf files (for the text index) or the image descriptions for images that are stored in the image index\n",
    "\n",
    "1. **Page number**: Represents the page number that the content is stemming from\n",
    "\n",
    "1. **Metadata**: This field within the index body contains information about the name of the file and entities. Entities represent names of organizations, people, and other important within the pdf text/image that is extracted and stored as metadata for future prefilter purposes to only get relevant documents during the process of search for relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921892a-e599-490b-bd91-accc5cdb5f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "index_body = \"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1536,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"file_path\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"file_text\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"page_number\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"metadata\": {\n",
    "        \"properties\": {\n",
    "          \"filename\": {\n",
    "            \"type\": \"text\"\n",
    "          },\n",
    "          \"entities\": {\n",
    "            \"type\": \"keyword\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# We would get an index already exists exception if the index already exists, and that is fine\n",
    "index_body = json.loads(index_body)\n",
    "try:\n",
    "    # Check if the image index exists\n",
    "    if not img_os_client.indices.exists(img_index_name):\n",
    "        img_response = img_os_client.indices.create(img_index_name, body=index_body)\n",
    "        logger.info(f\"Response received for the create index for images -> {img_response}\")\n",
    "    else:\n",
    "        logger.info(f\"The image index '{img_index_name}' already exists.\")\n",
    "\n",
    "    # Check if the text index exists\n",
    "    if not text_os_client.indices.exists(text_index_name):\n",
    "        txt_response = text_os_client.indices.create(text_index_name, body=index_body)\n",
    "        logger.info(f\"Response received for the create index for texts -> {txt_response}\")\n",
    "    else:\n",
    "        logger.info(f\"The text index '{text_index_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in creating index, exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d693976-9a99-41f7-ba48-4417fbe0a83b",
   "metadata": {},
   "source": [
    "### Check if the the index created has a `knn`/vector field count before the embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4934d3-08f8-49c0-8e51-a0c447a078c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    # Fetch the existing mapping for the text index\n",
    "    text_mapping = text_os_client.indices.get_mapping(index=text_index_name)\n",
    "    img_mapping = img_os_client.indices.get_mapping(index=img_index_name)\n",
    "    text_vector_embedding_mapping = text_mapping[text_index_name]['mappings']['properties'].get('vector_embedding', {})\n",
    "    img_vector_embedding_mapping = img_mapping[img_index_name]['mappings']['properties'].get('vector_embedding', {})\n",
    "\n",
    "    if text_vector_embedding_mapping.get('type') == 'knn_vector':\n",
    "        logger.info(f\"The vector_embedding type is found: {text_vector_embedding_mapping.get('type')} -> {text_mapping}\")\n",
    "    else:\n",
    "        raise ValueError(f\"The vector_embedding type is not 'knn_vector', found: {text_vector_embedding_mapping.get('type')}\")\n",
    "\n",
    "    if img_vector_embedding_mapping.get('type') == 'knn_vector':\n",
    "        logger.info(f\"The vector_embedding type is found: {img_vector_embedding_mapping.get('type')} -> {img_mapping}\")\n",
    "    else:\n",
    "        raise ValueError(f\"The vector_embedding type is not 'knn_vector', found: {img_vector_embedding_mapping.get('type')}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in fetching the index vector field mapping, exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369078c2-94b1-4510-940a-78e6b029e506",
   "metadata": {},
   "source": [
    "## Step 2. Download the images files from S3 and convert to Base64\n",
    "\n",
    "Now we download the image files from the S3 bucket into the `local directory`. Once downloaded these files are converted into [Base64](https://en.wikipedia.org/wiki/Base64) encoding so that we can create embeddings from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0ec15-3aca-41d9-a4b1-4b6dde4a2f75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the images from s3 into a local directory to convert into base64 images\n",
    "os.makedirs(g.LOCAL_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(g.LOCAL_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    image_files: List = download_image_files_from_s3(bucket_name, g.BUCKET_IMG_PREFIX, g.LOCAL_IMAGE_DIR, g.IMAGE_FILE_EXTN)\n",
    "    text_files: List = download_image_files_from_s3(bucket_name, g.BUCKET_TEXT_PREFIX, g.LOCAL_TEXT_DIR, g.TEXT_FILE_EXTN)\n",
    "    logger.info(f\"downloaded {len(image_files) + len(text_files)} files from s3\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Cannot download the images files from S3 into the local directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70833d4-b585-477b-93ec-5556c994b9ba",
   "metadata": {},
   "source": [
    "#### Convert jpg files fetched from `S3` into `Base64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c8f80-d457-46d5-9d20-f1ca8ebe106b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_file_path: str) -> str:\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        b64_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        b64_image_path = os.path.join(g.B64_ENCODED_IMAGES_DIR, f\"{Path(image_file_path).stem}.b64\")\n",
    "        with open(b64_image_path, \"wb\") as b64_image_file:\n",
    "            b64_image_file.write(bytes(b64_image, 'utf-8'))\n",
    "    return b64_image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d70875-7b8b-44ff-bb5a-0f8b9e43adca",
   "metadata": {},
   "source": [
    "## Step 3. Get embeddings for the base64 encoded images\n",
    "\n",
    "Now we are ready to use Amazon Bedrock via the  Anthropic’s Claude 3 Sonnet foundation model and Amazon Titan Text Embeddings model to convert the base64 version of the images into embeddings. We ingest embeddings into the pipeline using the [requests](https://pypi.org/project/requests/) HTTP library\n",
    "\n",
    "You must sign all HTTP requests to the pipeline using [Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef0ed32-9e7f-448b-93d5-ef0e65404f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_desc(image_file_path: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function uses a base64 file path of an image, and then uses ClaudeV3 Sonnet to \n",
    "    describe the image\n",
    "    \"\"\"\n",
    "    bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)\n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        input_image_b64 = image_file.read().decode('utf-8')\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/jpeg\",\n",
    "                                \"data\": input_image_b64\n",
    "                            },\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=claude_model_id,\n",
    "        body=body\n",
    "    )\n",
    "\n",
    "    resp_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "    resp_text = resp_body['content'][0]['text'].replace('\"', \"'\")\n",
    "    return resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf7db4-5d1f-4b12-99ec-a5484cc55036",
   "metadata": {},
   "source": [
    "### Use the image files downloaded from S3, and convert them into `Base64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67111d9-359d-46fe-997e-cbb8e5f7992f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(g.B64_ENCODED_IMAGES_DIR, exist_ok=True)\n",
    "try:\n",
    "    file_list: List = glob.glob(os.path.join(g.LOCAL_IMAGE_DIR, f\"*{g.IMAGE_FILE_EXTN}\"))\n",
    "    logger.info(f\"there are {len(file_list)} pdf image files in the {g.IMAGE_DIR} directory for conversion to base64\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not list any {g.IMAGE_FILE_EXTN} files from {g.IMAGE_DIR}: {e}\")\n",
    "\n",
    "# convert each file to base64 and store the base64 in a new file\n",
    "b64_image_file_list = list(map(encode_image_to_base64, file_list))\n",
    "logger.info(f\"base64 conversion done, there are {len(b64_image_file_list)} base64 encoded files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1431afe-11b1-4c88-bee1-78e9aeac0f8e",
   "metadata": {},
   "source": [
    "### Get Image Descriptions\n",
    "---\n",
    "\n",
    "This part of the notebook uses an `image_description_prompt` to describe the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42fa6d7-1291-4fed-a75a-cb6f225208bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the prompt to get the description of each image stored from the pdf file\n",
    "image_description_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['image_description_prompt'])\n",
    "image_desc_prompt: str = Path(image_description_prompt_fpath).read_text()\n",
    "print(image_desc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe56b3-b7e6-4012-b432-18289085ec55",
   "metadata": {},
   "source": [
    "### Hybrid Search: Extract `Entities` from the images for further `prefiltering` tasks\n",
    "---\n",
    "\n",
    "The purpose of using Hybrid search is to optimize the RAG workflow in retrieving the right image description for specific questions. Some images (full or split in different parts), might not contain the information that is being asked by the question, because of the surrounding embeddings in the vector DB and might fetch the wrong image if it has a similar structure, so Hybrid search helps optimizing that. In this case, we will extract the entities of an image description (including the file name to be precise), then extract the entities of the question being asked, to get the most accurate response possible. `Entities` will help match the question to the correct and most relevant documents in the vector index where the answer can searched for in another sub step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d4d276-37ca-49b8-9787-020c31b8bb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt is used to extract entities from an image\n",
    "entity_extraction_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['extract_image_entities_template'])\n",
    "entity_extraction_prompt: str = Path(entity_extraction_prompt_fpath).read_text()\n",
    "print(entity_extraction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b8685-58b6-42a2-b826-e243cac2b781",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 1: Loop through b64 images to 1/get image desc from Claude3, 2/get embedding from Titan text. Call OSI pipeline API to ingest embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0326f1-5d73-48d0-90ce-db151918631c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_txt_embeddings(bedrock: botocore.client, prompt_data: str) -> np.ndarray:\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt_data,\n",
    "    })\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, modelId=config['model_info']['embeddings_model_info'].get('model_id'), \n",
    "            accept=\"application/json\", contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        embedding = response_body.get('embedding')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        embedding = None\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f3fd2-303d-4a58-b707-5a6e8b731e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to get the image description and store the embeddings of that text in the image index\n",
    "def process_image_data(i: int, \n",
    "                       file_path: str, \n",
    "                       osi_endpoint, \n",
    "                       total: int, \n",
    "                       bucket_info: int) -> Dict:\n",
    "    bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)\n",
    "    json_data: Optional[Dict] = None\n",
    "    # name of the images that are saved (either split in 4 ways or saved as a single page)\n",
    "    image_name: Optional[str] = None\n",
    "    try:\n",
    "        logger.info(f\"going to convert {file_path} into embeddings\")\n",
    "        # first, get the entities from the image to prefilter the image description with the entities\n",
    "        entities_extracted = get_img_desc(file_path, entity_extraction_prompt)\n",
    "        # get the image description and prepend the image description with the entities extracted from the image\n",
    "        content_description = entities_extracted + get_img_desc(file_path, image_desc_prompt)\n",
    "        print(f\"file_path: {file_path}, image description (prefiltered with entities extracted): {content_description}\")\n",
    "        embedding = get_img_txt_embeddings(bedrock, content_description)\n",
    "        input_image_s3: str = f\"s3://{bucket_name}/{bucket_info['img_prefix']}/{Path(file_path).stem}{bucket_info['image_file_extn']}\"\n",
    "        obj_name: str = f\"{Path(file_path).stem}{bucket_info['image_file_extn']}\"\n",
    "        # data format for POSTING it to the osi_endpoint\n",
    "        data = json.dumps([{\n",
    "            \"file_path\": input_image_s3,\n",
    "            \"file_text\": content_description,\n",
    "            \"page_number\": re.search(r\"page_(\\d+)_?\", obj_name).group(1),\n",
    "            \"metadata\": {\n",
    "                \"filename\": obj_name,\n",
    "                \"entities\": entities_extracted\n",
    "            },\n",
    "            \"vector_embedding\": embedding\n",
    "        }])\n",
    "        # json data format for local files that are saved\n",
    "        json_data = {\n",
    "            \"file_type\": bucket_info['image_file_extn'],\n",
    "            \"file_name\": obj_name,\n",
    "            \"text\": content_description,\n",
    "            \"entities\": entities_extracted,\n",
    "            \"page_number\": re.search(r\"page_(\\d+)_?\", obj_name).group(1)\n",
    "            }\n",
    "        # save the information (image description, entities, file type, name, and page number)\n",
    "        # locally in a json file\n",
    "        image_dir: str = config['dir_info']['json_img_dir']\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        fpath = os.path.join(image_dir, f\"{Path(file_path).stem}.json\")\n",
    "        Path(fpath).write_text(json.dumps(json_data, default=str, indent=2))\n",
    "        r = requests.request(\n",
    "            method='POST', \n",
    "            url=osi_endpoint, \n",
    "            data=data,\n",
    "            auth=AWSSigV4('osis'))\n",
    "        logger.info(\"Ingesting data into pipeline\")\n",
    "        logger.info(f\"image desc: {r.text}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing image {file_path}: {e}\")\n",
    "        json_data: Optional[Dict] = None\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272e4057-a30e-4b88-a1e7-6c55986f8b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def async_process_image_data(i: int, file_path: str, osi_endpoint, total: int, bucket_info: Dict):\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return process_image_data(i, file_path, osi_endpoint, total, bucket_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b937d157-f346-499f-8ef1-79686f2f28e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count the number of images that throw an error while being saved into the index\n",
    "erroneous_page_count: int = 0\n",
    "n: int = config['inference_info']['parallel_inference_count']\n",
    "image_chunks = [b64_image_file_list[i:i + n] for i in range(0, len(b64_image_file_list), n)]\n",
    "bucket_info: Dict = {\n",
    "    'img_prefix': g.BUCKET_IMG_PREFIX,\n",
    "    'image_file_extn': g.IMAGE_FILE_EXTN\n",
    "}\n",
    "for chunk_index, image_chunk in enumerate(image_chunks):\n",
    "    try:\n",
    "        st = time.perf_counter()\n",
    "        logger.info(f\"------ getting text description for chunk {chunk_index}/{len(image_chunks)} -----\")\n",
    "        # Iterate over each file path in the chunk and process it individually\n",
    "        logger.info(f\"getting inference for list {chunk_index+1}/{len(image_chunks)}, size of list={len(image_chunk)} \")\n",
    "        results = ray.get([async_process_image_data.remote(index, file_path, osi_img_endpoint, len(image_chunk), bucket_info) for index, file_path in enumerate(image_chunk)])\n",
    "        elapsed_time = time.perf_counter() - st\n",
    "        logger.info(f\"------ completed chunk={chunk_index}/{len(image_chunks)} completed in {elapsed_time} ------ \")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing chunk {chunk_index}: {e}\")\n",
    "        erroneous_page_count += len(image_chunk)\n",
    "\n",
    "logger.info(f\"Number of erroneous pdf pages that are not processed: {erroneous_page_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8fe93-1cde-4d0f-ae1d-6dd5b3f9040d",
   "metadata": {},
   "source": [
    "### Part 2: Loop through text files to 1/get embedding from Titan text, 2/extract the text entities using `nltk`. Call OSI pipeline API to ingest embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68475cb8-0430-476d-8ae9-b22dad72aa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a list of all text files \n",
    "pdf_txt_file_list = os.listdir(g.LOCAL_TEXT_DIR)\n",
    "\n",
    "# Get absolute file paths by joining the directory path with each file name\n",
    "pdf_txt_file_list = [os.path.abspath(os.path.join(g.LOCAL_TEXT_DIR, file)) for file in pdf_txt_file_list]\n",
    "logger.info(f\"Number of text files from the PDF local directory to process: {len(pdf_txt_file_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc23f-7a18-4ce3-9147-214b05eb56d2",
   "metadata": {},
   "source": [
    "#### Entities extraction from PDF texts using [NLTK]('https://www.nltk.org/')\n",
    "---\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. We use `NLTK` to extract entities from the text files that are extracted from each `PDF page`, and use that as a prepend onto the extracted file to be sent to the `OSI endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c50ccdf-70cf-4dee-9ee9-97ef272451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker') \n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b546f2-90ad-483c-8670-b47d084e147c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    \"\"\"\n",
    "    This function uses nltk to get the entities from texts that are extracted from pdf files\n",
    "    \"\"\"\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b18389-711e-44b3-b06e-f9b50685f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text_data(txt_file: str, txt_page_index: int):\n",
    "    with open(txt_file, 'r') as file:\n",
    "        extracted_pdf_text = file.read()\n",
    "    # Extract entities from text using nltk\n",
    "    entities = get_continuous_chunks(extracted_pdf_text)\n",
    "    # Convert the entities list to string \n",
    "    entities_str = \", \".join(entities)\n",
    "    logger.info(f\"entities extracted from {txt_file}: {entities_str}\")\n",
    "    embedding = get_text_embedding(bedrock, extracted_pdf_text)\n",
    "    input_text_s3 = f\"s3://{bucket_name}/{g.BUCKET_TEXT_PREFIX}/{Path(txt_file).stem}{g.TEXT_FILE_EXTN}\"\n",
    "    obj_name = f\"{Path(txt_file).stem}{g.TEXT_FILE_EXTN}\"\n",
    "    # data format that is used to POST to the osi endpoint\n",
    "    data = json.dumps([{\n",
    "        \"file_path\": input_text_s3,\n",
    "        \"file_text\": extracted_pdf_text,\n",
    "        \"page_number\": txt_page_index,\n",
    "        \"metadata\": {\n",
    "            \"filename\": obj_name,\n",
    "            \"entities\": entities_str\n",
    "        },\n",
    "        \"vector_embedding\": embedding\n",
    "    }])\n",
    "    # json data format that is saved in a local directory\n",
    "    json_data = {\n",
    "        \"file_type\": g.TEXT_FILE_EXTN,\n",
    "        \"file_name\": Path(txt_file).stem,\n",
    "        \"text\": extracted_pdf_text, \n",
    "        \"page_number\": re.search(r\"text_(\\d+)_?\", obj_name).group(1),\n",
    "        \"entities\": entities_str  \n",
    "    } \n",
    "    os.makedirs(config['dir_info']['json_txt_dir'], exist_ok=True)\n",
    "    fpath = os.path.join(config['dir_info']['json_txt_dir'], f\"{Path(txt_file).stem}.json\")\n",
    "    print(f\"json_file_path: {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(json_data, default=str, indent=2))\n",
    "    r = requests.request(\n",
    "        method='POST',\n",
    "        url=osi_text_endpoint,\n",
    "        data=data,\n",
    "        auth=AWSSigV4('osis'))\n",
    "\n",
    "    logger.info(\"Ingesting data into pipeline\")\n",
    "    logger.info(f\"Response: {txt_page_index} - {r.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d48c1-8ca4-486c-abeb-989e14ef4ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt_page_index: int = 1\n",
    "os.makedirs(config['dir_info']['json_txt_dir'], exist_ok=True)\n",
    "for txt_file in pdf_txt_file_list:\n",
    "    logger.info(f\"going to convert {txt_file} into embeddings\")\n",
    "    process_text_data(txt_file, txt_page_index)\n",
    "    txt_page_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b3392-719e-41f9-af16-370f706146e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
