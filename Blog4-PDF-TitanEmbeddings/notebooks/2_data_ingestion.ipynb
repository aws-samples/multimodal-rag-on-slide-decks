{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bbb582e-d21a-41cf-b45e-e2f462424ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data ingestion\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.xlarge` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we download the images and text files corresponding to the `pdf file/slide deck` that we uploaded into Amazon S3 in the [1_data_prep.ipynb](./1_data_prep) notebook, get text description from `images` and `text files`, convert them into embeddings and then ingest these embeddings into a vector database i.e. [Amazon OpenSearch Service Serverless](https://aws.amazon.com/opensearch-service/features/serverless/).\n",
    "\n",
    "1. We use the [Anthropicâ€™s Claude 3 Sonnet foundation model](https://aws.amazon.com/about-aws/whats-new/2024/03/anthropics-claude-3-sonnet-model-amazon-bedrock/) available on Bedrock to convert image to text.\n",
    "\n",
    "1. We use the text extracted from each pdf page as is and convert them into embeddings using [Amazon Titan Text Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) and stored in a `text` index. Each image file is first described using `Claude Sonnet` then the embeddings of the text description of that image is stored in an `image index`.\n",
    "\n",
    "1. We use an `entities` field in the `index body metadata` to store entities from both images and texts in their respective `image and text indexes`. The entities from images are extracted using `Claude Sonnet` and entities from texts extracted files using `nltk`. The purpose of extracting these entities is to later use them as a `prefilter` to get only the related documents to any user question.\n",
    "\n",
    "1. We use `Ray` for running Bedrock inference concurrently in an asynchronous manner.\n",
    "\n",
    "1. The embeddings are then ingested into OpenSearch Service Serverless using the [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) pipeline. We ingest the embeddings into an OpenSearch Serverless index via the OpenSearch Ingestion API.\n",
    "\n",
    "1. The OpenSearch Service Serverless Collection is created via the AWS CloudFormation stack for this blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a62857-6a66-44db-9d92-3df221c6bd21",
   "metadata": {},
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f869e5d-8e4b-4d44-9e2a-4f20b77b92d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==2.221.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.221.1)\n",
      "Requirement already satisfied: pypdfium2==4.24.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.24.0)\n",
      "Requirement already satisfied: ray==2.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: PyPDF2==3.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: PyMuPDF==1.24.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.24.3)\n",
      "Requirement already satisfied: pdf2image==1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: litellm==1.37.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.37.20)\n",
      "Requirement already satisfied: httplib2==0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.19.0)\n",
      "Requirement already satisfied: langchain==0.0.340 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.0.340)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.31.0)\n",
      "Requirement already satisfied: pandas==1.5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.5.3)\n",
      "Requirement already satisfied: boto3==1.34.118 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.34.118)\n",
      "Requirement already satisfied: botocore==1.34.118 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.34.118)\n",
      "Requirement already satisfied: opensearch-py==2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.5.0)\n",
      "Requirement already satisfied: numexpr==2.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (2.8.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (24.1)\n",
      "Requirement already satisfied: docutils in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.16)\n",
      "Requirement already satisfied: openpyxl in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.1.4)\n",
      "Requirement already satisfied: requests-auth-aws-sigv4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (0.7)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (4.25.4)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (6.11.0)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (0.3.2)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (0.7.7)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (4.2.2)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (1.26.20)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (7.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (4.66.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker==2.221.1->-r requirements.txt (line 1)) (6.0.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 3)) (3.15.4)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 3)) (1.0.8)\n",
      "Requirement already satisfied: aiosignal in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from ray==2.10.0->-r requirements.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyMuPDF==1.24.3->-r requirements.txt (line 5)) (1.24.3)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pdf2image==1.17.0->-r requirements.txt (line 6)) (10.4.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (3.9.5)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (3.1.4)\n",
      "Requirement already satisfied: openai>=1.27.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (1.43.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from litellm==1.37.20->-r requirements.txt (line 7)) (0.20.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httplib2==0.19.0->-r requirements.txt (line 8)) (2.4.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (2.0.31)\n",
      "Requirement already satisfied: anyio<4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (0.0.92)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.340->-r requirements.txt (line 9)) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 10)) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 11)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas==1.5.3->-r requirements.txt (line 11)) (2024.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3==1.34.118->-r requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3==1.34.118->-r requirements.txt (line 12)) (0.10.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.5.0->-r requirements.txt (line 14)) (1.16.0)\n",
      "Requirement already satisfied: et-xmlfile in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openpyxl->-r requirements.txt (line 19)) (1.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->litellm==1.37.20->-r requirements.txt (line 7)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp->litellm==1.37.20->-r requirements.txt (line 7)) (1.9.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.340->-r requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.340->-r requirements.txt (line 9)) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340->-r requirements.txt (line 9)) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.340->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker==2.221.1->-r requirements.txt (line 1)) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.37.20->-r requirements.txt (line 7)) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.340->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (0.5.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.340->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.340->-r requirements.txt (line 9)) (2.20.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.340->-r requirements.txt (line 9)) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken>=0.4.0->litellm==1.37.20->-r requirements.txt (line 7)) (2024.7.24)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.221.1->-r requirements.txt (line 1)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.221.1->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker==2.221.1->-r requirements.txt (line 1)) (0.19.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.221.1->-r requirements.txt (line 1)) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.221.1->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.221.1->-r requirements.txt (line 1)) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker==2.221.1->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tokenizers->litellm==1.37.20->-r requirements.txt (line 7)) (0.24.6)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.27.0->litellm==1.37.20->-r requirements.txt (line 7)) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.37.20->-r requirements.txt (line 7)) (2024.6.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.340->-r requirements.txt (line 9)) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# install the requirements before running this notebook\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beda5d30-06f7-44a6-9b1d-8ac27fb93cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/nltk/metrics/association.py:26: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  from scipy.stats import fisher_exact\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "# import the libraries that are needed to run this notebook\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import ray\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import nltk\n",
    "import boto3\n",
    "import base64\n",
    "import logging\n",
    "import requests\n",
    "import botocore\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import opensearchpy\n",
    "import globals as g\n",
    "from pathlib import Path\n",
    "from nltk.tree import Tree\n",
    "from nltk.tag import pos_tag\n",
    "from typing import List, Dict\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, word_tokenize, punkt\n",
    "from requests_auth_aws_sigv4 import AWSSigV4\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_bucket_name, download_image_files_from_s3, get_text_embedding, load_and_merge_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e79b34-a7ee-4bfc-a7e0-109ef54426f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534bbced-2937-4ecd-945d-ca380f94a456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-04 20:17:57,220\tINFO worker.py:1752 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69256f6527154084bc7f15f3848e1dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<div class=\"lm-Widget p-Widget lm-Panel p-Panel jp-Cell-outputWrapper\">\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <div class=\"jp-RenderedHTMLCommon\" style=\"display: flex; flex-direction: row;\">\n",
       "  <svg viewBox=\"0 0 567 224\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\" style=\"height: 3em;\">\n",
       "    <g clip-path=\"url(#clip0_4338_178347)\">\n",
       "        <path d=\"M341.29 165.561H355.29L330.13 129.051C345.63 123.991 354.21 112.051 354.21 94.2307C354.21 71.3707 338.72 58.1807 311.88 58.1807H271V165.561H283.27V131.661H311.8C314.25 131.661 316.71 131.501 319.01 131.351L341.25 165.561H341.29ZM283.29 119.851V70.0007H311.82C331.3 70.0007 342.34 78.2907 342.34 94.5507C342.34 111.271 331.34 119.861 311.82 119.861L283.29 119.851ZM451.4 138.411L463.4 165.561H476.74L428.74 58.1807H416L367.83 165.561H380.83L392.83 138.411H451.4ZM446.19 126.601H398L422 72.1407L446.24 126.601H446.19ZM526.11 128.741L566.91 58.1807H554.35L519.99 114.181L485.17 58.1807H472.44L514.01 129.181V165.541H526.13V128.741H526.11Z\" fill=\"var(--jp-ui-font-color0)\"/>\n",
       "        <path d=\"M82.35 104.44C84.0187 97.8827 87.8248 92.0678 93.1671 87.9146C98.5094 83.7614 105.083 81.5067 111.85 81.5067C118.617 81.5067 125.191 83.7614 130.533 87.9146C135.875 92.0678 139.681 97.8827 141.35 104.44H163.75C164.476 101.562 165.622 98.8057 167.15 96.2605L127.45 56.5605C121.071 60.3522 113.526 61.6823 106.235 60.3005C98.9443 58.9187 92.4094 54.9203 87.8602 49.0574C83.3109 43.1946 81.0609 35.8714 81.5332 28.4656C82.0056 21.0599 85.1679 14.0819 90.4252 8.8446C95.6824 3.60726 102.672 0.471508 110.08 0.0272655C117.487 -0.416977 124.802 1.86091 130.647 6.4324C136.493 11.0039 140.467 17.5539 141.821 24.8501C143.175 32.1463 141.816 39.6859 138 46.0505L177.69 85.7505C182.31 82.9877 187.58 81.4995 192.962 81.4375C198.345 81.3755 203.648 82.742 208.33 85.3976C213.012 88.0532 216.907 91.9029 219.616 96.5544C222.326 101.206 223.753 106.492 223.753 111.875C223.753 117.258 222.326 122.545 219.616 127.197C216.907 131.848 213.012 135.698 208.33 138.353C203.648 141.009 198.345 142.375 192.962 142.313C187.58 142.251 182.31 140.763 177.69 138L138 177.7C141.808 184.071 143.155 191.614 141.79 198.91C140.424 206.205 136.44 212.75 130.585 217.313C124.731 221.875 117.412 224.141 110.004 223.683C102.596 223.226 95.6103 220.077 90.3621 214.828C85.1139 209.58 81.9647 202.595 81.5072 195.187C81.0497 187.779 83.3154 180.459 87.878 174.605C92.4405 168.751 98.9853 164.766 106.281 163.401C113.576 162.035 121.119 163.383 127.49 167.19L167.19 127.49C165.664 124.941 164.518 122.182 163.79 119.3H141.39C139.721 125.858 135.915 131.673 130.573 135.826C125.231 139.98 118.657 142.234 111.89 142.234C105.123 142.234 98.5494 139.98 93.2071 135.826C87.8648 131.673 84.0587 125.858 82.39 119.3H60C58.1878 126.495 53.8086 132.78 47.6863 136.971C41.5641 141.163 34.1211 142.972 26.7579 142.059C19.3947 141.146 12.6191 137.574 7.70605 132.014C2.79302 126.454 0.0813599 119.29 0.0813599 111.87C0.0813599 104.451 2.79302 97.2871 7.70605 91.7272C12.6191 86.1673 19.3947 82.5947 26.7579 81.6817C34.1211 80.7686 41.5641 82.5781 47.6863 86.7696C53.8086 90.9611 58.1878 97.2456 60 104.44H82.35ZM100.86 204.32C103.407 206.868 106.759 208.453 110.345 208.806C113.93 209.159 117.527 208.258 120.522 206.256C123.517 204.254 125.725 201.276 126.771 197.828C127.816 194.38 127.633 190.677 126.253 187.349C124.874 184.021 122.383 181.274 119.205 179.577C116.027 177.88 112.359 177.337 108.826 178.042C105.293 178.746 102.113 180.654 99.8291 183.44C97.5451 186.226 96.2979 189.718 96.3 193.32C96.2985 195.364 96.7006 197.388 97.4831 199.275C98.2656 201.163 99.4132 202.877 100.86 204.32ZM204.32 122.88C206.868 120.333 208.453 116.981 208.806 113.396C209.159 109.811 208.258 106.214 206.256 103.219C204.254 100.223 201.275 98.0151 197.827 96.97C194.38 95.9249 190.676 96.1077 187.348 97.4873C184.02 98.8669 181.274 101.358 179.577 104.536C177.879 107.714 177.337 111.382 178.041 114.915C178.746 118.448 180.653 121.627 183.439 123.911C186.226 126.195 189.717 127.443 193.32 127.44C195.364 127.443 197.388 127.042 199.275 126.259C201.163 125.476 202.878 124.328 204.32 122.88ZM122.88 19.4205C120.333 16.8729 116.981 15.2876 113.395 14.9347C109.81 14.5817 106.213 15.483 103.218 17.4849C100.223 19.4868 98.0146 22.4654 96.9696 25.9131C95.9245 29.3608 96.1073 33.0642 97.4869 36.3922C98.8665 39.7202 101.358 42.4668 104.535 44.1639C107.713 45.861 111.381 46.4036 114.914 45.6992C118.447 44.9949 121.627 43.0871 123.911 40.301C126.195 37.515 127.442 34.0231 127.44 30.4205C127.44 28.3772 127.038 26.3539 126.255 24.4664C125.473 22.5788 124.326 20.8642 122.88 19.4205ZM19.42 100.86C16.8725 103.408 15.2872 106.76 14.9342 110.345C14.5813 113.93 15.4826 117.527 17.4844 120.522C19.4863 123.518 22.4649 125.726 25.9127 126.771C29.3604 127.816 33.0638 127.633 36.3918 126.254C39.7198 124.874 42.4664 122.383 44.1635 119.205C45.8606 116.027 46.4032 112.359 45.6988 108.826C44.9944 105.293 43.0866 102.114 40.3006 99.8296C37.5145 97.5455 34.0227 96.2983 30.42 96.3005C26.2938 96.3018 22.337 97.9421 19.42 100.86ZM100.86 100.86C98.3125 103.408 96.7272 106.76 96.3742 110.345C96.0213 113.93 96.9226 117.527 98.9244 120.522C100.926 123.518 103.905 125.726 107.353 126.771C110.8 127.816 114.504 127.633 117.832 126.254C121.16 124.874 123.906 122.383 125.604 119.205C127.301 116.027 127.843 112.359 127.139 108.826C126.434 105.293 124.527 102.114 121.741 99.8296C118.955 97.5455 115.463 96.2983 111.86 96.3005C109.817 96.299 107.793 96.701 105.905 97.4835C104.018 98.2661 102.303 99.4136 100.86 100.86Z\" fill=\"#00AEEF\"/>\n",
       "    </g>\n",
       "    <defs>\n",
       "        <clipPath id=\"clip0_4338_178347\">\n",
       "            <rect width=\"566.93\" height=\"223.75\" fill=\"white\"/>\n",
       "        </clipPath>\n",
       "    </defs>\n",
       "  </svg>\n",
       "</div>\n",
       "\n",
       "        <table class=\"jp-RenderedHTMLCommon\" style=\"border-collapse: collapse;color: var(--jp-ui-font-color1);font-size: var(--jp-ui-font-size1);\">\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>3.10.14</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "        <td style=\"text-align: left\"><b>2.10.0</b></td>\n",
       "    </tr>\n",
       "    \n",
       "</table>\n",
       "\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.14', ray_version='2.10.0', ray_commit='09abba26b5bf2707639bb637c208d062a47b46f6')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m [2024-09-04 20:18:00,199] p928 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m [2024-09-04 20:18:00,262] p928 {4021940080.py:12} INFO - going to convert img/b64_images/ml-best-practices-healthcare-life-sciences_page_2.b64 into embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m file_path: img/b64_images/ml-best-practices-healthcare-life-sciences_page_1.b64, image description (prefiltered with entities extracted): The image does not contain any specific named entities like person names, organizations, locations, or dates. However, it does include the following data/metric entities and custom entities related to the healthcare and life sciences domain:\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Data/Metric Entities:\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m - Machine Learning Best Practices\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Custom Entities:\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m - Healthcare\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m - Life Sciences\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m The image is the cover page or title slide of what appears to be an AWS whitepaper specifically discussing machine learning best practices for the healthcare and life sciences industries.The image appears to be the cover page or title page of an AWS whitepaper on 'Machine Learning Best Practices in Healthcare and Life Sciences'. There is no detailed data or visual elements present other than the title text and some minimalistic graphical elements depicting shapes and lines.\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Upper Right: This portion of the image is blank, with no text or visual elements.\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Upper Left: This is the main section containing the title text 'AWS Whitepaper' and the main title 'Machine Learning Best Practices in Healthcare and Life Sciences'. The text is in a simple, clean font against a white background.\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Lower Right: This area contains some minimalistic graphical elements in a light gray color. They appear to be abstract shapes and lines, likely used as a design element.\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Lower Left: This section includes the copyright statement 'Copyright Â© 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.' in small text against the white background.\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m \n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m Overall, the image has a very minimalistic and clean design, with the main focus on the title text. There are no charts, tables, or other data-heavy visual elements present.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m [2024-09-04 20:18:17,141] p929 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m [2024-09-04 20:18:00,384] p927 {4021940080.py:12} INFO - going to convert img/b64_images/ml-best-practices-healthcare-life-sciences_page_3.b64 into embeddings\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m [2024-09-04 20:18:17,224] p929 {4021940080.py:51} INFO - Ingesting data into pipeline\n",
      "\u001b[36m(async_process_image_data pid=929)\u001b[0m [2024-09-04 20:18:17,224] p929 {4021940080.py:52} INFO - image desc: 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m file_path: img/b64_images/ml-best-practices-healthcare-life-sciences_page_2.b64, image description (prefiltered with entities extracted): The image does not contain any specific named entities like people's names, organizations, locations or dates. However, there are a few potential data entities or custom entities present in the text:\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m 1. 'Machine Learning Best Practices in Healthcare and Life Sciences'\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m 2. 'AWS Whitepaper'\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m 3. 'Amazon Web Services, Inc.'\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m 4. Potential custom entities related to legal/trademark terminology: 'trademarks', 'trade dress', 'product or service', 'customers', 'disparages or discredits'\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m The text appears to be legal/copyright notice related to an AWS (Amazon Web Services) whitepaper on machine learning best practices for the healthcare and life sciences domains. No other clearly identifiable entities are depicted in this particular image snippet.The image appears to be the cover page or title page of an AWS whitepaper titled 'Machine Learning Best Practices in Healthcare and Life Sciences: AWS Whitepaper'. The layout is relatively simple, with the title taking up the top portion and the bottom portion containing copyright information and legal disclaimers.\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Upper Right: This portion is blank, containing no text or visual elements.\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Upper Left: This area displays the title of the whitepaper, which is 'Machine Learning Best Practices in Healthcare and Life Sciences' and specifies that it is an 'AWS Whitepaper'. The text is in a large, bold font and appears to be the main focus of the image.\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Lower Right: This portion is also blank, with no text or visual elements present.\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Lower Left: This section contains the copyright notice and legal disclaimers. The text reads:\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m 'Copyright Â© 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Amazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.'\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m Overall, the image appears to be a simple title page or cover for an AWS whitepaper focused on machine learning best practices in the healthcare and life sciences domains. No tables, charts, or other visual elements are present aside from the text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m [2024-09-04 20:18:24,052] p927 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m [2024-09-04 20:18:18,817] p928 {4021940080.py:51} INFO - Ingesting data into pipeline\n",
      "\u001b[36m(async_process_image_data pid=928)\u001b[0m [2024-09-04 20:18:18,817] p928 {4021940080.py:52} INFO - image desc: 200 OK\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m [2024-09-04 20:18:24,087] p927 {4021940080.py:51} INFO - Ingesting data into pipeline\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m [2024-09-04 20:18:24,088] p927 {4021940080.py:52} INFO - image desc: 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m file_path: img/b64_images/ml-best-practices-healthcare-life-sciences_page_3.b64, image description (prefiltered with entities extracted): Based on the image provided, which appears to be a table of contents from a whitepaper on machine learning best practices in healthcare and life sciences, here are the relevant entities I can identify:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Named Entities:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - AWS (likely referring to Amazon Web Services)\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Data Entities:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Machine learning\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Life sciences\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Benefits of machine learning\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Life sciences at AWS\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Current regulatory situation\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - AI/ML enabled GxP workloads\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - GXP-compliant machine learning environment\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Machine learning lifecycle\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - ML lifecycle stages\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Data collection\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Data integration and preparation\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Feature engineering\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Model training\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Model validation\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Auditability\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Traceability\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Reproducibility\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Model interpretability\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Model monitoring\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - AI/ML workloads\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Training pipeline\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Inference pipeline\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - Orchestration\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m - SageMaker jobs\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The table of contents does not seem to contain any specific person names, locations, organizations (other than AWS), or precise dates that would qualify as named entities. The entities listed are primarily related to technical topics, processes, and components around machine learning in the healthcare and life sciences domain.The image appears to be a table of contents from an AWS whitepaper titled 'Machine Learning Best Practices in Healthcare and Life Sciences.' The overall layout is in a structured tabular format, with multiple levels of headings and subheadings organized in an outline-style hierarchy.\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Upper Left:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The upper-left portion contains the main title 'Table of Contents' and the first few top-level sections:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 1. 'Abstract and introduction'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 2. 'Benefits of machine learning'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m    - This section has subsections: 'Life sciences at AWS', 'Current regulatory situation', and 'Challenges to support AI/ML enabled GxP workloads'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Upper Right:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The upper-right portion continues with the next top-level section:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 3. 'Provision a secure and GxP-compliant machine learning environment'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m    - This section has subsections: 'Platform qualification', 'Compute and network isolation', 'Authentication and authorization', and 'Data encryption'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Lower Left:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The lower-left portion covers the following top-level section:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 4. 'Machine learning lifecycle'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m    - This section has subsections: 'Phase 1', 'Phase 2', 'Phase 3', 'Phase 4', and 'Phase 5'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m Lower Right:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The lower-right portion includes the next top-level sections:\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 5. 'Best practices for ML lifecycle stages'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m    - This section has subsections: 'Data collection', 'Data integration and preparation', 'Feature engineering', 'Model training', 'Model validation', 'Auditability', 'Traceability', 'Reproducibility', 'Model interpretability', and 'Model monitoring'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 6. 'Operationalize AI/ML workloads'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m 7. 'Reference architectures'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m    - This section has subsections: 'Training pipeline', 'Inference pipeline', 'Orchestration', and 'Orchestration for SageMaker jobs'\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m The table of contents provides an organized overview of the various topics and subtopics covered in the whitepaper, allowing readers to quickly navigate to specific sections of interest.\n",
      "\u001b[36m(async_process_image_data pid=927)\u001b[0m \u001b[32m [repeated 21x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21dcac8a-e241-4710-8499-0e834cf2df67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7153438-f58d-4f63-b04e-1f8a5a02fc71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:58,775] p596 {1465141196.py:3} INFO - config file -> {\n",
      "  \"aws\": {\n",
      "    \"cfn_stack_name\": \"multimodal-blog4-stack\",\n",
      "    \"os_service\": \"aoss\"\n",
      "  },\n",
      "  \"dir_info\": {\n",
      "    \"source_dir\": \"data\",\n",
      "    \"metrics_dir_name\": \"metrics\",\n",
      "    \"img_path\": \"images\",\n",
      "    \"txt_path\": \"text_files\",\n",
      "    \"extracted_data\": \"extracted_data\",\n",
      "    \"json_img_dir\": \"img_json_dir\",\n",
      "    \"json_txt_dir\": \"text_json_dir\",\n",
      "    \"manually_saved_images_path\": \"manually_saved_imgs\",\n",
      "    \"prompt_dir\": \"prompt_templates\",\n",
      "    \"image_description_prompt\": \"image_description_prompt.txt\",\n",
      "    \"search_in_images_template\": \"retrieve_answer_from_images_prompt.txt\",\n",
      "    \"search_in_text_template\": \"retrieve_answer_from_texts_prompt.txt\",\n",
      "    \"extract_entities_from_user_question\": \"extract_question_entities_prompt.txt\",\n",
      "    \"final_combined_llm_response_prompt\": \"final_combined_response_prompt_template.txt\",\n",
      "    \"final_llm_as_a_judge_summary_analysis\": \"claude_final_summary_analysis_prompt.txt\",\n",
      "    \"extract_image_entities_template\": \"extract_image_entities_prompt_template.txt\",\n",
      "    \"eval_prompt_template\": \"claude_eval_template.txt\",\n",
      "    \"processed_prompts_for_eval\": \"processed_llm_judge_evaluation_prompts.csv\",\n",
      "    \"judge_model_eval_completions\": \"model_eval_completions\",\n",
      "    \"llm_as_a_judge_completions\": \"llm_as_a_judge_completions.csv\",\n",
      "    \"index_response_distribution\": \"llm_strategy_pick_rate.csv\",\n",
      "    \"all_explanations\": \"all_explanations.txt\",\n",
      "    \"final_summary_analysis\": \"final_summary_analysis.txt\",\n",
      "    \"p95_metrics_file\": \"p95_summary_metrics.txt\",\n",
      "    \"eval_score_dataset\": \"quantitative_eval_metrics.csv\"\n",
      "  },\n",
      "  \"run_steps\": {\n",
      "    \"1_data_prep_files.ipynb\": true,\n",
      "    \"2_data_ingestion.ipynb\": true,\n",
      "    \"3_rag_inference.ipynb\": true,\n",
      "    \"4_rag_evaluation.ipynb\": true,\n",
      "    \"5_cleanup.ipynb\": false\n",
      "  },\n",
      "  \"page_split_imgs\": {\n",
      "    \"horizontal_split\": false,\n",
      "    \"vertical_split\": false,\n",
      "    \"image_scale\": 3\n",
      "  },\n",
      "  \"content_info\": {\n",
      "    \"local_files\": [\n",
      "      \"ml-best-practices-healthcare-life-sciences.pdf\"\n",
      "    ]\n",
      "  },\n",
      "  \"inference_info\": {\n",
      "    \"parallel_inference_count\": 10,\n",
      "    \"minimum_entities_to_match_from_question\": 2\n",
      "  },\n",
      "  \"model_info\": {\n",
      "    \"inference_model_info\": {\n",
      "      \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
      "      \"input_token_price\": 0.003,\n",
      "      \"output_token_price\": 0.015\n",
      "    },\n",
      "    \"eval_model_info\": {\n",
      "      \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
      "      \"input_tokens_price\": 0.003,\n",
      "      \"output_tokens_price\": 0.015\n",
      "    },\n",
      "    \"embeddings_model_info\": {\n",
      "      \"model_id\": \"amazon.titan-embed-text-v1\",\n",
      "      \"max_text_len_for_embedding\": 500,\n",
      "      \"rouge_metric_selection\": \"rougeL\"\n",
      "    },\n",
      "    \"final_analysis_llm_summarizer\": {\n",
      "      \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
      "    }\n",
      "  },\n",
      "  \"inference_parameters\": {\n",
      "    \"temperature\": 0.1,\n",
      "    \"max_tokens\": 500\n",
      "  },\n",
      "  \"other_inference_and_eval_metrics\": {\n",
      "    \"rouge_metric_selection\": \"rougeL\",\n",
      "    \"k_count_retrieval\": 4\n",
      "  },\n",
      "  \"eval_qna_dataset_info\": {\n",
      "    \"dir_name\": \"eval_data\",\n",
      "    \"eval_dataset_name\": \"dummy_data_file.csv\",\n",
      "    \"question_key\": \"Query\",\n",
      "    \"target_response_key\": \"Response\",\n",
      "    \"updated_eval_file\": \"updated_eval_dataset.csv\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load the merged config file - user config file, and parent config file\n",
    "config = load_and_merge_configs(g.CONFIG_SUBSET_FILE, g.FULL_CONFIG_FILE)\n",
    "logger.info(f\"config file -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e6c2cd-a79b-4e21-837f-9e37a9077689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region: str = boto3.Session().region_name\n",
    "claude_model_id: str = config['model_info']['inference_model_info'].get('model_id')\n",
    "endpoint_url: str = g.BEDROCK_EP_URL.format(region=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcf9ddf2-d8f5-404a-8fcf-a5efa3f3fad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:58,924] p596 {292065259.py:2} INFO - Bucket name being used to store extracted images and texts from data: multimodal-rag-poc-bucket-759878486861-us-east-1\n"
     ]
    }
   ],
   "source": [
    "bucket_name: str = get_bucket_name(config['aws']['cfn_stack_name'])\n",
    "logger.info(f\"Bucket name being used to store extracted images and texts from data: {bucket_name}\")\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d273a8-4d5a-4ede-bc3f-b7d9c8fe6825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "sm_client = sagemaker_session.sagemaker_client\n",
    "sm_runtime_client = sagemaker_session.sagemaker_runtime_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88ba8ccd-7e5e-48c5-93fd-2a98319f33c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,414] p596 {2042488222.py:5} INFO - opensearchhost=mdqhpj5uy4izw0n0czf9.us-east-1.aoss.amazonaws.com, text index=texts, image index=images\n"
     ]
    }
   ],
   "source": [
    "outputs = get_cfn_outputs(config['aws']['cfn_stack_name'])\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "text_index_name = outputs['OpenSearchTextIndexName']\n",
    "img_index_name = outputs['OpenSearchImgIndexName']\n",
    "logger.info(f\"opensearchhost={host}, text index={text_index_name}, image index={img_index_name}\")\n",
    "osi_text_endpoint = f\"https://{outputs['OpenSearchPipelineTextEndpoint']}/data/ingest\"\n",
    "osi_img_endpoint = f\"https://{outputs['OpenSearchPipelineImgEndpoint']}/data/ingest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9213968-f87b-4e19-9d4d-99d0ab8f19d6",
   "metadata": {},
   "source": [
    "#### We use the OpenSearch client to create an index.\n",
    "---\n",
    "For the purpose of segregation and ease of understanding, we are initializing two opensearch clients (for each image and text index). You can create/use just one index too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95923df7-fe11-4ace-ba2a-f56edf0ac1d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,448] p596 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, g.OS_SERVICE)\n",
    "\n",
    "# Represents the OSI client for images\n",
    "img_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "# Represents the OSI client for texts\n",
    "text_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f46cae-f11e-4ed2-9574-438c87a1d932",
   "metadata": {},
   "source": [
    "#### Index Body\n",
    "---\n",
    "Given below is the index body that is stored in the opensearch service. It contains information about:\n",
    "\n",
    "1. **File path**: The path of the text or image file in the index\n",
    "\n",
    "1. **File text**: The texts extracted from the pdf files (for the text index) or the image descriptions for images that are stored in the image index\n",
    "\n",
    "1. **Page number**: Represents the page number that the content is stemming from\n",
    "\n",
    "1. **Metadata**: This field within the index body contains information about the name of the file and entities. Entities represent names of organizations, people, and other important within the pdf text/image that is extracted and stored as metadata for future prefilter purposes to only get relevant documents during the process of search for relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1921892a-e599-490b-bd91-accc5cdb5f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,526] p596 {base.py:258} INFO - HEAD https://mdqhpj5uy4izw0n0czf9.us-east-1.aoss.amazonaws.com:443/images [status:200 request:0.068s]\n",
      "[2024-09-04 20:17:59,527] p596 {720552406.py:50} INFO - The image index 'images' already exists.\n",
      "[2024-09-04 20:17:59,590] p596 {base.py:258} INFO - HEAD https://mdqhpj5uy4izw0n0czf9.us-east-1.aoss.amazonaws.com:443/texts [status:200 request:0.062s]\n",
      "[2024-09-04 20:17:59,591] p596 {720552406.py:57} INFO - The text index 'texts' already exists.\n"
     ]
    }
   ],
   "source": [
    "index_body = \"\"\"\n",
    "{\n",
    "  \"settings\": {\n",
    "    \"index.knn\": true\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"vector_embedding\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1536,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"parameters\": {}\n",
    "        }\n",
    "      },\n",
    "      \"file_path\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"file_text\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"page_number\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"metadata\": {\n",
    "        \"properties\": {\n",
    "          \"filename\": {\n",
    "            \"type\": \"text\"\n",
    "          },\n",
    "          \"entities\": {\n",
    "            \"type\": \"keyword\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# We would get an index already exists exception if the index already exists, and that is fine\n",
    "index_body = json.loads(index_body)\n",
    "try:\n",
    "    # Check if the image index exists\n",
    "    if not img_os_client.indices.exists(img_index_name):\n",
    "        img_response = img_os_client.indices.create(img_index_name, body=index_body)\n",
    "        logger.info(f\"Response received for the create index for images -> {img_response}\")\n",
    "    else:\n",
    "        logger.info(f\"The image index '{img_index_name}' already exists.\")\n",
    "\n",
    "    # Check if the text index exists\n",
    "    if not text_os_client.indices.exists(text_index_name):\n",
    "        txt_response = text_os_client.indices.create(text_index_name, body=index_body)\n",
    "        logger.info(f\"Response received for the create index for texts -> {txt_response}\")\n",
    "    else:\n",
    "        logger.info(f\"The text index '{text_index_name}' already exists.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in creating index, exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d693976-9a99-41f7-ba48-4417fbe0a83b",
   "metadata": {},
   "source": [
    "### Check if the the index created has a `knn`/vector field count before the embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba4934d3-08f8-49c0-8e51-a0c447a078c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,636] p596 {base.py:258} INFO - GET https://mdqhpj5uy4izw0n0czf9.us-east-1.aoss.amazonaws.com:443/texts/_mapping [status:200 request:0.034s]\n",
      "[2024-09-04 20:17:59,671] p596 {base.py:258} INFO - GET https://mdqhpj5uy4izw0n0czf9.us-east-1.aoss.amazonaws.com:443/images/_mapping [status:200 request:0.034s]\n",
      "[2024-09-04 20:17:59,673] p596 {2002334925.py:18} ERROR - Error in fetching the index vector field mapping, exception: The vector_embedding type is not 'knn_vector', found: float\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    # Fetch the existing mapping for the text index\n",
    "    text_mapping = text_os_client.indices.get_mapping(index=text_index_name)\n",
    "    img_mapping = img_os_client.indices.get_mapping(index=img_index_name)\n",
    "    text_vector_embedding_mapping = text_mapping[text_index_name]['mappings']['properties'].get('vector_embedding', {})\n",
    "    img_vector_embedding_mapping = img_mapping[img_index_name]['mappings']['properties'].get('vector_embedding', {})\n",
    "\n",
    "    if text_vector_embedding_mapping.get('type') == 'knn_vector':\n",
    "        logger.info(f\"The vector_embedding type is found: {text_vector_embedding_mapping.get('type')} -> {text_mapping}\")\n",
    "    else:\n",
    "        raise ValueError(f\"The vector_embedding type is not 'knn_vector', found: {text_vector_embedding_mapping.get('type')}\")\n",
    "\n",
    "    if img_vector_embedding_mapping.get('type') == 'knn_vector':\n",
    "        logger.info(f\"The vector_embedding type is found: {img_vector_embedding_mapping.get('type')} -> {img_mapping}\")\n",
    "    else:\n",
    "        raise ValueError(f\"The vector_embedding type is not 'knn_vector', found: {img_vector_embedding_mapping.get('type')}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in fetching the index vector field mapping, exception: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369078c2-94b1-4510-940a-78e6b029e506",
   "metadata": {},
   "source": [
    "## Step 2. Download the images files from S3 and convert to Base64\n",
    "\n",
    "Now we download the image files from the S3 bucket into the `local directory`. Once downloaded these files are converted into [Base64](https://en.wikipedia.org/wiki/Base64) encoding so that we can create embeddings from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d0ec15-3aca-41d9-a4b1-4b6dde4a2f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,728] p596 {186159442.py:10} ERROR - Cannot download the images files from S3 into the local directory: 'Contents'\n"
     ]
    }
   ],
   "source": [
    "# download the images from s3 into a local directory to convert into base64 images\n",
    "os.makedirs(g.LOCAL_IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(g.LOCAL_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    image_files: List = download_image_files_from_s3(bucket_name, g.BUCKET_IMG_PREFIX, g.LOCAL_IMAGE_DIR, g.IMAGE_FILE_EXTN)\n",
    "    text_files: List = download_image_files_from_s3(bucket_name, g.BUCKET_TEXT_PREFIX, g.LOCAL_TEXT_DIR, g.TEXT_FILE_EXTN)\n",
    "    logger.info(f\"downloaded {len(image_files) + len(text_files)} files from s3\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Cannot download the images files from S3 into the local directory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70833d4-b585-477b-93ec-5556c994b9ba",
   "metadata": {},
   "source": [
    "#### Convert jpg files fetched from `S3` into `Base64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e2c8f80-d457-46d5-9d20-f1ca8ebe106b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_file_path: str) -> str:\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        b64_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        b64_image_path = os.path.join(g.B64_ENCODED_IMAGES_DIR, f\"{Path(image_file_path).stem}.b64\")\n",
    "        with open(b64_image_path, \"wb\") as b64_image_file:\n",
    "            b64_image_file.write(bytes(b64_image, 'utf-8'))\n",
    "    return b64_image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d70875-7b8b-44ff-bb5a-0f8b9e43adca",
   "metadata": {},
   "source": [
    "## Step 3. Get embeddings for the base64 encoded images\n",
    "\n",
    "Now we are ready to use Amazon Bedrock via the  Anthropicâ€™s Claude 3 Sonnet foundation model and Amazon Titan Text Embeddings model to convert the base64 version of the images into embeddings. We ingest embeddings into the pipeline using the [requests](https://pypi.org/project/requests/) HTTP library\n",
    "\n",
    "You must sign all HTTP requests to the pipeline using [Signature Version 4](https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aef0ed32-9e7f-448b-93d5-ef0e65404f8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_desc(image_file_path: str, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function uses a base64 file path of an image, and then uses ClaudeV3 Sonnet to \n",
    "    describe the image\n",
    "    \"\"\"\n",
    "    bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)\n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    with open(image_file_path, \"rb\") as image_file:\n",
    "        input_image_b64 = image_file.read().decode('utf-8')\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1000,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/jpeg\",\n",
    "                                \"data\": input_image_b64\n",
    "                            },\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=claude_model_id,\n",
    "        body=body\n",
    "    )\n",
    "\n",
    "    resp_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "    resp_text = resp_body['content'][0]['text'].replace('\"', \"'\")\n",
    "    return resp_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf7db4-5d1f-4b12-99ec-a5484cc55036",
   "metadata": {},
   "source": [
    "### Use the image files downloaded from S3, and convert them into `Base64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b67111d9-359d-46fe-997e-cbb8e5f7992f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,754] p596 {147230456.py:4} INFO - there are 3 pdf image files in the img directory for conversion to base64\n",
      "[2024-09-04 20:17:59,759] p596 {147230456.py:10} INFO - base64 conversion done, there are 3 base64 encoded files\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(g.B64_ENCODED_IMAGES_DIR, exist_ok=True)\n",
    "try:\n",
    "    file_list: List = glob.glob(os.path.join(g.LOCAL_IMAGE_DIR, f\"*{g.IMAGE_FILE_EXTN}\"))\n",
    "    logger.info(f\"there are {len(file_list)} pdf image files in the {g.IMAGE_DIR} directory for conversion to base64\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Could not list any {g.IMAGE_FILE_EXTN} files from {g.IMAGE_DIR}: {e}\")\n",
    "\n",
    "# convert each file to base64 and store the base64 in a new file\n",
    "b64_image_file_list = list(map(encode_image_to_base64, file_list))\n",
    "logger.info(f\"base64 conversion done, there are {len(b64_image_file_list)} base64 encoded files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1431afe-11b1-4c88-bee1-78e9aeac0f8e",
   "metadata": {},
   "source": [
    "### Get Image Descriptions\n",
    "---\n",
    "\n",
    "This part of the notebook uses an `image_description_prompt` to describe the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b42fa6d7-1291-4fed-a75a-cb6f225208bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Please provide a detailed description of the image. Describe the overall layout and design of the image. Identify and describe any tables, charts, or other visual elements present, including the specific data or information contained within them. Provide as much detail as possible about the content and format of the image. Your response should be extremely detailed and data oriented. Give the description for all four portions of the image, the upper right, upper left, lower right and lower left and include all key points data in each if possible. Be completely accurate.\n",
      "\n",
      "Assistant: \n"
     ]
    }
   ],
   "source": [
    "# this is the prompt to get the description of each image stored from the pdf file\n",
    "image_description_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['image_description_prompt'])\n",
    "image_desc_prompt: str = Path(image_description_prompt_fpath).read_text()\n",
    "print(image_desc_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fe56b3-b7e6-4012-b432-18289085ec55",
   "metadata": {},
   "source": [
    "### Hybrid Search: Extract `Entities` from the images for further `prefiltering` tasks\n",
    "---\n",
    "\n",
    "The purpose of using Hybrid search is to optimize the RAG workflow in retrieving the right image description for specific questions. Some images (full or split in different parts), might not contain the information that is being asked by the question, because of the surrounding embeddings in the vector DB and might fetch the wrong image if it has a similar structure, so Hybrid search helps optimizing that. In this case, we will extract the entities of an image description (including the file name to be precise), then extract the entities of the question being asked, to get the most accurate response possible. `Entities` will help match the question to the correct and most relevant documents in the vector index where the answer can searched for in another sub step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51d4d276-37ca-49b8-9787-020c31b8bb71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Please provide a detailed description of the entities present in the image. Entities, are specific pieces of information or objects within a text that carry particular significance. These can be real-world entities like names of people, places, organizations, or dates. Refer to the types of entities: Named entities: These include names of people, organizations, locations, and dates. You can have specific identifiers within this, such as person names or person occupations.\n",
      "\n",
      "Custom entities: These are entities specific to a particular application or domain, such as product names, medical terms, or technical jargon.\n",
      "\n",
      "Temporal entities: These are entities related to time, such as dates, times, and durations.\n",
      "\n",
      "Product entities: Names of products might be grouped together into product entities.\n",
      "\n",
      "Data entities: Names of the data and metrics present. This includes names of metrics in charts, graphs and tables, and throughout the image.\n",
      "\n",
      "Now based on the image, create a list of these entities. Your response should be accurate. Do not make up an answer.\n",
      "\n",
      "Assistant:\n"
     ]
    }
   ],
   "source": [
    "# prompt is used to extract entities from an image\n",
    "entity_extraction_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['extract_image_entities_template'])\n",
    "entity_extraction_prompt: str = Path(entity_extraction_prompt_fpath).read_text()\n",
    "print(entity_extraction_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b8685-58b6-42a2-b826-e243cac2b781",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 1: Loop through b64 images to 1/get image desc from Claude3, 2/get embedding from Titan text. Call OSI pipeline API to ingest embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0326f1-5d73-48d0-90ce-db151918631c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_img_txt_embeddings(bedrock: botocore.client, prompt_data: str) -> np.ndarray:\n",
    "    body = json.dumps({\n",
    "        \"inputText\": prompt_data,\n",
    "    })\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "            body=body, modelId=config['model_info']['embeddings_model_info'].get('model_id'), \n",
    "            accept=\"application/json\", contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        embedding = response_body.get('embedding')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        embedding = None\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c71f3fd2-303d-4a58-b707-5a6e8b731e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to get the image description and store the embeddings of that text in the image index\n",
    "def process_image_data(i: int, \n",
    "                       file_path: str, \n",
    "                       osi_endpoint, \n",
    "                       total: int, \n",
    "                       bucket_info: int) -> Dict:\n",
    "    bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=region, endpoint_url=endpoint_url)\n",
    "    json_data: Optional[Dict] = None\n",
    "    # name of the images that are saved (either split in 4 ways or saved as a single page)\n",
    "    image_name: Optional[str] = None\n",
    "    try:\n",
    "        logger.info(f\"going to convert {file_path} into embeddings\")\n",
    "        # first, get the entities from the image to prefilter the image description with the entities\n",
    "        entities_extracted = get_img_desc(file_path, entity_extraction_prompt)\n",
    "        # get the image description and prepend the image description with the entities extracted from the image\n",
    "        content_description = entities_extracted + get_img_desc(file_path, image_desc_prompt)\n",
    "        print(f\"file_path: {file_path}, image description (prefiltered with entities extracted): {content_description}\")\n",
    "        embedding = get_img_txt_embeddings(bedrock, content_description)\n",
    "        input_image_s3: str = f\"s3://{bucket_name}/{bucket_info['img_prefix']}/{Path(file_path).stem}{bucket_info['image_file_extn']}\"\n",
    "        obj_name: str = f\"{Path(file_path).stem}{bucket_info['image_file_extn']}\"\n",
    "        # data format for POSTING it to the osi_endpoint\n",
    "        data = json.dumps([{\n",
    "            \"file_path\": input_image_s3,\n",
    "            \"file_text\": content_description,\n",
    "            \"page_number\": re.search(r\"page_(\\d+)_?\", obj_name).group(1),\n",
    "            \"metadata\": {\n",
    "                \"filename\": obj_name,\n",
    "                \"entities\": entities_extracted\n",
    "            },\n",
    "            \"vector_embedding\": embedding\n",
    "        }])\n",
    "        # json data format for local files that are saved\n",
    "        json_data = {\n",
    "            \"file_type\": bucket_info['image_file_extn'],\n",
    "            \"file_name\": obj_name,\n",
    "            \"text\": content_description,\n",
    "            \"entities\": entities_extracted,\n",
    "            \"page_number\": re.search(r\"page_(\\d+)_?\", obj_name).group(1)\n",
    "            }\n",
    "        # save the information (image description, entities, file type, name, and page number)\n",
    "        # locally in a json file\n",
    "        image_dir: str = config['dir_info']['json_img_dir']\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        fpath = os.path.join(image_dir, f\"{Path(file_path).stem}.json\")\n",
    "        Path(fpath).write_text(json.dumps(json_data, default=str, indent=2))\n",
    "        r = requests.request(\n",
    "            method='POST', \n",
    "            url=osi_endpoint, \n",
    "            data=data,\n",
    "            auth=AWSSigV4('osis'))\n",
    "        logger.info(\"Ingesting data into pipeline\")\n",
    "        logger.info(f\"image desc: {r.text}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing image {file_path}: {e}\")\n",
    "        json_data: Optional[Dict] = None\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "272e4057-a30e-4b88-a1e7-6c55986f8b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def async_process_image_data(i: int, file_path: str, osi_endpoint, total: int, bucket_info: Dict):\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return process_image_data(i, file_path, osi_endpoint, total, bucket_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f4fc2bc-d4aa-4fb7-9854-f90a35dc26b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "img_entity_df = pd.DataFrame(columns=['file_name', 'text', 'entities'])\n",
    "txt_entity_df = pd.DataFrame(columns=['file_name', 'text', 'entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b937d157-f346-499f-8ef1-79686f2f28e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:17:59,836] p596 {1846079779.py:12} INFO - ------ getting text description for chunk 0/1 -----\n",
      "[2024-09-04 20:17:59,837] p596 {1846079779.py:14} INFO - getting inference for list 1/1, size of list=3 \n",
      "[2024-09-04 20:18:24,093] p596 {1846079779.py:17} INFO - ------ completed chunk=0/1 completed in 24.257364138000412 ------ \n",
      "[2024-09-04 20:18:24,097] p596 {1846079779.py:18} INFO - The results are: [{'file_type': '.jpg', 'file_name': 'ml-best-practices-healthcare-life-sciences_page_3.jpg', 'text': \"Based on the image provided, which appears to be a table of contents from a whitepaper on machine learning best practices in healthcare and life sciences, here are the relevant entities I can identify:\\n\\nNamed Entities:\\n- AWS (likely referring to Amazon Web Services)\\n\\nData Entities:\\n- Machine learning\\n- Life sciences\\n- Benefits of machine learning\\n- Life sciences at AWS\\n- Current regulatory situation\\n- AI/ML enabled GxP workloads\\n- GXP-compliant machine learning environment\\n- Machine learning lifecycle\\n- ML lifecycle stages\\n- Data collection\\n- Data integration and preparation\\n- Feature engineering\\n- Model training\\n- Model validation\\n- Auditability\\n- Traceability\\n- Reproducibility\\n- Model interpretability\\n- Model monitoring\\n- AI/ML workloads\\n- Training pipeline\\n- Inference pipeline\\n- Orchestration\\n- SageMaker jobs\\n\\nThe table of contents does not seem to contain any specific person names, locations, organizations (other than AWS), or precise dates that would qualify as named entities. The entities listed are primarily related to technical topics, processes, and components around machine learning in the healthcare and life sciences domain.The image appears to be a table of contents from an AWS whitepaper titled 'Machine Learning Best Practices in Healthcare and Life Sciences.' The overall layout is in a structured tabular format, with multiple levels of headings and subheadings organized in an outline-style hierarchy.\\n\\nUpper Left:\\nThe upper-left portion contains the main title 'Table of Contents' and the first few top-level sections:\\n\\n1. 'Abstract and introduction'\\n2. 'Benefits of machine learning'\\n   - This section has subsections: 'Life sciences at AWS', 'Current regulatory situation', and 'Challenges to support AI/ML enabled GxP workloads'\\n\\nUpper Right:\\nThe upper-right portion continues with the next top-level section:\\n\\n3. 'Provision a secure and GxP-compliant machine learning environment'\\n   - This section has subsections: 'Platform qualification', 'Compute and network isolation', 'Authentication and authorization', and 'Data encryption'\\n\\nLower Left:\\nThe lower-left portion covers the following top-level section:\\n\\n4. 'Machine learning lifecycle'\\n   - This section has subsections: 'Phase 1', 'Phase 2', 'Phase 3', 'Phase 4', and 'Phase 5'\\n\\nLower Right:\\nThe lower-right portion includes the next top-level sections:\\n\\n5. 'Best practices for ML lifecycle stages'\\n   - This section has subsections: 'Data collection', 'Data integration and preparation', 'Feature engineering', 'Model training', 'Model validation', 'Auditability', 'Traceability', 'Reproducibility', 'Model interpretability', and 'Model monitoring'\\n6. 'Operationalize AI/ML workloads'\\n7. 'Reference architectures'\\n   - This section has subsections: 'Training pipeline', 'Inference pipeline', 'Orchestration', and 'Orchestration for SageMaker jobs'\\n\\nThe table of contents provides an organized overview of the various topics and subtopics covered in the whitepaper, allowing readers to quickly navigate to specific sections of interest.\", 'entities': 'Based on the image provided, which appears to be a table of contents from a whitepaper on machine learning best practices in healthcare and life sciences, here are the relevant entities I can identify:\\n\\nNamed Entities:\\n- AWS (likely referring to Amazon Web Services)\\n\\nData Entities:\\n- Machine learning\\n- Life sciences\\n- Benefits of machine learning\\n- Life sciences at AWS\\n- Current regulatory situation\\n- AI/ML enabled GxP workloads\\n- GXP-compliant machine learning environment\\n- Machine learning lifecycle\\n- ML lifecycle stages\\n- Data collection\\n- Data integration and preparation\\n- Feature engineering\\n- Model training\\n- Model validation\\n- Auditability\\n- Traceability\\n- Reproducibility\\n- Model interpretability\\n- Model monitoring\\n- AI/ML workloads\\n- Training pipeline\\n- Inference pipeline\\n- Orchestration\\n- SageMaker jobs\\n\\nThe table of contents does not seem to contain any specific person names, locations, organizations (other than AWS), or precise dates that would qualify as named entities. The entities listed are primarily related to technical topics, processes, and components around machine learning in the healthcare and life sciences domain.', 'page_number': '3'}, {'file_type': '.jpg', 'file_name': 'ml-best-practices-healthcare-life-sciences_page_1.jpg', 'text': \"The image does not contain any specific named entities like person names, organizations, locations, or dates. However, it does include the following data/metric entities and custom entities related to the healthcare and life sciences domain:\\n\\nData/Metric Entities:\\n- Machine Learning Best Practices\\n\\nCustom Entities:\\n- Healthcare\\n- Life Sciences\\n\\nThe image is the cover page or title slide of what appears to be an AWS whitepaper specifically discussing machine learning best practices for the healthcare and life sciences industries.The image appears to be the cover page or title page of an AWS whitepaper on 'Machine Learning Best Practices in Healthcare and Life Sciences'. There is no detailed data or visual elements present other than the title text and some minimalistic graphical elements depicting shapes and lines.\\n\\nUpper Right: This portion of the image is blank, with no text or visual elements.\\n\\nUpper Left: This is the main section containing the title text 'AWS Whitepaper' and the main title 'Machine Learning Best Practices in Healthcare and Life Sciences'. The text is in a simple, clean font against a white background.\\n\\nLower Right: This area contains some minimalistic graphical elements in a light gray color. They appear to be abstract shapes and lines, likely used as a design element.\\n\\nLower Left: This section includes the copyright statement 'Copyright Â© 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.' in small text against the white background.\\n\\nOverall, the image has a very minimalistic and clean design, with the main focus on the title text. There are no charts, tables, or other data-heavy visual elements present.\", 'entities': 'The image does not contain any specific named entities like person names, organizations, locations, or dates. However, it does include the following data/metric entities and custom entities related to the healthcare and life sciences domain:\\n\\nData/Metric Entities:\\n- Machine Learning Best Practices\\n\\nCustom Entities:\\n- Healthcare\\n- Life Sciences\\n\\nThe image is the cover page or title slide of what appears to be an AWS whitepaper specifically discussing machine learning best practices for the healthcare and life sciences industries.', 'page_number': '1'}, {'file_type': '.jpg', 'file_name': 'ml-best-practices-healthcare-life-sciences_page_2.jpg', 'text': \"The image does not contain any specific named entities like people's names, organizations, locations or dates. However, there are a few potential data entities or custom entities present in the text:\\n\\n1. 'Machine Learning Best Practices in Healthcare and Life Sciences'\\n2. 'AWS Whitepaper'\\n3. 'Amazon Web Services, Inc.'\\n4. Potential custom entities related to legal/trademark terminology: 'trademarks', 'trade dress', 'product or service', 'customers', 'disparages or discredits'\\n\\nThe text appears to be legal/copyright notice related to an AWS (Amazon Web Services) whitepaper on machine learning best practices for the healthcare and life sciences domains. No other clearly identifiable entities are depicted in this particular image snippet.The image appears to be the cover page or title page of an AWS whitepaper titled 'Machine Learning Best Practices in Healthcare and Life Sciences: AWS Whitepaper'. The layout is relatively simple, with the title taking up the top portion and the bottom portion containing copyright information and legal disclaimers.\\n\\nUpper Right: This portion is blank, containing no text or visual elements.\\n\\nUpper Left: This area displays the title of the whitepaper, which is 'Machine Learning Best Practices in Healthcare and Life Sciences' and specifies that it is an 'AWS Whitepaper'. The text is in a large, bold font and appears to be the main focus of the image.\\n\\nLower Right: This portion is also blank, with no text or visual elements present.\\n\\nLower Left: This section contains the copyright notice and legal disclaimers. The text reads:\\n\\n'Copyright Â© 2024 Amazon Web Services, Inc. and/or its affiliates. All rights reserved.\\n\\nAmazon's trademarks and trade dress may not be used in connection with any product or service that is not Amazon's, in any manner that is likely to cause confusion among customers, or in any manner that disparages or discredits Amazon. All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon.'\\n\\nOverall, the image appears to be a simple title page or cover for an AWS whitepaper focused on machine learning best practices in the healthcare and life sciences domains. No tables, charts, or other visual elements are present aside from the text.\", 'entities': \"The image does not contain any specific named entities like people's names, organizations, locations or dates. However, there are a few potential data entities or custom entities present in the text:\\n\\n1. 'Machine Learning Best Practices in Healthcare and Life Sciences'\\n2. 'AWS Whitepaper'\\n3. 'Amazon Web Services, Inc.'\\n4. Potential custom entities related to legal/trademark terminology: 'trademarks', 'trade dress', 'product or service', 'customers', 'disparages or discredits'\\n\\nThe text appears to be legal/copyright notice related to an AWS (Amazon Web Services) whitepaper on machine learning best practices for the healthcare and life sciences domains. No other clearly identifiable entities are depicted in this particular image snippet.\", 'page_number': '2'}]\n",
      "[2024-09-04 20:18:24,109] p596 {1846079779.py:36} INFO - Number of erroneous pdf pages that are not processed: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with img results!\n"
     ]
    }
   ],
   "source": [
    "# count the number of images that throw an error while being saved into the index\n",
    "erroneous_page_count: int = 0\n",
    "n: int = config['inference_info']['parallel_inference_count']\n",
    "image_chunks = [b64_image_file_list[i:i + n] for i in range(0, len(b64_image_file_list), n)]\n",
    "bucket_info: Dict = {\n",
    "    'img_prefix': g.BUCKET_IMG_PREFIX,\n",
    "    'image_file_extn': g.IMAGE_FILE_EXTN\n",
    "}\n",
    "for chunk_index, image_chunk in enumerate(image_chunks):\n",
    "    try:\n",
    "        st = time.perf_counter()\n",
    "        logger.info(f\"------ getting text description for chunk {chunk_index}/{len(image_chunks)} -----\")\n",
    "        # Iterate over each file path in the chunk and process it individually\n",
    "        logger.info(f\"getting inference for list {chunk_index+1}/{len(image_chunks)}, size of list={len(image_chunk)} \")\n",
    "        results = ray.get([async_process_image_data.remote(index, file_path, osi_img_endpoint, len(image_chunk), bucket_info) for index, file_path in enumerate(image_chunk)])\n",
    "        elapsed_time = time.perf_counter() - st\n",
    "        logger.info(f\"------ completed chunk={chunk_index}/{len(image_chunks)} completed in {elapsed_time} ------ \")\n",
    "        logger.info(f\"The results are: {results}\")\n",
    "        # Assuming img_entity_df is already initialized with the correct columns\n",
    "        # Iterate over the list of JSON objects and append to the DataFrame\n",
    "        for json_obj in results:\n",
    "            new_data = {\n",
    "                'file_name': json_obj['file_name'],\n",
    "                'text': json_obj['text'],\n",
    "                'entities': json_obj['entities']\n",
    "            }\n",
    "            img_entity_df.loc[len(img_entity_df)] = new_data\n",
    "        #print(img_entity_df)\n",
    "        # Append the new data to the existing DataFrame\n",
    "        #print(f\"img_entity_df is: {img_entity_df}\")\n",
    "        print(\"Done with img results!\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing chunk {chunk_index}: {e}\")\n",
    "        erroneous_page_count += len(image_chunk)\n",
    "\n",
    "logger.info(f\"Number of erroneous pdf pages that are not processed: {erroneous_page_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8fe93-1cde-4d0f-ae1d-6dd5b3f9040d",
   "metadata": {},
   "source": [
    "### Part 2: Loop through text files to 1/get embedding from Titan text, 2/extract the text entities using `nltk`. Call OSI pipeline API to ingest embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68475cb8-0430-476d-8ae9-b22dad72aa26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:18:24,120] p596 {3290322578.py:6} INFO - Number of text files from the PDF local directory to process: 3\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all text files \n",
    "pdf_txt_file_list = os.listdir(g.LOCAL_TEXT_DIR)\n",
    "\n",
    "# Get absolute file paths by joining the directory path with each file name\n",
    "pdf_txt_file_list = [os.path.abspath(os.path.join(g.LOCAL_TEXT_DIR, file)) for file in pdf_txt_file_list]\n",
    "logger.info(f\"Number of text files from the PDF local directory to process: {len(pdf_txt_file_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc23f-7a18-4ce3-9147-214b05eb56d2",
   "metadata": {},
   "source": [
    "#### Entities extraction from PDF texts using [NLTK]('https://www.nltk.org/')\n",
    "---\n",
    "\n",
    "NLTK is a leading platform for building Python programs to work with human language data. We use `NLTK` to extract entities from the text files that are extracted from each `PDF page`, and use that as a prepend onto the extracted file to be sent to the `OSI endpoint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c50ccdf-70cf-4dee-9ee9-97ef272451e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker') \n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2b546f2-90ad-483c-8670-b47d084e147c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    \"\"\"\n",
    "    This function uses nltk to get the entities from texts that are extracted from pdf files\n",
    "    \"\"\"\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        if current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4b18389-711e-44b3-b06e-f9b50685f63f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text_data(txt_file: str, txt_page_index: int):\n",
    "    with open(txt_file, 'r') as file:\n",
    "        extracted_pdf_text = file.read()\n",
    "    # Extract entities from text using nltk\n",
    "    entities = get_continuous_chunks(extracted_pdf_text)\n",
    "    # Convert the entities list to string \n",
    "    entities_str = \", \".join(entities)\n",
    "    logger.info(f\"entities extracted from {txt_file}: {entities_str}\")\n",
    "    embedding = get_text_embedding(bedrock, extracted_pdf_text)\n",
    "    input_text_s3 = f\"s3://{bucket_name}/{g.BUCKET_TEXT_PREFIX}/{Path(txt_file).stem}{g.TEXT_FILE_EXTN}\"\n",
    "    obj_name = f\"{Path(txt_file).stem}{g.TEXT_FILE_EXTN}\"\n",
    "    # data format that is used to POST to the osi endpoint\n",
    "    data = json.dumps([{\n",
    "        \"file_path\": input_text_s3,\n",
    "        \"file_text\": extracted_pdf_text,\n",
    "        \"page_number\": txt_page_index,\n",
    "        \"metadata\": {\n",
    "            \"filename\": obj_name,\n",
    "            \"entities\": entities_str\n",
    "        },\n",
    "        \"vector_embedding\": embedding\n",
    "    }])\n",
    "    # json data format that is saved in a local directory\n",
    "    json_data = {\n",
    "        \"file_type\": g.TEXT_FILE_EXTN,\n",
    "        \"file_name\": Path(txt_file).stem,\n",
    "        \"text\": extracted_pdf_text, \n",
    "        \"page_number\": re.search(r\"text_(\\d+)_?\", obj_name).group(1),\n",
    "        \"entities\": entities_str  \n",
    "    } \n",
    "    os.makedirs(config['dir_info']['json_txt_dir'], exist_ok=True)\n",
    "    fpath = os.path.join(config['dir_info']['json_txt_dir'], f\"{Path(txt_file).stem}.json\")\n",
    "    print(f\"json_file_path: {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(json_data, default=str, indent=2))\n",
    "    r = requests.request(\n",
    "        method='POST',\n",
    "        url=osi_text_endpoint,\n",
    "        data=data,\n",
    "        auth=AWSSigV4('osis'))\n",
    "\n",
    "    logger.info(\"Ingesting data into pipeline\")\n",
    "    logger.info(f\"Response: {txt_page_index} - {r.text}\")\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9f47e1f-b730-4451-856a-931e3f7e5682",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:18:24,266] p596 {3417922390.py:4} INFO - going to convert /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_1.txt into embeddings\n",
      "[2024-09-04 20:18:24,533] p596 {651913043.py:8} INFO - entities extracted from /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_1.txt: AWS Whitepaper Machine, Healthcare, Life Sciences, Amazon, Inc.\n",
      "[2024-09-04 20:18:24,675] p596 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-09-04 20:18:24,760] p596 {651913043.py:41} INFO - Ingesting data into pipeline\n",
      "[2024-09-04 20:18:24,762] p596 {651913043.py:42} INFO - Response: 1 - 200 OK\n",
      "[2024-09-04 20:18:24,766] p596 {3417922390.py:4} INFO - going to convert /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_3.txt into embeddings\n",
      "[2024-09-04 20:18:24,826] p596 {651913043.py:8} INFO - entities extracted from /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_3.txt: Machine Learning Best, Healthcare, Life Sciences, AWS Whitepaper Table, Contents Abstract, AWS, GxP, Compute, ML, SageMaker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_file_path: text_json_dir/ml-best-practices-healthcare-life-sciences_text_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:18:24,934] p596 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-09-04 20:18:25,000] p596 {651913043.py:41} INFO - Ingesting data into pipeline\n",
      "[2024-09-04 20:18:25,002] p596 {651913043.py:42} INFO - Response: 2 - 200 OK\n",
      "[2024-09-04 20:18:25,017] p596 {3417922390.py:4} INFO - going to convert /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_2.txt into embeddings\n",
      "[2024-09-04 20:18:25,055] p596 {651913043.py:8} INFO - entities extracted from /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/multimodal/local_txts/ml-best-practices-healthcare-life-sciences_text_2.txt: Machine Learning Best, Healthcare, Life Sciences, AWS Whitepaper Machine, Healthcare Life Sciences, AWS Whitepaper, Amazon, Inc., Amazon Amazon, Amazon Amazon Amazon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_file_path: text_json_dir/ml-best-practices-healthcare-life-sciences_text_3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-04 20:18:25,154] p596 {credentials.py:1075} INFO - Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "[2024-09-04 20:18:25,247] p596 {651913043.py:41} INFO - Ingesting data into pipeline\n",
      "[2024-09-04 20:18:25,248] p596 {651913043.py:42} INFO - Response: 3 - 200 OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_file_path: text_json_dir/ml-best-practices-healthcare-life-sciences_text_2.json\n",
      "txt_entity_df is:                                            file_name  \\\n",
      "0  ml-best-practices-healthcare-life-sciences_text_1   \n",
      "1  ml-best-practices-healthcare-life-sciences_text_3   \n",
      "2  ml-best-practices-healthcare-life-sciences_text_2   \n",
      "\n",
      "                                                text  \\\n",
      "0  AWS Whitepaper\\nMachine Learning Best Practice...   \n",
      "1  Machine Learning Best Practices in Healthcare ...   \n",
      "2  Machine Learning Best Practices in Healthcare ...   \n",
      "\n",
      "                                            entities  \n",
      "0  AWS Whitepaper Machine, Healthcare, Life Scien...  \n",
      "1  Machine Learning Best, Healthcare, Life Scienc...  \n",
      "2  Machine Learning Best, Healthcare, Life Scienc...  \n",
      "Done with txt results!\n"
     ]
    }
   ],
   "source": [
    "txt_page_index: int = 1\n",
    "os.makedirs(config['dir_info']['json_txt_dir'], exist_ok=True)\n",
    "for txt_file in pdf_txt_file_list:\n",
    "    logger.info(f\"going to convert {txt_file} into embeddings\")\n",
    "    json_obj = process_text_data(txt_file, txt_page_index)\n",
    "    #print(json_obj)\n",
    "    new_data = {\n",
    "        'file_name': json_obj['file_name'],\n",
    "        'text': json_obj['text'],\n",
    "        'entities': json_obj['entities']\n",
    "    }\n",
    "    txt_entity_df.loc[len(txt_entity_df)] = new_data\n",
    "    txt_page_index += 1\n",
    "\n",
    "#print(f\"txt_entity_df is: {txt_entity_df}\")\n",
    "print(\"Done with txt results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b61213f6-a2dc-46c2-85fa-53eafe8f87b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_pag...</td>\n",
       "      <td>Based on the image provided, which appears to ...</td>\n",
       "      <td>Based on the image provided, which appears to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_pag...</td>\n",
       "      <td>The image does not contain any specific named ...</td>\n",
       "      <td>The image does not contain any specific named ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_pag...</td>\n",
       "      <td>The image does not contain any specific named ...</td>\n",
       "      <td>The image does not contain any specific named ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  ml-best-practices-healthcare-life-sciences_pag...   \n",
       "1  ml-best-practices-healthcare-life-sciences_pag...   \n",
       "2  ml-best-practices-healthcare-life-sciences_pag...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Based on the image provided, which appears to ...   \n",
       "1  The image does not contain any specific named ...   \n",
       "2  The image does not contain any specific named ...   \n",
       "\n",
       "                                            entities  \n",
       "0  Based on the image provided, which appears to ...  \n",
       "1  The image does not contain any specific named ...  \n",
       "2  The image does not contain any specific named ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img CSV file has been saved at: /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/img_entity_data.csv\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "# Define the file name for the CSV\n",
    "csv_file_name = \"img_entity_data.csv\"\n",
    "\n",
    "# Create the full file path\n",
    "csv_file_path = os.path.join(current_directory, csv_file_name)\n",
    "display(img_entity_df)\n",
    "# Save the DataFrame as a CSV file\n",
    "img_entity_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Img CSV file has been saved at: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fb23217-cc08-4393-b0e0-a287b4217b12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_text_1</td>\n",
       "      <td>AWS Whitepaper\\nMachine Learning Best Practice...</td>\n",
       "      <td>AWS Whitepaper Machine, Healthcare, Life Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_text_3</td>\n",
       "      <td>Machine Learning Best Practices in Healthcare ...</td>\n",
       "      <td>Machine Learning Best, Healthcare, Life Scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ml-best-practices-healthcare-life-sciences_text_2</td>\n",
       "      <td>Machine Learning Best Practices in Healthcare ...</td>\n",
       "      <td>Machine Learning Best, Healthcare, Life Scienc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  ml-best-practices-healthcare-life-sciences_text_1   \n",
       "1  ml-best-practices-healthcare-life-sciences_text_3   \n",
       "2  ml-best-practices-healthcare-life-sciences_text_2   \n",
       "\n",
       "                                                text  \\\n",
       "0  AWS Whitepaper\\nMachine Learning Best Practice...   \n",
       "1  Machine Learning Best Practices in Healthcare ...   \n",
       "2  Machine Learning Best Practices in Healthcare ...   \n",
       "\n",
       "                                            entities  \n",
       "0  AWS Whitepaper Machine, Healthcare, Life Scien...  \n",
       "1  Machine Learning Best, Healthcare, Life Scienc...  \n",
       "2  Machine Learning Best, Healthcare, Life Scienc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Txt CSV file has been saved at: /home/ec2-user/SageMaker/multimodal-rag-on-slide-decks/Blog4-PDF-TitanEmbeddings/notebooks/txt_entity_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file name for the CSV\n",
    "txt_csv_file_name = \"txt_entity_data.csv\"\n",
    "\n",
    "# Create the full file path\n",
    "txt_csv_file_path = os.path.join(current_directory, txt_csv_file_name)\n",
    "display(txt_entity_df)\n",
    "# Save the DataFrame as a CSV file\n",
    "txt_entity_df.to_csv(txt_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Txt CSV file has been saved at: {txt_csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
