{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Responses from Image, Text, and Combined Indexes\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook does as follows:\n",
    "\n",
    "1. Use the CSV file generated in the previous notebook to run evaluations on how the response quality is, based on the context, and the target responses that are given\n",
    "\n",
    "1. Record the ROUGE, COSINE, BLEU scores, and for subjective evaluation, use an `LLM as a judge`(in this case, ClaudeV3 Sonnet)\n",
    "\n",
    "1. Record the results for all kinds of responses from text only index, image only index, and combined (from both the text as well as the image index) from `OpenSearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries that are needed to run this notebook\n",
    "import os\n",
    "import re\n",
    "import ray\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import nltk\n",
    "import boto3\n",
    "import codecs\n",
    "import base64\n",
    "import logging\n",
    "import requests\n",
    "import botocore\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from numpy import dot\n",
    "from pathlib import Path\n",
    "from nltk.tag import pos_tag\n",
    "from typing import List, Dict\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion ## support for text generation models on bedrock\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from bedrock_utils import get_bedrock_client\n",
    "from requests_auth_aws_sigv4 import AWSSigV4\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_bucket_name, download_image_files_from_s3, get_text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n",
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get all the responses from the evaluation dataset in a dataframe to calculate ROUGE/COSINE and `LLM as a Judge` evaluation metrics\n",
    "\n",
    "Here, we fetch responses from the image index, image index to get a question bank created that acts as a curated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['metrics_dir']['dir_name'] is not None:\n",
    "    metrics_fpath = os.path.join(config['metrics_dir']['dir_name'], config['eval_qna_dataset_info']['updated_eval_file'])\n",
    "    metric_files = glob.glob(metrics_fpath, recursive=True)\n",
    "    logger.info(f\"there are {len(metric_files)} files in {metrics_fpath}\")\n",
    "    for file in metric_files:\n",
    "        eval_df = pd.read_csv(file) \n",
    "        eval_df = eval_df.loc[:, ~eval_df.columns.str.startswith('Unnamed')]\n",
    "\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE & Cosine Similarity Scores for multimodal completions:\n",
    "---\n",
    "\n",
    "Here, the amazon.titan-embed-text-v1 is used to get the embeddings of texts. To use a different embeddings model, change the model in the embeddings_model_info and modify this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "MAX_TEXT_LEN_FOR_EMBEDDING: int = config['embeddings_model_info']['max_text_len_for_embedding']\n",
    "bedrock: Optional[get_bedrock_client] = None\n",
    "\n",
    "def get_embedding(text: str, modelId: str=config['embeddings_model_info'].get('model'), accept: str='application/json', contentType: str='application/json'):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the responses from the image/text indexes and the target responses if any are\n",
    "    provided in the dataset\n",
    "    \"\"\"\n",
    "    global bedrock\n",
    "    if bedrock is None:\n",
    "        bedrock = get_bedrock_client()\n",
    "    body = json.dumps({\"inputText\": text[:MAX_TEXT_LEN_FOR_EMBEDDING]})\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    token_count = response_body.get('inputTextTokenCount')\n",
    "    return embedding, token_count\n",
    "\n",
    "def get_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between the image/text indexes and the target responses if any\n",
    "    \"\"\"\n",
    "    A,_ = get_embedding(text1)\n",
    "    B,_ = get_embedding(text2)\n",
    "    cosine = dot(A, B)/(norm(A)*norm(B))\n",
    "    return cosine\n",
    "\n",
    "def get_rouge_l_score(completion: str, golden: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the rouge-l score between the image/text indexes and the target responses (if any)\n",
    "    \"\"\"\n",
    "    rouge_metric_selection: str = config['embeddings_model_info']['rouge_metric_selection']\n",
    "    scorer = rouge_scorer.RougeScorer([rouge_metric_selection])\n",
    "    scores = scorer.score(golden, completion)\n",
    "    return round(scores[rouge_metric_selection].fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_response_key: str = config['eval_qna_dataset_info']['target_response_key']\n",
    "def compare_completions(row, index_type):\n",
    "    \"\"\"\n",
    "    Generates the rouge and cosine similarity scores for chapter titles and original titles\n",
    "    \"\"\"\n",
    "    if (row.get(index_type) and row.get(target_response_key) is not None) and (pd.notna(row.get(index_type)) and pd.notna(row.get(target_response_key))):\n",
    "        logger.info(f\"{index_type} response: {row[index_type]}, Target response: {row[target_response_key]}\")\n",
    "        rouge_l_score = get_rouge_l_score(row[index_type], row[target_response_key])\n",
    "        cosine_sim = get_cosine_similarity(row[index_type].lower(), row[target_response_key].lower())\n",
    "        return pd.Series([rouge_l_score, cosine_sim])\n",
    "    else:\n",
    "        logger.info(f'ROUGE, Cosine similarity and Bleu scores cannot be computed since original responses are not provided in the dataset')\n",
    "        rouge_l_score, cosine_sim = None, None\n",
    "\n",
    "if target_response_key in eval_df.columns:\n",
    "    for metric in config['embeddings_model_info']['get_quantitative_metrics_on']:\n",
    "        eval_df[[f'{metric}_rouge_l_f1_score', f'{metric}_cosine_similarity']] = eval_df.apply(lambda row: compare_completions(row, index_type=metric), axis=1)\n",
    "else:\n",
    "    logger.info('No evaluation metrics available since target responses are not provided in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['eval_qna_dataset_info']['target_response_key'] in eval_df.columns:\n",
    "    eval_df.rename(columns = {'Response':'target_response', 'text_response': 'text_only_response', 'img_response': 'img_only_response'}, inplace = True)\n",
    "    eval_df = eval_df.drop(columns=['image_and_text_source', 'text_source', 'img_source'])\n",
    "# Construct the file path\n",
    "metrics_dir: str = config['metrics_dir']['dir_name']\n",
    "rouge_cosine_file_path = os.path.join(metrics_dir, config['metrics_dir']['eval_score_dataset'])\n",
    "\n",
    "eval_df.to_csv(rouge_cosine_file_path, index=False)\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `LLM as a Judge` to evaluate responses from different indexes - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title Evaluation: Using LLM as a Judge in the loop\n",
    "---\n",
    "\n",
    "In this portion:\n",
    "\n",
    "Responses generated by each index and combined are evaluated on relevance and best match by `Claude Sonnet/Your model of choice`. Prompt for the model that acts as a judge in the loop can be viewed in: `eval_template.txt`. Edit and review this prompt based on the use case and criteria for subjective evaluation.\n",
    "\n",
    "The role of the model acting as a judge it to compare the responses generated by each index to a target response. It provides information on the `best selected response`, `response source` (from the text only index/image only index/or both indexes), and an `explanation` of its selection, with an in depth analysis of comparison between other responses and why it chose the one it did. \n",
    "\n",
    "A final evaluation metric is calculated that shows the distribution of the selected responses and their respective response sources. This will give a judgement call of which multimodal strategy to use in production ready workloads.\n",
    "\n",
    "Note: For more information on the use of having a Model act as a judge, view: https://huggingface.co/learn/cookbook/en/llm_judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate different chapter titles and suggest the most suitable title based on the evaluation criteria mentioned in the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(row):\n",
    "    \"\"\"\n",
    "    This function evaluates the prompts by incorporating all of the responses generated by various indexes into the evaluation prompt template.\n",
    "    \"\"\"\n",
    "    # represents the eval template used by the model judge\n",
    "    eval_template: Optional[str] = None\n",
    "    processed_eval_template: Optional[str] = None\n",
    "    candidate_responses: List[str] = []\n",
    "    try:\n",
    "        # file path to the eval template\n",
    "        eval_template_path: str = os.path.join(config['pdf_dir_info']['prompts'], config['eval_qna_dataset_info'].get('prompt_template'))\n",
    "        with open(eval_template_path, \"r\") as f:\n",
    "            eval_template = f.read()\n",
    "            logger.info(f\"evaluation prompt template recorded: {eval_template}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Evaluation template not found at {eval_template_path}\")\n",
    "    logger.info(f\"question: {row['Query']}\")\n",
    "    logger.info(f\"original_response: {row['target_response']}\")\n",
    "    for column in row.index:\n",
    "        if column.endswith(\"_response\") and column != \"target_response\":\n",
    "            response_source = column.split(\"_response\")[0]\n",
    "            candidate_response = row[column]\n",
    "            candidate_responses.append(f\"\\n<{response_source}>\\n{candidate_response}\\n</{response_source}>\\n\")\n",
    "    processed_eval_template = eval_template.format(\n",
    "        question=row['Query'], \n",
    "        original_response=row['target_response'],\n",
    "        candidate_responses=\"\\n\".join(candidate_responses)\n",
    "    )\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add evaluation prompt as a column into a df with respective response index sources and responses to send into the Model for further evaluation in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if eval_df is not None:\n",
    "    eval_df['eval_prompt'] = eval_df.apply(lambda r: prepare_eval_prompts(r), axis=1)\n",
    "    logger.info(\"preparing the evaluation prompt templates for the LLM judge....\")\n",
    "else:\n",
    "    logger.error(f\"Model evaluation dataset is not available to process.\")\n",
    "llm_as_a_judge_eval_df_f_path = os.path.join(metrics_dir, config['pdf_dir_info']['processed_prompts_for_eval'])\n",
    "eval_df.insert(0, 'query_id', eval_df.index)\n",
    "eval_df.to_csv(llm_as_a_judge_eval_df_f_path, index=False)\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `LLM (Claude) as a judge` in the loop to evaluate and narrow down the titles generated by different models of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_judge_json_evaluations(model_id: str, prompt: str):\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception = None,\n",
    "               prompt = prompt,\n",
    "               completion = None,\n",
    "               question = None,\n",
    "               target_response = None, \n",
    "               # initializing to 0 since none type throws an error later, this is used to calculate price per token input/output on ODT pricing\n",
    "               completion_token_count = 0,\n",
    "               # initializing to 0 since none type throws an error later\n",
    "               prompt_token_count=0,\n",
    "               input_token_price = None, \n",
    "               output_token_pricing = None,\n",
    "               model_id = model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    parameters = config['inference_parameters_for_explanations']\n",
    "    temperature = parameters.get('temperature', 0.1)\n",
    "    caching = parameters.get('caching', False)\n",
    "    max_tokens = parameters.get(\"max_tokens\", 500)\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        logger.info(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{ \"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=max_tokens,\n",
    "                              caching=caching)\n",
    "\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    # save all the responses from the model in a dictionary\n",
    "    resp: Dict = {}\n",
    "    print(f\"row={row}\")\n",
    "    logger.info(f\"row {i}/{total}, prompt_template={model_info['prompt_template']}, model_id={model_info['model']}\")\n",
    "    model_id = model_info['model']\n",
    "    # create the payload for model inference\n",
    "    prompt = row['eval_prompt']\n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = llm_judge_json_evaluations(model_id, prompt)\n",
    "    resp['question'] = row['Query']\n",
    "    resp['target_response'] = row['target_response']\n",
    "    # calculate the input and output token price for all of the calls\n",
    "    resp['input_token_price'] = (resp['prompt_token_count']/1000) * model_info['input_tokens_pricing']\n",
    "    resp['output_token_pricing'] = (resp['completion_token_count']/1000) * model_info['output_tokens_pricing']\n",
    "    dir_path = os.path.join(config['pdf_dir_info']['judge_model_eval_completions'], str(row['query_id']), model_id.replace(\":\", \"-\"))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fpath = os.path.join(dir_path, f\"model_evaluation_{row['query_id']}.json\")\n",
    "    logger.info(f\"writing response={resp} to {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(resp, default=str, indent=2))\n",
    "    logger.info(f\"response {i}: {resp}\")\n",
    "    return resp\n",
    "\n",
    "@ray.remote\n",
    "def async_get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return get_inference(i, row, total, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df = json.loads(eval_df.to_json(orient='records'))\n",
    "n: int = config['parallel_inference_count']\n",
    "from typing import List\n",
    "resp_list: List = []\n",
    "model_list = config['eval_model_info']\n",
    "st = time.perf_counter()\n",
    "logger.info(f\"------ running inference for {model_list.get('model')} -----\")\n",
    "list_of_lists = [eval_df[i * n:(i + 1) * n] for i in range((len(eval_df) + n - 1) // n )]\n",
    "logger.info(f\"split input list of size {len(eval_df)} into {len(list_of_lists)} lists\")\n",
    "for idx, l in enumerate(list_of_lists):\n",
    "    logger.info(f\"getting inference for list {idx+1}/{len(list_of_lists)}, size of list={len(l)} \")\n",
    "    resp_list.extend(ray.get([async_get_inference.remote(i+1, e, len(l), model_list) for i, e in enumerate(l)]))\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"------ model={model_list.get('model')} completed in {elapsed_time} ------ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents extracted all metric files\n",
    "fpath_evaluated_files = os.path.join(config['pdf_dir_info']['judge_model_eval_completions'], \"**\", \"*\", \"*.json\")\n",
    "eval_metric_files = glob.glob(fpath_evaluated_files, recursive=True)\n",
    "logger.info(f\"there are {len(eval_metric_files)} evaluated files by {config['eval_model_info']['model']} LLM judge in {fpath_evaluated_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation_responses = []\n",
    "for f in eval_metric_files:\n",
    "    with open(f, 'r') as file:\n",
    "        model_evaluation_responses.append(json.loads(file.read()))\n",
    "# results_df will contain the evaluation responses, including the completion and the model id\n",
    "results_df = pd.DataFrame(model_evaluation_responses)\n",
    "results_df = results_df.drop(columns=['exception', 'prompt'])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_model_eval_json(data):\n",
    "    \"\"\"\n",
    "    This function is to take in json data, and clean it, assign the selected title as outputted by the model evaluator\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = json.loads(data.replace('\\\\', '\\\\\\\\'))\n",
    "        return pd.Series({\n",
    "            'best_match_response': json_data['best_match_response'],\n",
    "            'response_source': json_data['response_source'],\n",
    "            'explanation': json_data['explanation'],\n",
    "        })\n",
    "    except json.JSONDecodeError:\n",
    "        return pd.Series({\n",
    "            'best_match_response': None,\n",
    "            'response_source': None,\n",
    "            'explanation': None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results_df = results_df['completion'].apply(clean_model_eval_json)\n",
    "new_results_df.insert(0, 'question', new_results_df.index)\n",
    "new_results_df.insert(1, 'target_response', new_results_df.index)\n",
    "new_results_df['question'] = results_df['question']\n",
    "new_results_df['target_response'] = results_df['target_response']\n",
    "new_exploded_df = tidy_split(new_results_df, 'response_source', sep=',')\n",
    "new_exploded_df['response_source'] = new_results_df['response_source'].str.replace('<', '').str.replace('>', '')\n",
    "logger.info(f\"All evaluation data is read into a dataframe of shape {results_df.shape}\")\n",
    "processed_prompts_for_eval_path = os.path.join(metrics_dir, config['pdf_dir_info']['llm_as_a_judge_completions'])\n",
    "new_results_df.to_csv(processed_prompts_for_eval_path, index=False)\n",
    "# display the selected title, model explanation and the respective golden title in a side by side view\n",
    "new_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the percentage of each model selection and reset the index\n",
    "new_exploded_df['response_source'] = new_exploded_df['response_source'].map(lambda x: x.strip())\n",
    "response_index_percentage_df = new_exploded_df['response_source'].value_counts(normalize=True).reset_index()\n",
    "response_distribution_fpath = os.path.join(metrics_dir, config['pdf_dir_info']['index_response_distribution'])\n",
    "response_index_percentage_df.rename(columns = {'response_source':'index_pick_rate'}, inplace = True)\n",
    "response_index_percentage_df['index_pick_rate'] *= 100\n",
    "response_index_percentage_df.to_csv(response_distribution_fpath, index=False)\n",
    "response_index_percentage_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
