{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal RAG Evaluation: `ROUGE`, `Cosine` & `LLM as a Judge`\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook does as follows:\n",
    "\n",
    "1. Uses the CSV file generated in the previous notebook to run evaluations on each response from the `image`, `text`, and `combined` indexes.\n",
    "\n",
    "1. Records the `ROUGE` and `Cosine Similarity` scores. For subjective evaluation, this notebook uses an `LLM as a judge`(in this case, ClaudeV3 Sonnet) in the loop to check for the best match answer given the `target response` and the `questions` provided by the user.\n",
    "\n",
    "1. Records the results for all kinds of responses from `text only index`, `image only index`, and `combined` (from both the text as well as the image index) from `OpenSearch`\n",
    "\n",
    "1. Uses `litellm` for interfacing with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install all the requirements\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the libraries that are needed to run this notebook\n",
    "import os\n",
    "import ray\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "import logging\n",
    "import globals as g\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from pathlib import Path\n",
    "from numpy.linalg import norm\n",
    "from litellm import completion ## support for text generation models on bedrock\n",
    "from rouge_score import rouge_scorer\n",
    "from typing import List, Dict, Optional\n",
    "from utils import load_and_merge_configs\n",
    "from bedrock_utils import get_bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the merged config file - user config file, and parent config file\n",
    "config = load_and_merge_configs(g.CONFIG_SUBSET_FILE, g.FULL_CONFIG_FILE)\n",
    "logger.info(f\"config file -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get all the responses from the evaluation dataset in a dataframe to calculate `ROUGE`, `Cosine Similarity` and `LLM as a Judge` evaluation metrics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUERY_COL: str = config['eval_qna_dataset_info']['question_key']\n",
    "region = boto3.Session().region_name \n",
    "QUERY_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the metrics path is not none, then we get the updated evaluation file with responses to \n",
    "# user provided questions from the text/image/combined indexes\n",
    "if config['dir_info']['metrics_dir_name'] is not None:\n",
    "    metrics_fpath: str = os.path.join(config['dir_info']['metrics_dir_name'], config['eval_qna_dataset_info']['updated_eval_file'])\n",
    "    metric_files = glob.glob(metrics_fpath, recursive=True)\n",
    "    logger.info(f\"there are {len(metric_files)} files in {metrics_fpath}\")\n",
    "    for file in metric_files:\n",
    "        eval_df = pd.read_csv(file)\n",
    "        # drop columns that are not needed\n",
    "        eval_df = eval_df.loc[:, ~eval_df.columns.str.startswith('Unnamed')]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the `ROUGE` & `Cosine Similarity` Scores for completions:\n",
    "---\n",
    "\n",
    "Here, the amazon.titan-embed-text-v1 is used to get the embeddings of texts. To use a different embeddings model, change the model in the embeddings_model_info and modify this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_TEXT_LEN_FOR_EMBEDDING: int = config['model_info']['embeddings_model_info'].get('max_text_len_for_embedding')\n",
    "bedrock: Optional[get_bedrock_client] = None\n",
    "\n",
    "def get_embedding(text: str, modelId: str=config['model_info']['embeddings_model_info'].get('model_id'), accept: str='application/json', contentType: str='application/json'):\n",
    "    \"\"\"\n",
    "    Generates embeddings for the responses from the image/text indexes and the target responses if any are\n",
    "    provided in the dataset\n",
    "    \"\"\"\n",
    "    global bedrock\n",
    "    if bedrock is None:\n",
    "        bedrock = get_bedrock_client()\n",
    "    body = json.dumps({\"inputText\": text[:MAX_TEXT_LEN_FOR_EMBEDDING]})\n",
    "    response = bedrock.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    embedding = response_body.get('embedding')\n",
    "    token_count = response_body.get('inputTextTokenCount')\n",
    "    return embedding, token_count\n",
    "\n",
    "def get_cosine_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between the image/text indexes and the target responses if any\n",
    "    \"\"\"\n",
    "    A,_ = get_embedding(text1)\n",
    "    B,_ = get_embedding(text2)\n",
    "    cosine = dot(A, B)/(norm(A)*norm(B))\n",
    "    return cosine\n",
    "\n",
    "def get_rouge_l_score(completion: str, golden: str) -> float:\n",
    "    \"\"\"\n",
    "    This function calculates the rouge-l score between the image/text indexes and the target responses (if any)\n",
    "    \"\"\"\n",
    "    rouge_metric_selection: str = config['model_info']['embeddings_model_info'].get('rouge_metric_selection')\n",
    "    scorer = rouge_scorer.RougeScorer([rouge_metric_selection])\n",
    "    scores = scorer.score(golden, completion)\n",
    "    return round(scores[rouge_metric_selection].fmeasure, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# target_response_key is the target response column name, if any in the evaluation dataset that the user provides\n",
    "target_response_key: str = config['eval_qna_dataset_info']['target_response_key']\n",
    "def compare_completions(row, index_type):\n",
    "    \"\"\"\n",
    "    Generates the rouge and cosine similarity scores for chapter titles and original titles\n",
    "    \"\"\"\n",
    "    if (row.get(index_type) and row.get(target_response_key) is not None) and (pd.notna(row.get(index_type)) and pd.notna(row.get(target_response_key))):\n",
    "        logger.info(f\"{index_type} response: {row[index_type]}, Target response: {row[target_response_key]}\")\n",
    "        rouge_l_score = get_rouge_l_score(row[index_type], row[target_response_key])\n",
    "        cosine_sim = get_cosine_similarity(row[index_type].lower(), row[target_response_key].lower())\n",
    "        return pd.Series([rouge_l_score, cosine_sim])\n",
    "    else:\n",
    "        logger.info(f'ROUGE, Cosine similarity and Bleu scores cannot be computed since original responses are not provided in the dataset')\n",
    "        rouge_l_score, cosine_sim = None, None\n",
    "\n",
    "if target_response_key in eval_df.columns:\n",
    "    for metric in g.QUALITATIVE_METRICS_LIST:\n",
    "        eval_df[[f'{metric}_rouge_l_f1_score', f'{metric}_cosine_similarity']] = eval_df.apply(lambda row: compare_completions(row, index_type=metric), axis=1)\n",
    "else:\n",
    "    logger.info('No evaluation metrics available since target responses are not provided in the dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['eval_qna_dataset_info']['target_response_key'] in eval_df.columns:\n",
    "    eval_df.rename(columns = {'text_response': 'text_only_response', 'img_response': 'img_only_response'}, inplace = True)\n",
    "    eval_df = eval_df.drop(columns=['image_and_text_source', 'text_source', 'img_source'])\n",
    "# Construct the file path\n",
    "metrics_dir: str = config['dir_info']['metrics_dir_name']\n",
    "rouge_cosine_file_path = os.path.join(metrics_dir, config['dir_info']['eval_score_dataset'])\n",
    "eval_df.to_csv(rouge_cosine_file_path, index=False)\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `LLM as a Judge` to evaluate responses from different indexes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: Using LLM as a Judge in the loop\n",
    "---\n",
    "\n",
    "Responses generated by each index (and combined responses from both indexes) are evaluated on relevance and best match (to ground truth) by _Claude Sonnet/Your model of choice_. Prompt for the model that acts as a judge in the loop can be viewed in: [`eval_template.txt`]('prompt_templates/eval_template.txt'). Edit and review this prompt based on the use case and criteria for subjective evaluation.\n",
    "\n",
    "The role of the model acting as a judge it to compare the responses generated by each index to a target response and checked which index gives a response that best matches the `target response` given the question. It provides information on the `best selected response`, `response source` (from the text only index/image only index/or both indexes), and an `explanation` of its selection, with an in depth analysis of comparison between other responses and why it chose the one it did. \n",
    "\n",
    "A final evaluation metric is calculated that shows the distribution of the selected responses and their respective response sources. This will give a judgement call of which multimodal strategy to use in production ready workloads.\n",
    "\n",
    "Note: For more information on the use of having a Model act as a judge, view: https://huggingface.co/learn/cookbook/en/llm_judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the evaluation prompt payloads\n",
    "\n",
    "Here, the evaluation prompt template is used by the LLM judge to evaluate different chapter titles and suggest the most suitable title based on the evaluation criteria mentioned in the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_eval_prompts(row):\n",
    "    \"\"\"\n",
    "    This function evaluates the prompts by incorporating all of the responses generated by various indexes into the evaluation prompt template.\n",
    "    \"\"\"\n",
    "    # represents the eval template used by the model judge\n",
    "    eval_template: Optional[str] = None\n",
    "    processed_eval_template: Optional[str] = None\n",
    "    candidate_responses: List[str] = []\n",
    "    try:\n",
    "        # file path to the eval template\n",
    "        eval_template: str = Path(os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['eval_prompt_template'])).read_text()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Evaluation template not found at {eval_template}\")\n",
    "    logger.info(f\"question: {QUERY_COL}\")\n",
    "    logger.info(f\"original_response: {target_response_key}\")\n",
    "    for column in row.index:\n",
    "        if column.endswith(\"_response\") and column != target_response_key:\n",
    "            response_source = column.split(\"_response\")[0]\n",
    "            candidate_response = row[column]\n",
    "            candidate_responses.append(f\"\\n<{response_source}>\\n{candidate_response}\\n</{response_source}>\\n\")\n",
    "    processed_eval_template = eval_template.format(\n",
    "        question=row[QUERY_COL], \n",
    "        original_response=row[target_response_key],\n",
    "        candidate_responses=\"\\n\".join(candidate_responses)\n",
    "    )\n",
    "    return processed_eval_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add evaluation prompt as a column into a df with respective response index sources and responses to send into the Model for further evaluation in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if eval_df is not None:\n",
    "    eval_df['eval_prompt'] = eval_df.apply(lambda r: prepare_eval_prompts(r), axis=1)\n",
    "    logger.info(\"preparing the evaluation prompt templates for the LLM judge....\")\n",
    "else:\n",
    "    logger.error(f\"Model evaluation dataset is not available to process.\")\n",
    "llm_as_a_judge_eval_df_f_path = os.path.join(metrics_dir, config['dir_info']['processed_prompts_for_eval'])\n",
    "eval_df.insert(0, 'query_id', eval_df.index)\n",
    "eval_df.to_csv(llm_as_a_judge_eval_df_f_path, index=False)\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `LLM (Claude) as a judge` in the loop to evaluate get a judgement on which Index gives the best response based on the question (`image only/text only/image and text indexes combined`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm_judge_json_evaluations(model_id: str, prompt: str):\n",
    "    # represents the service name\n",
    "    service_name: str = \"bedrock\"\n",
    "    # represents creating the bedrock model to invoke the litellm api for response for titan, llama and claude\n",
    "    bedrock_model: str = f\"{service_name}/{model_id}\"\n",
    "    # represents the current aws region\n",
    "    aws_region = boto3.Session().region_name \n",
    "    # initialize the response dict\n",
    "    ret = dict(exception = None,\n",
    "               prompt = prompt,\n",
    "               completion = None,\n",
    "               question = None,\n",
    "               target_response = None, \n",
    "               completion_token_count = None,\n",
    "               prompt_token_count= None,\n",
    "               input_token_price = None, \n",
    "               output_token_price = None,\n",
    "               model_id = model_id)\n",
    "    body = ret['prompt']\n",
    "    os.environ[\"AWS_REGION_NAME\"] = aws_region\n",
    "    parameters = config['inference_parameters']\n",
    "    temperature = config['inference_parameters'].get('temperature', 0.1)\n",
    "    max_tokens = config['inference_parameters'].get('max_tokens', 0.1)\n",
    "    try:\n",
    "        # Represents calling the litellm completion/messaging api utilizing the completion/embeddings API\n",
    "        logger.info(f\"Invoking {bedrock_model}......\")\n",
    "        response = completion(model=bedrock_model,\n",
    "                              messages=[{ \"content\": body,\"role\": \"user\"}],\n",
    "                              temperature=temperature,\n",
    "                              max_tokens=max_tokens)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Exception occurred during invoking {model_id}, exception={e}\")\n",
    "        ret['exception'] = e\n",
    "    logger.info(f\"completion: {ret['completion']}\")\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_inference(i: int, row: Dict, total: int, judge_model_info: Dict) -> Dict:\n",
    "    # save all the responses from the model in a dictionary\n",
    "    resp: Dict = {}\n",
    "    print(f\"row={row}\")\n",
    "    logger.info(f\"row {i}/{total}, prompt_template={config['dir_info']['eval_prompt_template']}, model_id={judge_model_info['model_id']}\")\n",
    "    model_id = judge_model_info['model_id']\n",
    "    # create the payload for model inference\n",
    "    prompt = row['eval_prompt']\n",
    "    # generate the chapter title based on the given chapter in the prompt \n",
    "    resp = llm_judge_json_evaluations(model_id, prompt)\n",
    "    resp[QUERY_COL] = row[QUERY_COL]\n",
    "    resp[target_response_key] = row[target_response_key]\n",
    "    # calculate the input and output token price for all of the calls\n",
    "    resp['input_token_price'] = (resp['prompt_token_count']/1000) * judge_model_info['input_tokens_price']\n",
    "    resp['output_token_price'] = (resp['completion_token_count']/1000) * judge_model_info['output_tokens_price']\n",
    "    dir_path = os.path.join(config['dir_info']['judge_model_eval_completions'], str(row['query_id']), model_id.replace(\":\", \"-\"))\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    fpath = os.path.join(dir_path, f\"model_evaluation_{row['query_id']}.json\")\n",
    "    logger.info(f\"writing response={resp} to {fpath}\")\n",
    "    Path(fpath).write_text(json.dumps(resp, default=str, indent=2))\n",
    "    logger.info(f\"response {i}: {resp}\")\n",
    "    return resp\n",
    "\n",
    "@ray.remote\n",
    "def async_get_inference(i: int, row: Dict, total: int, model_info: Dict) -> Dict:\n",
    "    logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    return get_inference(i, row, total, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_records_list = json.loads(eval_df.to_json(orient='records'))\n",
    "n: int = config['inference_info']['parallel_inference_count']\n",
    "from typing import List\n",
    "resp_list: List = []\n",
    "judge_model_info = config['model_info']['eval_model_info']\n",
    "st = time.perf_counter()\n",
    "logger.info(f\"------ running inference for {judge_model_info.get('model_id')} -----\")\n",
    "list_of_lists = [eval_records_list[i * n:(i + 1) * n] for i in range((len(eval_records_list) + n - 1) // n )]\n",
    "logger.info(f\"split input list of size {len(eval_records_list)} into {len(list_of_lists)} lists\")\n",
    "for idx, l in enumerate(list_of_lists):\n",
    "    logger.info(f\"getting inference for list {idx+1}/{len(list_of_lists)}, size of list={len(l)} \")\n",
    "    resp_list.extend(ray.get([async_get_inference.remote(i+1, e, len(l), judge_model_info) for i, e in enumerate(l)]))\n",
    "elapsed_time = time.perf_counter() - st\n",
    "logger.info(f\"------ model={judge_model_info.get('model_id')} completed in {elapsed_time} ------ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize `LLM as a judge` completions and get more evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Represents extracted all metric files\n",
    "fpath_evaluated_files = os.path.join(config['dir_info']['judge_model_eval_completions'], \"**\", \"*\", \"*.json\")\n",
    "eval_metric_files = glob.glob(fpath_evaluated_files, recursive=True)\n",
    "logger.info(f\"there are {len(eval_metric_files)} evaluated files by {config['model_info']['eval_model_info'].get('model_id')} LLM judge in {fpath_evaluated_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation_responses = []\n",
    "for f in eval_metric_files:\n",
    "    with open(f, 'r') as file:\n",
    "        model_evaluation_responses.append(json.loads(file.read()))\n",
    "# results_df will contain the evaluation responses, including the completion and the model id\n",
    "results_df = pd.DataFrame(model_evaluation_responses)\n",
    "results_df = results_df.drop(columns=['exception', 'prompt'])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_as_json(x: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Convert a string into a dictionary. Remove any\n",
    "    stray whitespaces which could break the json parsing\n",
    "    \"\"\"\n",
    "    d: Optional[Dict] = None\n",
    "    try:\n",
    "        x = x.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        d = json.loads(x)\n",
    "    except Exception as e:\n",
    "        print(f\"parse_as_json, error parsing string as json, string={x}\")\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tidy_split(df, column, sep=',', keep=False):\n",
    "    \"\"\"\n",
    "    Split the values of a column and expand so the new DataFrame has one split\n",
    "    value per row. Filters rows where the column is missing.\n",
    "    Params\n",
    "    ------\n",
    "    df : pandas.DataFrame\n",
    "        dataframe with the column to split and expand\n",
    "    column : str\n",
    "        the column to split and expand\n",
    "    sep : str\n",
    "        the string used to split the column's values\n",
    "    keep : bool\n",
    "        whether to retain the presplit value as it's own row\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Returns a dataframe with the same columns as `df`.\n",
    "    \"\"\"\n",
    "    indexes = list()\n",
    "    new_values = list()\n",
    "    df = df.dropna(subset=[column])\n",
    "    for i, presplit in enumerate(df[column].astype(str)):\n",
    "        values = presplit.split(sep)\n",
    "        if keep and len(values) > 1:\n",
    "            indexes.append(i)\n",
    "            new_values.append(presplit)\n",
    "        for value in values:\n",
    "            indexes.append(i)\n",
    "            new_values.append(value)\n",
    "    new_df = df.iloc[indexes, :].copy()\n",
    "    new_df[column] = new_values\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results_df = results_df['completion'].apply(parse_as_json).apply(pd.Series)\n",
    "new_results_df.insert(0, QUERY_COL, new_results_df.index)\n",
    "new_results_df.insert(1, target_response_key, new_results_df.index)\n",
    "new_results_df[QUERY_COL] = results_df[QUERY_COL]\n",
    "new_results_df[target_response_key] = results_df[target_response_key]\n",
    "new_exploded_df = tidy_split(new_results_df, 'response_source', sep=',')\n",
    "new_exploded_df['response_source'] = new_results_df['response_source'].str.replace('<', '').str.replace('>', '')\n",
    "logger.info(f\"All evaluation data is read into a dataframe of shape {results_df.shape}\")\n",
    "processed_prompts_for_eval_path = os.path.join(metrics_dir, config['dir_info']['llm_as_a_judge_completions'])\n",
    "new_results_df.to_csv(processed_prompts_for_eval_path, index=False)\n",
    "# display the selected title, model explanation and the respective golden title in a side by side view\n",
    "new_results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the percentage of each model selection and reset the index\n",
    "new_exploded_df['response_source'] = new_exploded_df['response_source'].map(lambda x: x.strip())\n",
    "response_index_percentage_df = new_exploded_df['response_source'].value_counts(normalize=True).reset_index()\n",
    "response_distribution_fpath = os.path.join(metrics_dir, config['dir_info']['index_response_distribution'])\n",
    "response_index_percentage_df.rename(columns = {'response_source':'index_pick_rate'}, inplace = True)\n",
    "response_index_percentage_df['index_pick_rate'] *= 100\n",
    "response_index_percentage_df.to_csv(response_distribution_fpath, index=False)\n",
    "response_index_percentage_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a final summary of which strategy to use for this solution\n",
    "---\n",
    "\n",
    "This portion gets all of the LLM explanations and best match responses to user question, the LLM pick rate, and gives a final summary of which strategy to use for this solution (`combined`, `text_only` or `image_only` or all three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple function to get a final summary on all of the data provided from LLM as a judge\n",
    "def final_analysis_summary(bedrock: botocore.client, \n",
    "                           prompt: str, \n",
    "                           modelID: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the prompt that checks whether the text file has a response to the question and if not, \n",
    "    returns \"not found\" to move to the next hit\n",
    "    \"\"\"\n",
    "    modelId=modelID\n",
    "    body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.1,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "        modelId=modelId,\n",
    "        body=body)\n",
    "\n",
    "        response_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "        llm_response = response_body['content'][0]['text'].replace('\"', \"'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        llm_response = None\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ALL_EXPLANATIONS_FPATH: str = os.path.join(metrics_dir, config['dir_info']['all_explanations'])\n",
    "with open(ALL_EXPLANATIONS_FPATH, 'w') as file:\n",
    "    for index, row in new_results_df.iterrows():\n",
    "        file.write(f\"Question: {row[QUERY_COL]}\\nSelected Index: {row['response_source']}\\nExplanation: {row['explanation']}\\n\\n\")\n",
    "\n",
    "# Read the content back to use as analysis context\n",
    "with open(ALL_EXPLANATIONS_FPATH, 'r') as file:\n",
    "    analysis_context = file.read()\n",
    "print(analysis_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the prompt template and prepare it for inference\n",
    "processed_summary_eval_prompt: str = Path(os.path.join(config['dir_info']['prompt_dir'], config['dir_info']['final_llm_as_a_judge_summary_analysis'])).read_text().format(context=analysis_context)\n",
    "endpoint_url: str = g.BEDROCK_EP_URL.format(region=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)\n",
    "final_analysis: str = final_analysis_summary(bedrock, prompt=processed_summary_eval_prompt, modelID=config['model_info']['final_analysis_llm_summarizer'].get('model_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(final_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FINAL_SUMMARY_ANALYSIS: str = os.path.join(metrics_dir, config['dir_info']['final_summary_analysis'])\n",
    "Path(FINAL_SUMMARY_ANALYSIS).write_text(final_analysis + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
