---
# Name of the application
app_name: multi-modal-rag-bedrock

aws:
  region: us-west-2
  # adding the cfn stack name in the config.yml file in case users
  # deploy the template using their custom name
  cfn_stack_name: multi-modal-revised
  os_service: aoss

# directory paths
# these directory paths contain the source data PDF files that are consumed, 
# the text and image descriptions that are stored as embeddings in the opensearch
# text and image indexes
pdf_dir_info:
  # a public pdf file is used by default in this folder but if you would like to use
  # your own pdf files, put them manually in this local directory
  source_pdf_dir: pdf_data
  pdf_img_path: images
  slide_deck_img_dir: img
  pdf_txt_path: text_files
  pdf_extracted_data: pdf_extracted_data
  json_img_dir: pdf_img_json_dir
  json_txt_dir: pdf_text_json_dir
  bucket_prefix: multimodal
  bucket_img_prefix: img
  qna_dir: question_answer_files
  image_format: JPEG
  prompts: prompt_templates
  # save the images manually from the pdf if need be
  manually_saved_images_path: manually_saved_imgs
  # if you want to use the manually uploaded images 
  # (that you extract from the document, for example, screenshots), 
  # set "manually_saved_images_provided" to yes 
  manually_saved_images_provided: no
  processed_prompts_for_eval: processed_llm_judge_evaluation_prompts.csv
  judge_model_eval_completions: data/model_eval_completions
  llm_as_a_judge_completions: llm_as_a_judge_completions.csv
  index_response_distribution: index_response_distribution.csv
  # save the txt files with the final llm analysis
  all_explanations: all_explanations.txt
  final_summary_analysis: final_summary_analysis.txt
  claude_final_summary_eval_prompt: claude_final_summary_prompt.txt
  

# Set the steps you want to run all via a python script to get the 
# response to your evaluation data/save the embeddings, and prepare the 
# data for multimodal image RAG inference
run_steps:
  1_data_prep_pdf_files.ipynb: no
  # set this to yes if you are running a slide deck instead of a PDF - this notebook
  # is work in progress
  1_data_prep_slide_deck.ipynb: no
  2_data_ingestion.ipynb: no
  3_rag_inference.ipynb: yes
  4_rag_evaluation.ipynb: yes
  # set this step to 'yes' only when you want to delete the indexes and images/texts from the 
  # s3 bucket to store new data in both from scratch
  5_cleanup.ipynb: no

metrics_dir:
  dir_name: metrics
  text_and_image_raw_content: all_content_description.csv
  eval_score_dataset: quantitative_eval_metrics.csv
  QnA_bank: synthetic_qna_bank.csv

# this section gives user the control to split the image into parts, or ingest the image as a whole as is
# there are 4 options to split the image. 
# 1. if you do not want to split the image, set all of the three below to "no" - manually_saved_images_provided, horizontal_split and vertical_split
# 2. if you want to split the image horizontally, only set the "horizontal_split" to yes and the rest to no
# 3. if you want to split the image vertically, only set the "vertical_split" to yes and the rest to no
# 4. if you want to split the image 4 ways, set "horizontal_split" and "vertical_split" to yes
page_split_imgs: 
  # all of the options below are set to 'no', so the entire pdf page will be
  # saved as an image without any cropping
  horizontal_split: no
  vertical_split: no
  # set the image resolution
  image_scale: 3

# content information: pdf files and slide decks
content_info:
  # if you are using pdfs, then your content type is 'pdf', else it is 'slide_deck'
  content_type: pdf 
  # either list the names of the pdf files you manually upload or the 
  # list of public pdf urls
  pdf_local_files:
  # this pdf file is fetched from the publicly available on the aws whitepaper's website: 
  # https://aws.amazon.com/whitepapers/?whitepapers-main.sort-by=item.additionalFields.sortDate&whitepapers-main.sort-order=desc&awsf.whitepapers-content-type=*all&awsf.whitepapers-global-methodology=*all&awsf.whitepapers-tech-category=*all&awsf.whitepapers-industries=*all&awsf.whitepapers-business-category=*all&whitepapers-main.q=AI&whitepapers-main.q_operator=AND
  - https://docs.aws.amazon.com/pdfs/whitepapers/latest/ml-best-practices-healthcare-life-sciences/ml-best-practices-healthcare-life-sciences.pdf#ml-best-practices-healthcare-life-sciences
  # - <list your manually uploaded pdf file names in pdf_data directory>
  
  slide_deck:
    url: "https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf"
    local_path:
  image_extn: .jpg
  text_extn: .txt

parallel_inference_count: 10
minimum_entities_to_match_from_question: 2

# enter information about the llm to get responses from (in this case claude sonnet), 
# bedrock ep url, the embeddings model - TODO - Change these names to be more generic
bedrock_model_info:
  bedrock_ep_url: https://bedrock-runtime.{region}.amazonaws.com
  claude_sonnet_model_id: anthropic.claude-3-sonnet-20240229-v1:0
  claude_input_tokens_pricing: 0.00300
  claude_output_tokens_pricing: 0.01500
  titan_model_id: amazon.titan-embed-text-v1

# judge in the loop model to evaluate completions against human generated/golden titles
eval_model_info:
  model: anthropic.claude-3-sonnet-20240229-v1:0
  prompt_template: eval_template.txt
  input_tokens_pricing: 0.00300
  output_tokens_pricing: 0.01500
  
encoding_info:
  accept_encoding: "application/json"
  content_encoding: "application/json"

# enter the file - enter information in here if you are bringing in a curated dataset of questions to test against
eval_qna_dataset_info:
  dir_name: eval_data
  # enter the file name of your evaluation dataset. This can be a .csv or a .xls/.xlsx file
  eval_dataset_name: newds.csv
  # set 'is_given' to no if the eval dataset is not given
  is_given: Yes
  # this is the key/column name in the df that represents
  # the user question/query
  question_key: Query
  target_response_key: Response
  updated_eval_file: updated_eval_dataset.csv
  prompt_template: eval_template.txt

# These are the paths to the prompts that are used in the process
# of generating responses from the image and the text indices in the
# run inference step. Additional, this contains an evaluation prompt that 
# is used by the LLM as a judge while making subjective evaluations of 
# which index strategy generates the most optimal and accurate responses to the user
# questions based on correctness and relevancy
search_response_prompt_templates:
  # this prompt template is used to search for responses in the image description as well as
  # in the image directly during the RAG inference pipeline
  search_in_images_template: prompt_templates/retrieve_answer_from_images_prompt.txt
  # this prompt template is used to retrieve answers from the texts that are extracted from the 
  # PDF pages
  search_in_text_template: prompt_templates/retrieve_answer_from_texts_prompt.txt
  # this prompt template is used to extract entities/metadata from the user question in the 
  # process of prefiltering to get only the matching documents from the index with similar metadata
  extract_question_entities_prompt: prompt_templates/extract_question_entities_prompt.txt
  # this prompt template is used by a final LLM call to give a combined response from both the 
  # indices
  final_combined_llm_response_prompt: prompt_templates/final_combined_response_prompt_template.txt
  
# This is the 'k' parameter. Increase or decrease it for your use case
k_count_retrieval: 5

# these are the inference parameters used by the bedrock model while generating 
# inferences during the process of the RAG workflow
inference_parameters_for_explanations:
  temperature: 0.1
  caching: False
  max_tokens: 500

# information on the embeddings model, including the ROUGE and Cosine Similarity scores
embeddings_model_info:
  model: amazon.titan-embed-text-v1
  max_text_len_for_embedding: 500
  rouge_metric_selection: 'rougeL' # can be rouge-1, rouge-2, rouge-w, rouge-s, etc
  # this parameter below helps fetch the ROUGE and Cosine similarity
  # scores on responses from both the indexes, text, image seperately and then
  # from both combined
  get_quantitative_metrics_on:
  - 'combined_response'
  - 'text_response'
  - 'img_response'

# this LLM is used to gather all of the evaluations and explanations from 
# the LLM as a judge and provide a summary analysis of which strategy to use
# for the multimodal use case (image only responses/text only responses/or responses
# from both of the indexes combined)
final_llm_summarize_eval_analysis: anthropic.claude-3-sonnet-20240229-v1:0

# This prompt is used by Claude 3 Sonnet to generate entities from the images that are 
# extracted from the pdf files. Change this to match your domain tailored data so that
# during the retrieval process, the prefilter happens effectively and matches metadata
# from the user question to the domain tailored metadata of your documents
entity_extraction_prompt_images: |
  "Please provide a detailed description of the entities present in the image. Entities, are specific pieces of information or objects within a text that carry particular significance. These can be real-world entities like names of people, places, organizations, or dates. Refer to the types of entities: Named entities: These include names of people, organizations, locations, and dates. You can have specific identifiers within this, such as person names or person occupations.
  Custom entities: These are entities specific to a particular application or domain, such as product names, medical terms, or technical jargon.
  Temporal entities: These are entities related to time, such as dates, times, and durations.
  Product entities: Names of products might be grouped together into product entities.
  Data entities: Names of the data and metrics present. This includes names of metrics in charts, graphs and tables, and throughout the image.
  Now based on the image, create a list of these entities. Your response should be accurate. Do not make up an answer."

# This prompt is used to generate a synthetic question answering bank from all of the 
# data stored in OpenSearch serverless
QnA_generator_prompt: |
  "Human: Based on the text description provided in <text_desc></text_desc> tags, generate a list of five to 10 questions. Only refer to the context in the <text_desc> tags, and do not provide questions that are not related to the context provided. Your response should be in a JSON format containing two elements: 'question' and 'answer'. The question should be directly related to the context provided in the <text_desc> tags and the answer should be the answer to that question from the <text_desc> context. Do not make up an answer.If you do not know the answer to the question just say that you don't know the answer. Don't try to make up an answer or a question. Refer to the context below:
  <text_desc>
  {context}
  </text_desc>
  Assistant: Sure, here are a list of Questions and Answers generated from the context in JSON format:"
  
# Inference parameters used when questions are generated 
# (Note: this is used outside of the RAG and evaluation pipeline in a separate notebook
inference_parameters_for_qna_generation:
  temperature: 0.1
  caching: False
  max_tokens: 1000
 