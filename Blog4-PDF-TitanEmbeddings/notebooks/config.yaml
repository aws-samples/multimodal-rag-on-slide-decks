---
# Name of the application
app_name: multi-modal-rag-bedrock

aws:
  region: us-west-2
  # adding the cfn stack name in the config.yml file in case users
  # deploy the template using their custom name
  cfn_stack_name: multi-modal-revised
  os_service: aoss

# directory paths
# these directory paths contain the source data PDF files that are consumed, 
# the text and image descriptions that are stored as embeddings in the opensearch
# text and image indexes
pdf_dir_info:
  # a public pdf file is used by default in this folder but if you would like to use
  # your own pdf files, put them manually in this local directory
  source_pdf_dir: pdf_data
  pdf_img_path: images
  pdf_txt_path: text_files
  pdf_extracted_data: pdf_extracted_data
  json_img_dir: pdf_img_json_dir
  json_txt_dir: pdf_text_json_dir
  bucket_prefix: multimodal
  bucket_img_prefix: img
  qna_dir: question_answer_files
  image_format: JPEG
  prompts: prompt_templates

# Set the steps you want to run all via a python script to get the 
# response to your evaluation data/save the embeddings, and prepare the 
# data for multimodal image RAG inference
run_steps:
  1_data_prep_pdf_files.ipynb: yes
  1_data_prep_slide_deck.ipynb: no
  2_data_ingestion.ipynb: yes
  3_rag_inference.ipynb: yes
  4_rag_evaluation.ipynb: yes

metrics_dir:
  dir_name: metrics
  text_and_image_raw_content: all_content_description.csv
  eval_score_dataset: quantitative_eval_metrics.csv
  QnA_bank: synthetic_qna_bank.csv

# this section gives user the control to split the image into parts, or ingest the image as a whole as is
# there are 4 options to split the image. 
# 1. if you do not want to split the image, set all of the three below to "no" - manually_saved_images_provided, horizontal_split and vertical_split
# 2. if you want to split the image horizontally, only set the "horizontal_split" to yes and the rest to no
# 3. if you want to split the image vertically, only set the "vertical_split" to yes and the rest to no
# 4. if you want to split the image 4 ways, set "horizontal_split" and "vertical_split" to yes
# if you want to use the manually uploaded images (that you extract from the document), set "manually_saved_images_provided" to yes
page_split_imgs: 
  manually_saved_images_provided: no
  horizontal_split: no
  vertical_split: no
  # set the image resolution
  image_scale: 3

# content information: pdf files and slide decks
content_info:
  # if you are using pdfs, then your content type is 'pdf', else it is 'slide_deck'
  content_type: pdf 
  pdf_file_url: 
  pdf_local_files:
  - tesla_rec.pdf
  # - Nvidia_rec.pdf
  - Microsoft_rec.pdf
  - Intel_rec.pdf
  - Cisco_rec.pdf
  # - Broadcom_rec.pdf
  - Boeing_rec.pdf
  - APPLE_rec.pdf
  - AMD_rec.pdf
  - Amazon_rec.pdf
  # save the images manually from the pdf if need be
  manually_saved_images_path: manually_saved_imgs
  slide_deck:
    url: "https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf"
    local_path:
  image_extn: .jpg
  text_extn: .txt

parallel_inference_count: 10
minimum_entities_to_match_from_question: 2

# enter information about the llm to get responses from (in this case claude sonnet), 
# bedrock ep url, the embeddings model - TODO - Change these names to be more generic
bedrock_model_info:
  bedrock_ep_url: https://bedrock-runtime.{region}.amazonaws.com
  claude_sonnet_model_id: anthropic.claude-3-sonnet-20240229-v1:0
  claude_input_tokens_pricing: 0.00300
  claude_output_tokens_pricing: 0.01500
  titan_model_id: amazon.titan-embed-text-v1
  llm_as_a_judge: anthropic.claude-3-sonnet-20240229-v1:0
  
encoding_info:
  accept_encoding: "application/json"
  content_encoding: "application/json"

# enter the file - enter information in here if you are bringing in a curated dataset of questions to test against
eval_qna_dataset_info:
  dir_name: eval_data
  # set 'is_given' to no if the eval dataset is not given
  is_given: Yes
  # this is the key/column name in the df that represents
  # the user question/query
  question_key: Query
  target_response_key: Response
  updated_eval_file: updated_eval_dataset.csv

k_count_retrieval: 4

QnA_generator_prompt: |
  "Human: Based on the text description provided in <text_desc></text_desc> tags, generate a list of five to 10 questions. Only refer to the context in the <text_desc> tags, and do not provide questions that are not related to the context provided. Your response should be in a JSON format containing two elements: 'question' and 'answer'. The question should be directly related to the context provided in the <text_desc> tags and the answer should be the answer to that question from the <text_desc> context. Do not make up an answer.If you do not know the answer to the question just say that you don't know the answer. Don't try to make up an answer or a question. Refer to the context below:
  <text_desc>
  {context}
  </text_desc>
  Assistant: Sure, here are a list of Questions and Answers generated from the context in JSON format:"
  
inference_parameters_for_qna_generation:
  temperature: 0.1
  caching: False
  max_tokens: 1000

embeddings_model_info:
  model: amazon.titan-embed-text-v1
  max_text_len_for_embedding: 500
  rouge_metric_selection: 'rougeL' # can be rouge-1, rouge-2, rouge-w, rouge-s, etc
  # this parameter below helps fetch the ROUGE and Cosine similarity
  # scores on responses from both the indexes, text, image seperately and then
  # from both combined
  get_quantitative_metrics_on:
  - 'combined_response'
  - 'text_response'
  - 'img_response'

 