---
# Name of the application
app_name: multi-modal-rag-bedrock

aws:
  region: us-east-1
  # adding the cfn stack name in the config.yml file in case users
  # deploy the template using their custom name
  cfn_stack_name: multi-modal-revised

# directory paths
pdf_dir_info:
  source_pdf_dir: pdf_data
  pdf_img_path: images
  pdf_txt_path: text_files
  pdf_extracted_data: pdf_extracted_data
  json_img_dir: pdf_img_json_dir
  json_txt_dir: pdf_text_json_dir
  bucket_prefix: multimodal
  bucket_img_prefix: img
  qna_dir: question_answer_files

metrics_dir:
  dir_name: metrics
  text_and_image_raw_content: all_content_description.csv

# set this to yes if you want to split the images 4 ways, and no if 
# you want the entire page to be a single image
# if you want to split the image in 4 ways, set both to yes.
# if you want to split the page either horizontally or vertically, set either to yes
# if you want to save the entire page to an image without any splits, set both to no
page_split_imgs: 
  horizontal_split: no
  vertical_split: no
  # set the image resolution
  image_scale: 2.5

# content information: pdf files and slide decks
content_info:
  # if you are using pdfs, then your content type is 'pdf', else it is 'slide_deck'
  content_type: pdf 
  pdf_file_url: 
  pdf_local_files:
  - <enter your pdf files as a list here>
  slide_deck:
    url: "https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf"
    local_path:
  image_extn: .jpg
  text_extn: .txt

parallel_inference_count: 10

# enter information about the llm to get responses from (in this case claude sonnet), 
# bedrock ep url, the embeddings model - TODO - Change these names to be more generic
bedrock_model_info:
  bedrock_ep_url: https://bedrock-runtime.{region}.amazonaws.com
  claude_sonnet_model_id: anthropic.claude-3-sonnet-20240229-v1:0
  claude_input_tokens_pricing: 0.00300
  claude_output_tokens_pricing: 0.01500
  titan_model_id: amazon.titan-embed-text-v1

encoding_info:
  accept_encoding: "application/json"
  content_encoding: "application/json"

# enter the file - enter information in here if you are bringing in a curated dataset of questions to test against
eval_qna_dataset_info:
  dir_name: eval_data
  # set 'is_given' to no if the eval dataset is not given
  is_given: Yes
  # this is the key/column name in the df that represents
  # the user question/query
  question_key: Query
  golden_response_key: Response
  updated_eval_file: updated_eval_dataset.csv
  k_size: 1
  
# this is the prompt that is used to search for the answer in the image if not present already in the image
# text description
direct_image_answer_retrieval_prompt: |
  "Human: Your role is to retrieve information from the image provided if the question provided in the <question></question> tags does not have an answer directly mentioned in the <img_text_desc></img_text_desc> tags.
  If the image description (img_text_desc) does not contain the answer to the question below, then search the "data" for it. If the image description (img_text_desc) does contain answer to the question, then your response should be only one word - "answer_in_text_desc". 
  If the answer to the question is not provided in the <img_text_desc>, then search the actual image provided in the "data" and look for the answer to the question. Refer to the img_text_desc and question below:
  
  <img_text_desc>
  {context}
  </img_text_desc>
  <question>
  {question}
  </question>
  
  If the answer to the question is not in the img_text_desc, search the actual image provided in "data" and retrieve the answer directly from the image provided. Search for the question directly in the image in this case and retrieve the most accurate answer. Do not make up an answer.
  
  Refer to an example in the <answer_to_question_in_img_desc_exists></answer_to_question_in_img_desc_exists> tags when the answer to the question is given in the img_text_desc below:

    <answer_to_question_in_img_desc_exists>
    "answer_in_text_desc"
    <answer_to_question_in_img_desc_exists>
    
    if the answer to the <question> is not in the img_text_desc, search for it in the actual image in the "data" provided to you.
    
    Assistant: "
    

QnA_generator_prompt: |
  "Human: Based on the text description provided in <text_desc></text_desc> tags, generate a list of five to 10 questions. Only refer to the context in the <text_desc> tags, and do not provide questions that are not related to the context provided. Your response should be in a JSON format containing two elements: 'question' and 'answer'. The question should be directly related to the context provided in the <text_desc> tags and the answer should be the answer to that question from the <text_desc> context. Do not make up an answer.If you do not know the answer to the question just say that you don't know the answer. Don't try to make up an answer or a question. Refer to the context below:
  <text_desc>
  {context}
  </text_desc>
  Assistant: Sure, here are a list of Questions and Answers generated from the context in JSON format:"
  
inference_parameters_for_qna_generation:
  temperature: 0.1
  caching: False
  max_tokens: 1000

 