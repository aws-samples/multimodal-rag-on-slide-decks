---
# Name of the application
app_name: multi-modal-rag-bedrock

aws:
  region: us-east-1
  # adding the cfn stack name in the config.yml file in case users
  # deploy the template using their custom name
  cfn_stack_name: multi-modal-revised

# directory paths
pdf_dir_info:
  source_pdf_dir: pdf_data
  pdf_img_path: images
  pdf_txt_path: text_files
  pdf_extracted_data: pdf_extracted_data
  json_img_dir: pdf_img_json_dir
  json_txt_dir: pdf_text_json_dir
  bucket_prefix: multimodal
  bucket_img_prefix: img
  qna_dir: question_answer_files

metrics_dir:
  dir_name: metrics
  text_and_image_raw_content: all_content_description.csv

# this section gives user the control to split the image into parts, or ingest the image as a whole as is
# there are 4 options to split the image. 
# 1. if you do not want to split the image, set all of the three below to "no" - manually_saved_images_provided, horizontal_split and vertical_split
# 2. if you want to split the image horizontally, only set the "horizontal_split" to yes and the rest to no
# 3. if you want to split the image vertically, only set the "vertical_split" to yes and the rest to no
# 4. if you want to split the image 4 ways, set "horizontal_split" and "vertical_split" to yes
# if you want to use the manually uploaded images (that you extract from the document), set "manually_saved_images_provided" to yes
page_split_imgs: 
  manually_saved_images_provided: no
  horizontal_split: no
  vertical_split: no
  # set the image resolution
  image_scale: 3

# content information: pdf files and slide decks
content_info:
  # if you are using pdfs, then your content type is 'pdf', else it is 'slide_deck'
  content_type: pdf 
  pdf_file_url: 
  pdf_local_files:
  - tesla_rec.pdf
  # - Nvidia_rec.pdf
  - Microsoft_rec.pdf
  - Intel_rec.pdf
  - Cisco_rec.pdf
  # - Broadcom_rec.pdf
  - Boeing_rec.pdf
  - APPLE_rec.pdf
  - AMD_rec.pdf
  - Amazon_rec.pdf
  # save the images manually from the pdf if need be
  manually_saved_images_path: manually_saved_imgs
  slide_deck:
    url: "https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf"
    local_path:
  image_extn: .jpg
  text_extn: .txt

parallel_inference_count: 10

# enter information about the llm to get responses from (in this case claude sonnet), 
# bedrock ep url, the embeddings model - TODO - Change these names to be more generic
bedrock_model_info:
  bedrock_ep_url: https://bedrock-runtime.{region}.amazonaws.com
  claude_sonnet_model_id: anthropic.claude-3-sonnet-20240229-v1:0
  claude_input_tokens_pricing: 0.00300
  claude_output_tokens_pricing: 0.01500
  titan_model_id: amazon.titan-embed-text-v1

encoding_info:
  accept_encoding: "application/json"
  content_encoding: "application/json"

# enter the file - enter information in here if you are bringing in a curated dataset of questions to test against
eval_qna_dataset_info:
  dir_name: eval_data
  # set 'is_given' to no if the eval dataset is not given
  is_given: Yes
  # this is the key/column name in the df that represents
  # the user question/query
  question_key: Query
  golden_response_key: Response
  updated_eval_file: updated_eval_dataset.csv

k_count_retrieval: 4

QnA_generator_prompt: |
  "Human: Based on the text description provided in <text_desc></text_desc> tags, generate a list of five to 10 questions. Only refer to the context in the <text_desc> tags, and do not provide questions that are not related to the context provided. Your response should be in a JSON format containing two elements: 'question' and 'answer'. The question should be directly related to the context provided in the <text_desc> tags and the answer should be the answer to that question from the <text_desc> context. Do not make up an answer.If you do not know the answer to the question just say that you don't know the answer. Don't try to make up an answer or a question. Refer to the context below:
  <text_desc>
  {context}
  </text_desc>
  Assistant: Sure, here are a list of Questions and Answers generated from the context in JSON format:"
  
inference_parameters_for_qna_generation:
  temperature: 0.1
  caching: False
  max_tokens: 1000

 