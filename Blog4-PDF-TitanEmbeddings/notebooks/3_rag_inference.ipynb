{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Retrieval Augumented Generation (RAG) Inference\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "1. Takes in a user question, and extracts entities from that question\n",
    "\n",
    "1. Uses the entities from the question, performs `prefiltering` and extracts the top `k` hits from the index based on the `entities matching`\n",
    "\n",
    "1. Uses an `LLM in the loop` to go over each `k` hit, check for if the answer to the question is given in that hit and if not, move to the next `hit` until the answer is found. If the answer is not found in any, return `I don't know`.\n",
    "\n",
    "1. Use an eval dataset that a user provides with a question bank, iterate through each question and query the `text` and the `image indexes` to look for answers to the questions in the eval dataset.\n",
    "\n",
    "1. During retrieval, answers are searched from both, the `text` and the `image` index to provide a combined answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install the requirements\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries to run this notebook\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import boto3\n",
    "import logging\n",
    "import litellm\n",
    "import requests\n",
    "import botocore\n",
    "import opensearchpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from litellm import completion\n",
    "from IPython.display import Image\n",
    "from urllib.parse import urlparse\n",
    "from botocore.auth import SigV4Auth\n",
    "from pandas.core.series import Series\n",
    "from sagemaker import get_execution_role\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_text_embedding, get_llm_response, get_question_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize ray to run inferences asynchronously\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n",
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region: str = config['aws']['region']\n",
    "os_service: str = config['aws']['os_service']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. Create two OpenSearch clients for images and texts separately\n",
    "---\n",
    "\n",
    "We create an OpenSearch client so that we can query the vector database for embeddings (pdf files) similar to the questions that we might want to ask of our `PDF file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the OpenSearch Service Serverless collection endpoint and index name from the CloudFormation stack outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = get_cfn_outputs(config['aws']['cfn_stack_name'])\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "text_index_name = outputs['OpenSearchTextIndexName']\n",
    "img_index_name = outputs['OpenSearchImgIndexName']\n",
    "logger.info(f\"opensearchhost={host}, text index={text_index_name}, image index={img_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, os_service)\n",
    "\n",
    "# Represents the OSI client for images\n",
    "img_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "# Represents the OSI client for images\n",
    "text_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Read for RAG\n",
    "\n",
    "We now have all the pieces for RAG. Here is how we talk to our PDF files.\n",
    "\n",
    "1. Extract entities from the `question`.\n",
    "\n",
    "1. Using the entites in the question, perform a `prefilter` step to get the content that matches the entities from the question in the `text` and `image` indexes the most.\n",
    "\n",
    "1. Based on the top `k` results, use an `LLM in a loop` that you can configure in the [config.yaml](`config.yaml`) file as a check to verify if the answer to the question is in that `hit` and if not, move to the next `hit` to look for an answer. \n",
    "\n",
    "1. Responses from each image (if valid and if they are contained in the `hit`) are added as context to the final context. At the end, all the responses are aggregated into a final context that is run through by a final LLM call to give a final response to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_url: str = config['bedrock_model_info']['bedrock_ep_url'].format(region=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform `prefiltering`\n",
    "---\n",
    "\n",
    "The `find_similar_data_with_entities` function performs a [knn-prefiltering]('https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/'). It takes in the question entites extracted from the question asked, and searches for relevant docs that have similar entities in the `metadata.entities` field that was added during the `data ingestion` step for both `text` and `image` indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_data_with_entities(text_embedding, size: int, os_client, index_name: str, question_entities: List[str]):\n",
    "    \"\"\"\n",
    "    This function is used to prefilter the responses only with images/texts that have entities that match\n",
    "    with the entities provided in the question. Once the documents are refiltered, the search from the index\n",
    "    is returned.\n",
    "    \"\"\"\n",
    "    should_clauses: List[str] = []\n",
    "    # Convert filter_string to lowercase to ensure case-insensitive matching\n",
    "    q_entities_lower: str = question_entities.lower()\n",
    "    q_entities_upper: str = question_entities.upper()\n",
    "    # entities can either be lower case or upper case. search for both cases is supported\n",
    "    question_entities_variants = [q_entities_lower, q_entities_upper]\n",
    "    logger.info(f\"Entities extracted: {question_entities_variants}\")\n",
    "    for word in question_entities_variants:\n",
    "        for entity in word.split(\",\"):\n",
    "            should_clauses.append({\n",
    "                \"wildcard\": {\n",
    "                    \"metadata.entities\": {\n",
    "                        # wildcard queries are to search for terms that match a wildcard pattern\n",
    "                        # in this case, we are searching for an entity within the text embedding\n",
    "                        \"value\": f\"*{entity.strip()}*\",\n",
    "                        \"case_insensitive\": True\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"knn\": {\n",
    "                        \"vector_embedding\": {\n",
    "                            \"vector\": text_embedding,\n",
    "                            \"k\": size\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": should_clauses,\n",
    "                        # should match at least a single entity to fetch a response. Increase if there\n",
    "                        # is a lot being asked in the question\n",
    "                        \"minimum_should_match\": config['minimum_entities_to_match_from_question']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        content_based_search = os_client.search(body=query, index=index_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"error occured while querying OpenSearch index={index_name}, exception={e}\")\n",
    "        content_based_search = None\n",
    "    return content_based_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nearest_img_search_response(nearest_image_path: str, prompt: str, modelId: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the file path that is most similar to the text embeddings\n",
    "    of the question, returns the image and checks for if the text description does not\n",
    "    contain the answer, directly search for the answer in the selected image.\n",
    "    \"\"\"\n",
    "   # extract the file name from the nearest image path stored in s3\n",
    "    filename: str = os.path.basename(nearest_image_path)\n",
    "    local_directory: str = os.path.join(g.IMAGE_DIR, 'b64_images')\n",
    "    local_image_path = os.path.join(local_directory, filename.replace('.jpg', '.b64'))\n",
    "    logger.info(f\"Nearest image path being used for search: {local_image_path}\")\n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    try:\n",
    "        with open(local_image_path, \"rb\") as image_file:\n",
    "            input_image_b64 = image_file.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading base64 image from local directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,\" + input_image_b64\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    # suppress the litellm logger responses\n",
    "    lite_llm_logger = logging.getLogger('LiteLLM')\n",
    "    lite_llm_logger.setLevel(logging.CRITICAL)\n",
    "    ret = {\n",
    "        \"exception\": None,\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": None,\n",
    "        \"completion_token_count\": None,\n",
    "        \"prompt_token_count\": None,\n",
    "        \"model_id\": modelId\n",
    "    }\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=modelId,\n",
    "            messages=messages,\n",
    "        )\n",
    "        # Suppress logging output\n",
    "        logging.getLogger('LiteLLM').setLevel(logging.CRITICAL)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        ret[\"exception\"] = e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response_from_text_extracted(prompt: str, modelId: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes in the prompt that checks whether the text file has a response to the question and if not, \n",
    "    returns \"not found\" to move to the next hit.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    # suppress the litellm logger responses\n",
    "    lite_llm_logger = logging.getLogger('LiteLLM')\n",
    "    lite_llm_logger.setLevel(logging.CRITICAL)\n",
    "    ret = {\n",
    "        \"exception\": None,\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": None,\n",
    "        \"completion_token_count\": None,\n",
    "        \"prompt_token_count\": None,\n",
    "        \"model_id\": modelId\n",
    "    }\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=modelId,\n",
    "            messages=messages,\n",
    "        )\n",
    "        # Suppress logging output\n",
    "        logging.getLogger('LiteLLM').setLevel(logging.CRITICAL)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        ret[\"exception\"] = e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_llm_response(llm_response: str) -> str:\n",
    "    \"\"\"\n",
    "    This function sanitizes the LLM response generated. If the LLM response contains a sentence that has \n",
    "    \"not found\" or \"Not found\" within it, it returns only \"not found\" and no other characters. Case sensitivity does not matter.\n",
    "    \"\"\"\n",
    "    string_to_match: str = \"not found\"\n",
    "    sanitized_response: Optional[str] = None\n",
    "    try:\n",
    "        # Normalize the case for comparison\n",
    "        normalized_response = llm_response.lower()\n",
    "        if llm_response is None:\n",
    "            sanitized_response: Optional[str] = None\n",
    "        # Check if the normalized response contains the string \"not found\"\n",
    "        if string_to_match in normalized_response:\n",
    "            sanitized_response = string_to_match\n",
    "        else:\n",
    "            sanitized_response = normalized_response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The LLM response cannot be sanitized: {e}\")\n",
    "        sanitized_response: Optional[str] = None\n",
    "    return sanitized_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts used in the RAG worklow\n",
    "---\n",
    "\n",
    "1. `direct_image_answer_retrieval_prompt`: This prompt is used for a check against retrieval from the `image index`. It checks for if the answer to the user question is given in the `image description` and if not, then it searches the image directly. If the answer is not in the `image description` or the `image` itself, it returns a `not found` message.\n",
    "\n",
    "1. `direct_text_answer_retrieval_prompt`: This prompt is used for a check against retrieval from the `text index`. It checks for if the answer to the user question is given in the `text extracted` from the pdf page and if not, then returns a `not found` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "direct_image_answer_retrieval_prompt: str = Path(config['search_response_prompt_templates']['search_in_images_template']).read_text()\n",
    "direct_text_answer_retrieval_prompt: str = Path(config['search_response_prompt_templates']['search_in_text_template']).read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Inference: `Enhanced Multimodal RAG workflow`\n",
    "---\n",
    "\n",
    "The function `get_index_response` performs the following steps:\n",
    "\n",
    "1. It takes in a list of tuples. Each tuple contains the `osi client` and `index name`.\n",
    "\n",
    "1. Given both text and image indexes, it iterates through each tuple.\n",
    "\n",
    "1. Extracts the entities from the `user question`, gets the text embeddings of the question and searches for the nearest hits using those entities as a `prefilter`.\n",
    "\n",
    "1. After fetching the `vector db response`, it iterates through each content in every hit. For an image index, it uses an `LLM in the loop` to check for whether the answer is given in the image index and if not searches the image directly. If none have an answer, it returns a `not found` and moves to the next hit. For the text index, it uses an `LLM in the loop` to check if the extracted text has the answer to the question and if not, returns a `not found` message and moves to the next hit.\n",
    "\n",
    "1. For all the valid responses from each hit and each index, the answer is stored as context for a final `LLM call`. Once all hits are traversed, an `LLM` is invoked to check if the context (that contains answers from all hits) contains the actual answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define global variables to track the total input tokens and output tokens for both the image\n",
    "# and text response indices. This is tracked to give an average token count for a user query \n",
    "# at the end of the inference run\n",
    "# Input tokens\n",
    "INPUT_TOKENS_IMG: int = 0\n",
    "INPUT_TOKENS_TEXT: int = 0\n",
    "# Output tokens\n",
    "OUTPUT_TOKENS_IMG: int = 0\n",
    "OUTPUT_TOKENS_TEXT: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_response(question: str,\n",
    "                       size: int,\n",
    "                       index_clients: List[Tuple[opensearchpy.client.OpenSearch, str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Get LLM responses from retrieved data on questions asked from image, text, or both indexes combined\n",
    "    :param question: Question that a user asks on the content\n",
    "    :param size: 'k' size\n",
    "    :param index_clients: List of tuples containing OpenSearch clients and index names\n",
    "    :Dict: Dictionary with the context used to answer the question and the final response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)\n",
    "        index_llm_response_and_context: Dict = {'source': ''}\n",
    "        model_id: str = config['bedrock_model_info']['claude_sonnet_model_id']\n",
    "        # Represents the list of extracted text and paths from the most similar hits\n",
    "        all_hits = []\n",
    "        logger.info(f\"Going to answer the question: {question}\")\n",
    "        # iterate through each tuple\n",
    "        for os_client, index_name in index_clients:\n",
    "            logger.info(f\"searching for the answer in the {index_name} index:\")\n",
    "            # first extract the entities in the question and get those embeddings to get the hits related to that entities by prefiltering\n",
    "            question_entities = get_question_entities(bedrock, question, model_id)\n",
    "            # Get the text embedding for the given question\n",
    "            text_embedding = get_text_embedding(bedrock, question)\n",
    "            vector_db_response = find_similar_data_with_entities(text_embedding, size, os_client, index_name, question_entities)\n",
    "            if vector_db_response:\n",
    "                hits = vector_db_response.get('hits', {}).get('hits', [])\n",
    "                for hit in hits:\n",
    "                    content_path = hit.get('_source').get('file_path')\n",
    "                    extracted_text = hit.get('_source').get('file_text')\n",
    "                    all_hits.append((content_path, extracted_text, index_name))\n",
    "        logger.info(f\"Iterating through all relevant hits to search for an answer....\")\n",
    "        for content_path, extracted_text, index_name in all_hits:\n",
    "            file_text: str = \"\"\n",
    "            if index_name == outputs['OpenSearchImgIndexName']:\n",
    "                # If the response is from the image index, append to file_text\n",
    "                !aws s3 cp {content_path} .\n",
    "                local_img_path = os.path.basename(content_path)\n",
    "                display(Image(filename=local_img_path))\n",
    "            # Now getting a response from the text or image index\n",
    "            if index_name == outputs['OpenSearchImgIndexName']:\n",
    "                # If the response is not given in the extracted text, search the image directly\n",
    "                search_in_img_prompt: str = direct_image_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_response = get_nearest_img_search_response(content_path, search_in_img_prompt, config['bedrock_model_info']['claude_sonnet_model_id'])\n",
    "                sanitized_response = sanitize_llm_response(direct_response['completion'])\n",
    "                # Update token counts for image\n",
    "                global INPUT_TOKENS_IMG, OUTPUT_TOKENS_IMG\n",
    "                INPUT_TOKENS_IMG += direct_response['prompt_token_count']\n",
    "                OUTPUT_TOKENS_IMG += direct_response['completion_token_count']\n",
    "                # If the answer is not contained in the image description, then \n",
    "                # add the llm response to that specific chosen image to the file_text/context.\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response FOUND from the image {content_path}, moving to the next relevant hit.\")\n",
    "                    # Update the context sources if the answer is given directly from the image as context to give the final answer\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "            elif index_name == outputs['OpenSearchTextIndexName']:\n",
    "                search_in_txt_prompt: str = direct_text_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_response = response_from_text_extracted(search_in_txt_prompt, \n",
    "                                                               config['bedrock_model_info']['claude_sonnet_model_id'])\n",
    "                sanitized_response = sanitize_llm_response(direct_response['completion'])\n",
    "                # Update token counts for text\n",
    "                global INPUT_TOKENS_TEXT, OUTPUT_TOKENS_TEXT\n",
    "                INPUT_TOKENS_TEXT += direct_response['prompt_token_count']\n",
    "                OUTPUT_TOKENS_TEXT += direct_response['completion_token_count']\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response FOUND from the image {content_path}, moving to the next relevant hit.\")\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "        index_llm_response = get_llm_response(question, index_llm_response_and_context['source'], model_id)\n",
    "        index_llm_response_and_context['response'] = index_llm_response['completion']\n",
    "        index_llm_response_and_context['image_input_tokens'] = INPUT_TOKENS_IMG\n",
    "        index_llm_response_and_context['image_output_tokens'] = OUTPUT_TOKENS_IMG\n",
    "        index_llm_response_and_context['text_input_tokens'] = INPUT_TOKENS_TEXT\n",
    "        index_llm_response_and_context['text_output_tokens'] = OUTPUT_TOKENS_TEXT\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not get a response: {e}\")\n",
    "        index_llm_response_and_context['response'] = None\n",
    "        index_llm_response_and_context['source'] = None\n",
    "        index_llm_response_and_context['image_input_tokens'] = None\n",
    "        index_llm_response_and_context['image_output_tokens'] = None\n",
    "        index_llm_response_and_context['text_input_tokens'] = None\n",
    "        index_llm_response_and_context['text_output_tokens'] = None\n",
    "    return index_llm_response_and_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Inference: Inference format\n",
    "---\n",
    "\n",
    "Use the format below to get inference from the given index (text, image, or both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"<enter your example question here>\"\n",
    "# # to get a combined response from both image and text indexes, pass in a list of both tuple variations\n",
    "# index_clients: List[Tuple] = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "\n",
    "# # to get a response only from the text index\n",
    "# # index_clients: List[Tuple] = [(text_os_client, text_index_name)]\n",
    "\n",
    "# # to get a response only from the image index\n",
    "# # index_clients: List[Tuple] = [(img_os_client, img_index_name)]\n",
    "# get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "---\n",
    "\n",
    "We will ask a question about an architecture image on page 28 about a specific portion in the architecture.\n",
    "\n",
    "*Uncomment the cells below and run them to see the logs and final response*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"What are the Trade-offs between performance and model interpretability graph?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name), (text_os_client, text_index_name)]\n",
    "# get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "---\n",
    "\n",
    "We will ask a question about a specific text in the pdf file on page 22 about automating data integration and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"For automating data integration deployment, why do we have to minimize human touch point in deployment pipelines?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name), (text_os_client, text_index_name)]\n",
    "# get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "---\n",
    "\n",
    "We will ask an open ended question about an image and text on page 33 aboutoperationalizing AI/ML workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"In Operationalizing AI/ML workloads, what comes under 'Prepare for production'?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name), (text_os_client, text_index_name)]\n",
    "# get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eval Dataset Comparison\n",
    "---\n",
    "\n",
    "In this section of the notebook we do as follows:\n",
    "\n",
    "1. Check for if a user has provided an `evaluation dataset` - Any dataset with a question bank and/or target responses\n",
    "\n",
    "1. Iterate through each question provided in the dataset, and call the `get_index_response` function\n",
    "\n",
    "1. Record responses from the text response, the image response and combined responses\n",
    "\n",
    "1. Update the df and store the result in the `eval directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['eval_qna_dataset_info']['is_given'] is True:\n",
    "    eval_dir = config['eval_qna_dataset_info']['dir_name']\n",
    "    eval_fpath = os.path.join(eval_dir, config['eval_qna_dataset_info']['eval_dataset_name'])\n",
    "    eval_file = Path(eval_fpath)\n",
    "    if eval_file.suffix == '.csv':\n",
    "        eval_df = pd.read_csv(eval_file)\n",
    "    elif eval_file.suffix in ['.xls', '.xlsx']:\n",
    "        eval_df = pd.read_excel(eval_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {eval_file.suffix}\")\n",
    "    eval_df = eval_df.drop(columns=['Unnamed: 3'])\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get responses from combined (both text and image index), text only, and image only index\n",
    "---\n",
    "\n",
    "1. This part of the notebook iterates through an evaluation dataset (if any)\n",
    "\n",
    "1. Iterates through each user question in a column and generates outputs using the three different ways (combined index approach, text only and image only approaches)\n",
    "\n",
    "1. Appends all responses from each approach to the original dataframe and saves it as a `CSV` file in the `metrics` directory for further evaluations and downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through the evaluation dataframe and record responses for the question bank\n",
    "# using the text, image and both indexes combined for further evaluations\n",
    "if eval_df is not None:\n",
    "    index_clients_both = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "    text_index_client = [(text_os_client, text_index_name)]\n",
    "    img_index_client = [(img_os_client, img_index_name)]\n",
    "    for i, question in enumerate(eval_df[config['eval_qna_dataset_info']['question_key']]):\n",
    "        combined_response = get_index_response(question, config['k_count_retrieval'], index_clients_both)\n",
    "        eval_df.at[i, 'combined_response'] = combined_response['response']\n",
    "        logger.info(f\"combined_response['response']: {combined_response['response']}\")\n",
    "        eval_df.at[i, 'image_and_text_source'] = combined_response['source']    \n",
    "        text_response = get_index_response(question, config['k_count_retrieval'], text_index_client)\n",
    "        eval_df.at[i, 'text_response'] = text_response['response']\n",
    "        eval_df.at[i, 'text_source'] = text_response['source']\n",
    "        image_response = get_index_response(question, config['k_count_retrieval'], img_index_client)\n",
    "        eval_df.at[i, 'img_response'] = image_response['response']\n",
    "        eval_df.at[i, 'img_source'] = image_response['source']\n",
    "    print(eval_df.head(10))\n",
    "    metrics_dir = config['metrics_dir']['dir_name']\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    side_view_eval_file = os.path.join(metrics_dir, config['eval_qna_dataset_info']['updated_eval_file'])\n",
    "    eval_df.to_csv(side_view_eval_file, index=False)\n",
    "else:\n",
    "    logger.info(f\"Evaluation dataset not provided. Provide a data set in the eval directory and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
