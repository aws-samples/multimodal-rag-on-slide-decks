{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Retrieval Augumented Generation (RAG) Inference\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "1. Takes in a user question, and extracts entities from that question\n",
    "\n",
    "1. Uses the entities from the question, performs `prefiltering` and extracts the top `k` hits from the index based on the `entities matching`\n",
    "\n",
    "1. Uses an `LLM in the loop` to go over each `k` hit, check for if the answer to the question is given in that hit and if not, move to the next `hit` until the answer is found. If the answer is not found in any, return `I don't know`.\n",
    "\n",
    "1. Use an eval dataset that a user provides with a question bank, iterate through each question and query the `text` and the `image indexes` to look for answers to the questions in the eval dataset.\n",
    "\n",
    "1. During retrieval, answers are searched from both, the `text` and the `image` index to provide a combined answer.\n",
    "\n",
    "***Make sure to check for the data ingested into both indices. Data ingestion might take a couple of minutes depending on the amount of data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install the requirements\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries to run this notebook\n",
    "import os\n",
    "import ray\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import boto3\n",
    "import logging\n",
    "import litellm\n",
    "import requests\n",
    "import botocore\n",
    "import opensearchpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from litellm import completion\n",
    "from IPython.display import Image\n",
    "from urllib.parse import urlparse\n",
    "from botocore.auth import SigV4Auth\n",
    "from pandas.core.series import Series\n",
    "from sagemaker import get_execution_role\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_text_embedding, get_llm_response, get_question_entities, load_and_merge_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the merged config file - user config file, and parent config file\n",
    "config = load_and_merge_configs(g.CONFIG_SUBSET_FILE, g.FULL_CONFIG_FILE)\n",
    "logger.info(f\"config file -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region: str = boto3.Session().region_name\n",
    "os_service: str = config['aws']['os_service']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. Create two OpenSearch clients for images and texts separately\n",
    "---\n",
    "\n",
    "We create an OpenSearch client so that we can query the vector database for embeddings (pdf files) similar to the questions that we might want to ask of our `PDF file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the OpenSearch Service Serverless collection endpoint and index name from the CloudFormation stack outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = get_cfn_outputs(config['aws']['cfn_stack_name'])\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "text_index_name = outputs['OpenSearchTextIndexName']\n",
    "img_index_name = outputs['OpenSearchImgIndexName']\n",
    "logger.info(f\"opensearchhost={host}, text index={text_index_name}, image index={img_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, os_service)\n",
    "\n",
    "# Represents the OSI client\n",
    "os_client= OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Read for RAG\n",
    "\n",
    "We now have all the pieces for RAG. Here is how we talk to our PDF files.\n",
    "\n",
    "1. Extract entities from the `question`.\n",
    "\n",
    "1. Using the entites in the question, perform a `prefilter` step to get the content that matches the entities from the question in the `text` and `image` indexes the most.\n",
    "\n",
    "1. Based on the top `k` results, use an `LLM in a loop` that you can configure in the [config.yaml](`config.yaml`) file as a check to verify if the answer to the question is in that `hit` and if not, move to the next `hit` to look for an answer. \n",
    "\n",
    "1. Responses from each image (if valid and if they are contained in the `hit`) are added as context to the final context. At the end, all the responses are aggregated into a final context that is run through by a final LLM call to give a final response to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_url: str = g.BEDROCK_EP_URL.format(region=region)\n",
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform `prefiltering`\n",
    "---\n",
    "\n",
    "The `find_similar_data_with_entities` function performs a [knn-prefiltering]('https://opensearch.org/docs/latest/search-plugins/knn/filter-search-knn/'). It takes in the question entites extracted from the question asked, and searches for relevant docs that have similar entities in the `metadata.entities` field that was added during the `data ingestion` step for both `text` and `image` indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_data_with_entities(text_embedding, size: int, os_client, index_name: str, question_entities: List[str], prefiltering: bool):\n",
    "    \"\"\"\n",
    "    This function is used to prefilter the responses only with images/texts that have entities that match\n",
    "    with the entities provided in the question. Once the documents are refiltered, the search from the index\n",
    "    is returned.\n",
    "    \"\"\"\n",
    "    should_clauses: List[str] = []\n",
    "    if prefiltering:\n",
    "        # Convert filter_string to lowercase to ensure case-insensitive matching\n",
    "        q_entities_lower: str = question_entities.lower()\n",
    "        q_entities_upper: str = question_entities.upper()\n",
    "        # entities can either be lower case or upper case. search for both cases is supported\n",
    "        question_entities_variants = [q_entities_lower, q_entities_upper]\n",
    "        logger.info(f\"Entities extracted: {question_entities_variants}\")\n",
    "        for word in question_entities_variants:\n",
    "            for entity in word.split(\",\"):\n",
    "                should_clauses.append({\n",
    "                    \"wildcard\": {\n",
    "                        \"metadata.entities\": {\n",
    "                            # wildcard queries are to search for terms that match a wildcard pattern\n",
    "                            # in this case, we are searching for an entity within the text embedding\n",
    "                            \"value\": f\"*{entity.strip()}*\",\n",
    "                            \"case_insensitive\": True\n",
    "                        }\n",
    "                    }\n",
    "                })\n",
    "    else:\n",
    "        logger.info(\"Prefiltering disabled\")\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"knn\": {\n",
    "                        \"vector_embedding\": {\n",
    "                            \"vector\": text_embedding,\n",
    "                            \"k\": size\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": should_clauses,\n",
    "                        # should match at least a single entity to fetch a response. Increase if there\n",
    "                        # is a lot being asked in the question\n",
    "                        \"minimum_should_match\": config['inference_info']['minimum_entities_to_match_from_question']\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        content_based_search = os_client.search(body=query, index=index_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"error occured while querying OpenSearch index={index_name}, exception={e}\")\n",
    "        content_based_search = None\n",
    "    return content_based_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nearest_img_search_response(nearest_image_path: str, prompt: str, modelId: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the file path that is most similar to the text embeddings\n",
    "    of the question, returns the image and checks for if the text description does not\n",
    "    contain the answer, directly search for the answer in the selected image.\n",
    "    \"\"\"\n",
    "   # extract the file name from the nearest image path stored in s3\n",
    "    filename: str = os.path.basename(nearest_image_path)\n",
    "    local_directory: str = os.path.join(g.IMAGE_DIR, 'b64_images')\n",
    "    local_image_path = os.path.join(local_directory, filename.replace('.jpg', '.b64'))\n",
    "    logger.info(f\"Nearest image path being used for search: {local_image_path}\")\n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    try:\n",
    "        with open(local_image_path, \"rb\") as image_file:\n",
    "            input_image_b64 = image_file.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading base64 image from local directory: {e}\")\n",
    "        return None\n",
    "    temperature = config['inference_parameters'].get('temperature', 0.1)\n",
    "    max_tokens = config['inference_parameters'].get('max_tokens', 500)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,\" + input_image_b64\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    # suppress the litellm logger responses\n",
    "    lite_llm_logger = logging.getLogger('LiteLLM')\n",
    "    lite_llm_logger.setLevel(logging.CRITICAL)\n",
    "    ret = {\n",
    "        \"exception\": None,\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": None,\n",
    "        \"time_taken_in_seconds\":None,\n",
    "        \"completion_token_count\": None,\n",
    "        \"prompt_token_count\": None,\n",
    "        \"model_id\": modelId,\n",
    "        \"input_token_cost\": None,\n",
    "        \"output_token_cost\": None,\n",
    "    }\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=modelId,\n",
    "            messages=messages,\n",
    "            temperature=temperature, \n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        # Suppress logging output\n",
    "        logging.getLogger('LiteLLM').setLevel(logging.CRITICAL)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        ret[\"exception\"] = e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response_from_text_extracted(prompt: str, modelId: str) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes in the prompt that checks whether the text file has a response to the question and if not, \n",
    "    returns \"not found\" to move to the next hit.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    temperature = config['inference_parameters'].get('temperature', 0.1)\n",
    "    max_tokens = config['inference_parameters'].get('max_tokens', 500)\n",
    "    # suppress the litellm logger responses\n",
    "    lite_llm_logger = logging.getLogger('LiteLLM')\n",
    "    lite_llm_logger.setLevel(logging.CRITICAL)\n",
    "    ret = {\n",
    "        \"exception\": None,\n",
    "        \"prompt\": prompt,\n",
    "        \"completion\": None,\n",
    "        \"completion_token_count\": None,\n",
    "        \"prompt_token_count\": None,\n",
    "        \"model_id\": modelId, \n",
    "        \"time_taken_in_seconds\":None,\n",
    "        \"input_token_cost\": None,\n",
    "        \"output_token_cost\": None,\n",
    "    }\n",
    "    try:\n",
    "        response = completion(\n",
    "            model=modelId,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        # Suppress logging output\n",
    "        logging.getLogger('LiteLLM').setLevel(logging.CRITICAL)\n",
    "        # iterate through the entire model response\n",
    "        for idx, choice in enumerate(response.choices):\n",
    "            # extract the message and the message's content from litellm\n",
    "            if choice.message and choice.message.content:\n",
    "                # extract the response from the dict\n",
    "                ret[\"completion\"] = choice.message.content.strip()\n",
    "        # Extract number of input and completion prompt tokens (this is the same structure for embeddings and text generation models on Amazon Bedrock)\n",
    "        ret['prompt_token_count'] = response.usage.prompt_tokens\n",
    "        ret['completion_token_count'] = response.usage.completion_tokens\n",
    "        # Extract latency in seconds\n",
    "        latency_ms = response._response_ms\n",
    "        ret['time_taken_in_seconds']  = latency_ms / 1000\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        ret[\"exception\"] = e\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sanitize_llm_response(llm_response: str) -> str:\n",
    "    \"\"\"\n",
    "    This function sanitizes the LLM response generated. If the LLM response contains a sentence that has \n",
    "    \"not found\" or \"Not found\" within it, it returns only \"not found\" and no other characters. Case sensitivity does not matter.\n",
    "    \"\"\"\n",
    "    string_to_match: str = \"not found\"\n",
    "    sanitized_response: Optional[str] = None\n",
    "    try:\n",
    "        # Normalize the case for comparison\n",
    "        normalized_response = llm_response.lower()\n",
    "        if llm_response is None:\n",
    "            sanitized_response: Optional[str] = None\n",
    "        # Check if the normalized response contains the string \"not found\"\n",
    "        if string_to_match in normalized_response:\n",
    "            sanitized_response = string_to_match\n",
    "        else:\n",
    "            sanitized_response = normalized_response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The LLM response cannot be sanitized: {e}\")\n",
    "        sanitized_response: Optional[str] = None\n",
    "    return sanitized_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts used in the RAG worklow\n",
    "---\n",
    "\n",
    "1. `direct_image_answer_retrieval_prompt`: This prompt is used for a check against retrieval from the `image index`. It checks for if the answer to the user question is given in the `image description` and if not, then it searches the image directly. If the answer is not in the `image description` or the `image` itself, it returns a `not found` message.\n",
    "\n",
    "1. `direct_text_answer_retrieval_prompt`: This prompt is used for a check against retrieval from the `text index`. It checks for if the answer to the user question is given in the `text extracted` from the pdf page and if not, then returns a `not found` message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_retrieval_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'],\n",
    "                                                 config['dir_info']['search_in_images_template'])\n",
    "\n",
    "txt_retrieval_prompt_fpath: str = os.path.join(config['dir_info']['prompt_dir'],\n",
    "                                                 config['dir_info']['search_in_text_template'])\n",
    "# read the image and text retrieval prompts that will be used in the RAG pipeline\n",
    "direct_image_answer_retrieval_prompt: str = Path(image_retrieval_prompt_fpath).read_text()\n",
    "direct_text_answer_retrieval_prompt: str = Path(txt_retrieval_prompt_fpath).read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"image retrieval prompt: {direct_image_answer_retrieval_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"text retrieval prompt: {direct_text_answer_retrieval_prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Inference: `Enhanced Multimodal RAG workflow`\n",
    "---\n",
    "\n",
    "The function `get_index_response` performs the following steps:\n",
    "\n",
    "1. It takes in a list of tuples. Each tuple contains the `osi client` and `index name`.\n",
    "\n",
    "1. Given both text and image indexes, it iterates through each tuple.\n",
    "\n",
    "1. Extracts the entities from the `user question`, gets the text embeddings of the question and searches for the nearest hits using those entities as a `prefilter`.\n",
    "\n",
    "1. After fetching the `vector db response`, it iterates through each content in every hit. For an image index, it uses an `LLM in the loop` to check for whether the answer is given in the image index and if not searches the image directly. If none have an answer, it returns a `not found` and moves to the next hit. For the text index, it uses an `LLM in the loop` to check if the extracted text has the answer to the question and if not, returns a `not found` message and moves to the next hit.\n",
    "\n",
    "1. For all the valid responses from each hit and each index, the answer is stored as context for a final `LLM call`. Once all hits are traversed, an `LLM` is invoked to check if the context (that contains answers from all hits) contains the actual answer to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define global variables to track the total input tokens and output tokens for both the image\n",
    "# and text response indices. This is tracked to give an average token count for a user query \n",
    "# at the end of the inference run\n",
    "# Input tokens\n",
    "INPUT_TOKENS_IMG: int = 0\n",
    "INPUT_TOKENS_TEXT: int = 0\n",
    "# Output tokens\n",
    "OUTPUT_TOKENS_IMG: int = 0\n",
    "OUTPUT_TOKENS_TEXT: int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_response(question: str,\n",
    "                       size: int,\n",
    "                       index_clients: List[Tuple[opensearchpy.client.OpenSearch, str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Get LLM responses from retrieved data on questions asked from image, text, or both indexes combined.\n",
    "    :param question: Question that a user asks on the content.\n",
    "    :param size: 'k' size.\n",
    "    :param index_clients: List of tuples containing OpenSearch clients and index names.\n",
    "    :return: Dictionary with the context used to answer the question and the final response.\n",
    "    \"\"\"\n",
    "    index_llm_response_and_context = {'source': ''}\n",
    "    model_id = config['model_info']['inference_model_info'].get('model_id')\n",
    "    # initialize all relevant hits that are returned during vector search\n",
    "    all_hits = []\n",
    "    # Initialize latency counters\n",
    "    total_image_latency: int = 0.0\n",
    "    total_text_latency: int = 0.0\n",
    "\n",
    "    try:\n",
    "        bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=endpoint_url)\n",
    "        logger.info(f\"Going to answer the question: {question}\")\n",
    "        # iterate through each index tuple\n",
    "        for os_client, index_name in index_clients:\n",
    "            logger.info(f\"Searching for the answer in the {index_name} index:\")\n",
    "            # get the entities from the user question, and search for similar data \n",
    "            # and perform prefilter to get the most relevant documents\n",
    "            question_entities = get_question_entities(bedrock, question, model_id)\n",
    "            text_embedding = get_text_embedding(bedrock, question)\n",
    "            # perform pre filtering to get the most relevant documents\n",
    "            #later read in from config file\n",
    "            prefiltering = True\n",
    "            vector_db_response = find_similar_data_with_entities(text_embedding, size, os_client, index_name, question_entities, prefiltering)\n",
    "            if vector_db_response:\n",
    "                logger.info(f\"prefiltering set\")\n",
    "                hits = vector_db_response.get('hits', {}).get('hits', [])\n",
    "                if len(hits) == 0:\n",
    "                    prefiltering = False\n",
    "                    vector_db_response = find_similar_data_with_entities(text_embedding, size, os_client, index_name, question_entities, prefiltering)\n",
    "                    logger.info(f\"prefiltering disabled\")\n",
    "                    if vector_db_response:\n",
    "                        hits = vector_db_response.get('hits', {}).get('hits', [])\n",
    "                        logger.info(f\"currently have a hits list\")\n",
    "                    else:\n",
    "                        logger.info(f\"No vector_db_response\")\n",
    "                else:\n",
    "                    logger.info(f\"Hits length is non-zero\")\n",
    "                all_hits.extend([(hit['_source']['file_path'], hit['_source']['file_text'], index_name) for hit in hits])\n",
    "        logger.info(f\"Iterating through all relevant hits to search for an answer....\")\n",
    "\n",
    "        # iterate through each of the content fetched from the vectorDB\n",
    "        for content_path, extracted_text, index_name in all_hits:\n",
    "            file_text = \"\"\n",
    "            # for the image index, display each image that is used in the search and\n",
    "            # retrieve the response to the user question\n",
    "            if index_name == outputs['OpenSearchImgIndexName']:\n",
    "                !aws s3 cp {content_path} .\n",
    "                local_img_path = os.path.basename(content_path)\n",
    "                display(Image(filename=local_img_path))\n",
    "                # search for the answer in the image description and if not, directly in the image\n",
    "                search_in_img_prompt = direct_image_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_image_response = get_nearest_img_search_response(content_path, search_in_img_prompt, model_id)\n",
    "                # calculate the input and output token pricing\n",
    "                direct_image_response['input_token_cost'] = (direct_image_response['prompt_token_count']/1000) * config['model_info']['eval_model_info'].get('input_tokens_price')\n",
    "                direct_image_response['output_token_cost'] = (direct_image_response['completion_token_count']/1000) * config['model_info']['eval_model_info'].get('output_tokens_price')\n",
    "                # sanitize the response if the response contains \"not found\"\n",
    "                sanitized_response = sanitize_llm_response(direct_image_response['completion'])\n",
    "\n",
    "                # record the latency for image search\n",
    "                total_image_latency += direct_image_response['time_taken_in_seconds']\n",
    "                logger.info(f\"latency recorded at this image index iteration: {total_image_latency}\")\n",
    "                # update the total image input and output tokens\n",
    "                global INPUT_TOKENS_IMG, OUTPUT_TOKENS_IMG\n",
    "                INPUT_TOKENS_IMG += direct_image_response['prompt_token_count']\n",
    "                OUTPUT_TOKENS_IMG += direct_image_response['completion_token_count']\n",
    "                logger.info(f\"current img input and output tokens: {INPUT_TOKENS_IMG}, {OUTPUT_TOKENS_IMG}\")\n",
    "                # if the response is found, then append the answer to the context for the final LLM call\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response FOUND from the image {content_path}, exiting out of the search process.\")\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "                    if config['other_inference_and_eval_metrics']['summarize_all_hits'] is False:\n",
    "                        break\n",
    "\n",
    "            # for the text index\n",
    "            elif index_name == outputs['OpenSearchTextIndexName']:\n",
    "                # search for the response in the texts extracted from PDFs\n",
    "                search_in_txt_prompt = direct_text_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_text_response = response_from_text_extracted(search_in_txt_prompt, model_id)\n",
    "                # calculate the input and output token pricing\n",
    "                direct_text_response['input_token_cost'] = (direct_text_response['prompt_token_count']/1000) * config['model_info']['eval_model_info'].get('input_tokens_price')\n",
    "                direct_text_response['output_token_cost'] = (direct_text_response['completion_token_count']/1000) * config['model_info']['eval_model_info'].get('output_tokens_price')\n",
    "                # sanitize the response if the response contains \"not found\"\n",
    "                sanitized_response = sanitize_llm_response(direct_text_response['completion'])\n",
    "\n",
    "                # record the latency for the text search\n",
    "                total_text_latency += direct_text_response['time_taken_in_seconds']\n",
    "                logger.info(f\"latency recorded at this image index iteration: {total_text_latency}\")\n",
    "                # update the total text input and output tokens\n",
    "                global INPUT_TOKENS_TEXT, OUTPUT_TOKENS_TEXT\n",
    "                INPUT_TOKENS_TEXT += direct_text_response['prompt_token_count']\n",
    "                OUTPUT_TOKENS_TEXT += direct_text_response['completion_token_count']\n",
    "                logger.info(f\"current text input and output tokens: {INPUT_TOKENS_TEXT}, {OUTPUT_TOKENS_TEXT}\")\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response FOUND from the text {content_path}, exiting out of the search process.\")\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "                    if config['other_inference_and_eval_metrics']['summarize_all_hits'] is False:\n",
    "                        break\n",
    "        # get the final response from a final LLM call from all iterations\n",
    "        index_llm_response = get_llm_response(question, index_llm_response_and_context['source'], model_id)\n",
    "        index_llm_response_and_context.update({\n",
    "            'response': index_llm_response['completion'],\n",
    "            'image_input_tokens': INPUT_TOKENS_IMG,\n",
    "            'image_output_tokens': OUTPUT_TOKENS_IMG,\n",
    "            'text_input_tokens': INPUT_TOKENS_TEXT,\n",
    "            'text_output_tokens': OUTPUT_TOKENS_TEXT, \n",
    "            'total_input_tokens': (INPUT_TOKENS_IMG + INPUT_TOKENS_TEXT + index_llm_response['prompt_token_count']),\n",
    "            'total_output_tokens': (OUTPUT_TOKENS_IMG + OUTPUT_TOKENS_TEXT + index_llm_response['completion_token_count']),\n",
    "            'total_image_latency': total_image_latency,\n",
    "            'total_text_latency': total_text_latency,\n",
    "            'total_combined_latency': (index_llm_response['time_taken_in_seconds'] + total_image_latency + total_text_latency)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not get a response: {e}\")\n",
    "        index_llm_response_and_context = None\n",
    "    return index_llm_response_and_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Inference: Inference format\n",
    "---\n",
    "\n",
    "Use the format below to get inference from the given index (text, image, or both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"<enter your example question here>\"\n",
    "# # to get a combined response from both image and text indexes, pass in a list of both tuple variations\n",
    "# index_clients: List[Tuple] = [(os_client, text_index_name), (os_client, img_index_name)]\n",
    "\n",
    "# # to get a response only from the text index\n",
    "# # index_clients: List[Tuple] = [(os_client, text_index_name)]\n",
    "\n",
    "# # to get a response only from the image index\n",
    "# # index_clients: List[Tuple] = [(os_client, img_index_name)]\n",
    "# get_index_response(question, config['other_inference_and_eval_metrics']['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "---\n",
    "\n",
    "We will ask a question about an architecture image on page 28 about a specific portion in the architecture.\n",
    "\n",
    "*Uncomment the cells below and run them to see the logs and final response*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"What are the Trade-offs between performance and model interpretability graph?\"\n",
    "# index_clients: List[Tuple] = [(os_client, img_index_name), (os_client, text_index_name)]\n",
    "# get_index_response(question, config['other_inference_and_eval_metrics']['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "---\n",
    "\n",
    "We will ask a question about a specific text in the pdf file on page 22 about automating data integration and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"For automating data integration deployment, why do we have to minimize human touch point in deployment pipelines?\"\n",
    "# index_clients: List[Tuple] = [(os_client, img_index_name), (os_client, text_index_name)]\n",
    "# get_index_response(question, config['other_inference_and_eval_metrics']['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "---\n",
    "\n",
    "We will ask an open ended question about an image and text on page 33 about operationalizing AI/ML workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question: str = \"In Operationalizing AI/ML workloads, what comes under 'Prepare for production'?\"\n",
    "index_clients: List[Tuple] = [(os_client, img_index_name), (os_client, text_index_name)]\n",
    "get_index_response(question, config['other_inference_and_eval_metrics']['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Eval Dataset Comparison\n",
    "---\n",
    "\n",
    "In this section of the notebook we do as follows:\n",
    "\n",
    "1. Check for if a user has provided an `evaluation dataset` - Any dataset with a question bank and/or target responses\n",
    "\n",
    "1. Iterate through each question provided in the dataset, and call the `get_index_response` function\n",
    "\n",
    "1. Record responses from the text response, the image response and combined responses\n",
    "\n",
    "1. Update the df and store the result in the `eval directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(config['eval_qna_dataset_info']['dir_name'], exist_ok=True)\n",
    "eval_dataset_file_list = os.listdir(config['eval_qna_dataset_info']['dir_name'])\n",
    "logger.info(f\"Number of evaluation files: {len(eval_dataset_file_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config['eval_qna_dataset_info']['eval_dataset_name'] in eval_dataset_file_list:\n",
    "    eval_dir = config['eval_qna_dataset_info']['dir_name']\n",
    "    eval_fpath = os.path.join(eval_dir, config['eval_qna_dataset_info']['eval_dataset_name'])\n",
    "    eval_file = Path(eval_fpath)\n",
    "    if eval_file.suffix == '.csv':\n",
    "        eval_df = pd.read_csv(eval_file,encoding='latin-1')\n",
    "    elif eval_file.suffix in ['.xls', '.xlsx']:\n",
    "        eval_df = pd.read_excel(eval_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {eval_file.suffix}\")\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get responses from combined (both text and image index), along with metrics such as the input/output token count, and latency\n",
    "---\n",
    "\n",
    "1. This part of the notebook iterates through an evaluation dataset (if any)\n",
    "\n",
    "1. Iterates through each user question in a column and generates outputs using the three different ways (combined index approach, text only and image only approaches)\n",
    "\n",
    "1. Appends all responses from each approach to the original dataframe and saves it as a `CSV` file in the `metrics` directory for further evaluations and downstream tasks.\n",
    "\n",
    "1. Calculates the aggregate metrics from the image, text and combined (image and text responses) such as the prompt token count, completion token count, and latency (in seconds) at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the index clients to run inferences against the evaluation dataset provided by the user\n",
    "index_clients_both = [(os_client, text_index_name), (os_client, img_index_name)]\n",
    "text_index_client = [(os_client, text_index_name)]\n",
    "img_index_client = [(os_client, img_index_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through the evaluation dataframe and record responses for the question bank\n",
    "# using the text, image and both indexes combined for further evaluations\n",
    "def evaluate_responses(eval_df, index_clients_both, text_index_client, img_index_client):\n",
    "    try:\n",
    "        question_key = config['eval_qna_dataset_info']['question_key']\n",
    "        k_count_retrieval = config['other_inference_and_eval_metrics']['k_count_retrieval']\n",
    "        for i, question in enumerate(eval_df[question_key]):\n",
    "            # populate the dataframe with the combined response, token counts, and source\n",
    "            combined_response = get_index_response(question, k_count_retrieval, index_clients_both)\n",
    "            eval_df.at[i, 'combined_response'] = combined_response['response']\n",
    "            eval_df.at[i, 'combined_total_input_tokens'] = combined_response['total_input_tokens']\n",
    "            eval_df.at[i, 'combined_total_output_tokens'] = combined_response['total_output_tokens']\n",
    "            eval_df.at[i, 'total_combined_latency'] = combined_response['total_combined_latency']\n",
    "            eval_df.at[i, 'image_and_text_source'] = combined_response['source']\n",
    "            # populate the dataframe with response from the text only index\n",
    "            text_response = get_index_response(question, k_count_retrieval, text_index_client)\n",
    "            eval_df.at[i, 'text_response'] = text_response['response']\n",
    "            eval_df.at[i, 'text_index_input_tokens'] = text_response['text_input_tokens']\n",
    "            eval_df.at[i, 'text_index_output_tokens'] = text_response['text_output_tokens']\n",
    "            eval_df.at[i, 'total_text_latency'] = text_response['total_text_latency']\n",
    "            eval_df.at[i, 'text_source'] = text_response['source']\n",
    "            # populate the dataframe with response from the image only index\n",
    "            image_response = get_index_response(question, k_count_retrieval, img_index_client)\n",
    "            eval_df.at[i, 'img_response'] = image_response['response']\n",
    "            eval_df.at[i, 'image_index_input_tokens'] = image_response['image_input_tokens']\n",
    "            eval_df.at[i, 'image_index_output_tokens'] = image_response['image_output_tokens']\n",
    "            eval_df.at[i, 'total_image_latency'] = image_response['total_image_latency']\n",
    "            eval_df.at[i, 'img_source'] = image_response['source']\n",
    "            logger.info(f\"Combined response: {combined_response['response']}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Cannot generate inferences to the provided questions in the dataset: {e}\")\n",
    "        eval_df = None\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_eval_df = evaluate_responses(eval_df, index_clients_both, text_index_client, img_index_client)\n",
    "metrics_dir = config['dir_info']['metrics_dir_name']\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "side_view_eval_file = os.path.join(metrics_dir, config['eval_qna_dataset_info']['updated_eval_file'])\n",
    "final_eval_df.to_csv(side_view_eval_file, index=False)\n",
    "final_eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics_dir = config['dir_info']['metrics_dir_name']\n",
    "os.makedirs(metrics_dir, exist_ok=True)\n",
    "side_view_eval_file = os.path.join(metrics_dir, \"updated_eval_dataset.csv\")\n",
    "eval_df.to_csv(side_view_eval_file, index=False)\n",
    "eval_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the p95 count for the input/output token lengths and total latency for text, image, and combined search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(final_eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latency_cols_of_interest = ['total_text_latency', 'total_image_latency', 'total_combined_latency']\n",
    "token_cols_of_interest = [\n",
    "    'combined_total_input_tokens', 'combined_total_output_tokens',\n",
    "    'text_index_input_tokens', 'text_index_output_tokens',\n",
    "    'image_index_input_tokens', 'image_index_output_tokens'\n",
    "]\n",
    "\n",
    "p95_metrics_summary: str = \"\"\n",
    "\n",
    "# Calculate p95 for latency columns\n",
    "for c in latency_cols_of_interest:\n",
    "    quantiles = list(round(final_eval_df[c].quantile([0.5, 0.95]), 2))\n",
    "    s = f\"[p50, p95] for {c} = {quantiles}\\n\"\n",
    "    p95_metrics_summary += s\n",
    "    logger.info(s)\n",
    "\n",
    "# Calculate p95 for token count columns\n",
    "for c in token_cols_of_interest:\n",
    "    quantiles = list(round(final_eval_df[c].quantile([0.5, 0.95]), 2))\n",
    "    s = f\"[p50, p95] for {c} = {quantiles}\\n\"\n",
    "    p95_metrics_summary += s\n",
    "    logger.info(s)\n",
    "\n",
    "p95_metric_fpath: str = os.path.join(metrics_dir, config['dir_info']['p95_metrics_file'])\n",
    "Path(p95_metric_fpath).write_text(p95_metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(p95_metrics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
