{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augumented Generation (RAG) inference\n",
    "\n",
    "***This notebook works best with the `conda_python3` on the `ml.t3.large` instance***.\n",
    "\n",
    "---\n",
    "\n",
    "At this point our slide deck data is ingested into Amazon OpenSearch Service Serverless collection. We are now ready to talk to our slide deck using a large multimodal model. We are using the [Anthropicâ€™s Claude 3 Sonnet foundation model](https://aws.amazon.com/about-aws/whats-new/2024/03/anthropics-claude-3-sonnet-model-amazon-bedrock/) for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1. Setup\n",
    "\n",
    "Install the required Python packages and import the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries to run this notebook\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import glob\n",
    "import boto3\n",
    "import base64\n",
    "import logging\n",
    "import requests\n",
    "import botocore\n",
    "import sagemaker\n",
    "import opensearchpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import globals as g\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from IPython.display import Image\n",
    "from urllib.parse import urlparse\n",
    "from botocore.auth import SigV4Auth\n",
    "from pandas.core.series import Series\n",
    "from sagemaker import get_execution_role\n",
    "from botocore.awsrequest import AWSRequest\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from utils import get_cfn_outputs, get_text_embedding, get_llm_response, get_question_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set a logger\n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "CONFIG_FILE_PATH = \"config.yaml\"\n",
    "# read the config yaml file\n",
    "fpath = CONFIG_FILE_PATH\n",
    "with open(fpath, 'r') as yaml_in:\n",
    "    config = yaml.safe_load(yaml_in)\n",
    "logger.info(f\"config read from {fpath} -> {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize globals.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2. Create two OpenSearch clients for images and texts separately\n",
    "\n",
    "We create an OpenSearch client so that we can query the vector database for embeddings (pdf files) similar to the questions that we might want to ask of our `PDF file`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the OpenSearch Service Serverless collection endpoint and index name from the CloudFormation stack outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = get_cfn_outputs(config['aws']['cfn_stack_name'])\n",
    "host = outputs['MultimodalCollectionEndpoint'].split('//')[1]\n",
    "text_index_name = outputs['OpenSearchTextIndexName']\n",
    "img_index_name = outputs['OpenSearchImgIndexName']\n",
    "logger.info(f\"opensearchhost={host}, text index={text_index_name}, image index={img_index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "credentials = session.get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, g.AWS_REGION, g.OS_SERVICE)\n",
    "\n",
    "# Represents the OSI client for images\n",
    "img_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "# Represents the OSI client for images\n",
    "text_os_client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Read for RAG\n",
    "\n",
    "We now have all the pieces for RAG. Here is how we _talk to our slide deck_.\n",
    "\n",
    "1. Convert the user question into embeddings using the Titan Text Embeddings model.\n",
    "\n",
    "1. Find the most similar slide (image) corresponding to the the embeddings (for the user question) from the vector database (OpenSearch Serverless).\n",
    "\n",
    "1. Now ask Claude3 to answer the user question using the retrieved image description for the most similar slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name=\"bedrock-runtime\", endpoint_url=g.TITAN_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy function for similarity search in the vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_similar_data_with_string_filter(text_embedding, size, os_client, index_name, filter_string):\n",
    "    \"\"\"\n",
    "    This function is used to prefilter the responses only with images/texts that have entities that match\n",
    "    with the entities provided in the question. Once the documents are refiltered, the search from the index\n",
    "    is returned.\n",
    "    \"\"\"\n",
    "    logger.info(f\"filter_string: {filter_string}\")\n",
    "    should_clauses = []\n",
    "    # Convert filter_string to lowercase to ensure case-insensitive matching\n",
    "    filter_string_lower = filter_string.lower()\n",
    "    filter_string_upper = filter_string.upper()\n",
    "    # entities can either be lower case or upper case. search for both cases is supported\n",
    "    filter_string_variants = [filter_string_lower, filter_string_upper]\n",
    "    logger.info(f\"Entities extracted: {filter_string_variants}\")\n",
    "    for word in filter_string_variants:\n",
    "        for entity in word.split(\",\"):\n",
    "            should_clauses.append({\n",
    "                \"wildcard\": {\n",
    "                    \"metadata.entities\": {\n",
    "                        # wildcard queries are to search for terms that match a wildcard pattern\n",
    "                        # in this case, we are searching for an entity within the text embedding\n",
    "                        \"value\": f\"*{entity.strip()}*\",\n",
    "                        \"case_insensitive\": True\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"knn\": {\n",
    "                        \"vector_embedding\": {\n",
    "                            \"vector\": text_embedding,\n",
    "                            \"k\": size\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": should_clauses,\n",
    "                        # should match at least a single entity to fetch a response. Increase if there\n",
    "                        # is a lot being asked in the question\n",
    "                        \"minimum_should_match\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        content_based_search = os_client.search(body=query, index=index_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"error occured while querying OpenSearch index={index_name}, exception={e}\")\n",
    "        content_based_search = None\n",
    "    return content_based_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nearest_img_search_response(nearest_image_path: str, prompt: str, modelId: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the file path that is most similarto the text embeddings\n",
    "    of the question, returns the image and checks for if the text description does not\n",
    "    contain the answer, directly search for the answer in the selected image.\n",
    "    \"\"\"\n",
    "    bedrock = boto3.client(service_name=\"bedrock-runtime\", region_name=g.AWS_REGION, endpoint_url=g.TITAN_URL)\n",
    "    # extract the file name from the nearest image path stored in s3\n",
    "    filename: str = os.path.basename(nearest_image_path)\n",
    "    local_directory: str = os.path.join(g.IMAGE_DIR, 'b64_images')\n",
    "    local_image_path = os.path.join(local_directory, filename.replace('.jpg', '.b64'))\n",
    "    print(local_image_path)\n",
    "    # read the file, MAX image size supported is 2048 * 2048 pixels\n",
    "    try:\n",
    "        with open(local_image_path, \"rb\") as image_file:\n",
    "            input_image_b64 = image_file.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading base64 image from local directory: {e}\")\n",
    "        return None\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 2000,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"image\",\n",
    "                            \"source\": {\n",
    "                                \"type\": \"base64\",\n",
    "                                \"media_type\": \"image/jpeg\",\n",
    "                                \"data\": input_image_b64\n",
    "                            },\n",
    "                        },\n",
    "                        {\"type\": \"text\", \"text\": prompt},\n",
    "                    ],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId=modelId,\n",
    "        body=body\n",
    "    )\n",
    "\n",
    "    resp_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "    resp_text = resp_body['content'][0]['text'].replace('\"', \"'\")\n",
    "    return resp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response_from_text_extracted(bedrock: botocore.client, \n",
    "                     prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in the prompt that checks whether the text file has a response and if not, \n",
    "    returns a \"not found\"\n",
    "    \"\"\"\n",
    "    modelId=config['bedrock_model_info']['claude_sonnet_model_id']\n",
    "    body = json.dumps(\n",
    "    {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock.invoke_model(\n",
    "        modelId=modelId,\n",
    "        body=body)\n",
    "\n",
    "        response_body = json.loads(response['body'].read().decode(\"utf-8\"))\n",
    "        llm_response = response_body['content'][0]['text'].replace('\"', \"'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"exception={e}\")\n",
    "        llm_response = None\n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_llm_response(llm_response: str) -> str:\n",
    "    \"\"\"\n",
    "    This function sanitizes the LLM response generated. If the LLM response contains a sentence that has \n",
    "    \"not found\" or \"Not found\" within it, it returns only \"not found\". Case sensitivity does not matter.\n",
    "    \"\"\"\n",
    "    string_to_match: str = \"not found\"\n",
    "    sanitized_response: Optional[str] = None\n",
    "    try:\n",
    "        # Normalize the case for comparison\n",
    "        normalized_response = llm_response.lower()\n",
    "        if llm_response is None:\n",
    "            sanitized_response: Optional[str] = None\n",
    "        # Check if the normalized response contains the string \"not found\"\n",
    "        if string_to_match in normalized_response:\n",
    "            sanitized_response = string_to_match\n",
    "        else:\n",
    "            sanitized_response = normalized_response\n",
    "    except Exception as e:\n",
    "        logger.error(f\"The LLM response cannot be sanitized: {e}\")\n",
    "        sanitized_response: Optional[str] = None\n",
    "    return sanitized_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get response from indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "direct_image_answer_retrieval_prompt: str = \"\"\"\n",
    "Human: Your role is to give the answer to the question in the <question></question> tags. If the image description in the <img_text_desc></img_text_desc> tags does not contain the answer to the question, then search the \"data\" for it. \n",
    "If the img_text_desc or the actual image in the \"data\", both do not contain the answer to the question, then respond with two words only - \"not found\".\n",
    "\n",
    "Refer to the img_text_desc and question below:\n",
    "\n",
    "<img_text_desc>\n",
    "{context}\n",
    "</img_text_desc>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "If the answer to the question is not in the image description, search the actual image provided in \"data\" and retrieve the answer directly from the image provided. Search for the question directly in the image in this case and retrieve the most accurate answer. Do not make up an answer.\n",
    "\n",
    "Assistant: Based on the question and context, if there is an answer, here is my response in 1 sentence. I will ONLY respond with the two words \"not found\" in lower case if no answer to the question is found:\n",
    "\"\"\"\n",
    "\n",
    "direct_text_answer_retrieval_prompt: str = \"\"\"\n",
    "Human: Your role is to give the answer to the question in the <question></question> tags. If the text description in the <text_desc></text_desc> tags does not contain the answer to the question, then respond with two words only - \"not found\".\n",
    "\n",
    "Refer to the text_desc and question below:\n",
    "\n",
    "<text_desc>\n",
    "{context}\n",
    "</text_desc>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "If the answer to the question is not in the text desctipion, then respond with two words only: \"not found\"\n",
    "\n",
    "If the answer to the question is mentioned in the text_desc, then give the answer.\n",
    "\n",
    "Assistant: Based on the question and context, if there is an answer, here is my response in 1 sentence. I will ONLY respond with the two words \"not found\" in lower case if no answer to the question is found:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_index_response(question: str,\n",
    "                       size: int,\n",
    "                       index_clients: List[Tuple[opensearchpy.client.OpenSearch, str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Get LLM responses from retrieved data on questions asked from image, text, or both indexes combined\n",
    "    :param question: Question that a user asks on the content\n",
    "    :param size: 'k' size\n",
    "    :param index_clients: List of tuples containing OpenSearch clients and index names\n",
    "    :Dict: Dictionary with the context used to answer the question and the final response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index_llm_response_and_context: Dict = {'source': ''}\n",
    "        model_id: str = config['bedrock_model_info']['claude_sonnet_model_id']\n",
    "        # Represents the list of extracted text and paths from the most similar hits\n",
    "        all_hits = []\n",
    "        for os_client, index_name in index_clients:\n",
    "            question_entities = get_question_entities(bedrock, question)\n",
    "            print(f\"question entities: {question_entities}\")\n",
    "            # Get the text embedding for the given question\n",
    "            text_embedding = get_text_embedding(bedrock, question)\n",
    "            vector_db_response = find_similar_data_with_string_filter(text_embedding, size, os_client, index_name, question_entities)\n",
    "            if vector_db_response:\n",
    "                hits = vector_db_response.get('hits', {}).get('hits', [])\n",
    "                for hit in hits:\n",
    "                    content_path = hit.get('_source').get('file_path')\n",
    "                    extracted_text = hit.get('_source').get('file_text')\n",
    "                    all_hits.append((content_path, extracted_text, index_name))\n",
    "        for content_path, extracted_text, index_name in all_hits:\n",
    "            file_text: str = \"\"\n",
    "            logger.info(f\"Iterating through all relevant hits to search for an answer....\")\n",
    "\n",
    "            if index_name == outputs['OpenSearchImgIndexName']:\n",
    "                # If the response is from the image index, append to file_text\n",
    "                !aws s3 cp {content_path} .\n",
    "                local_img_path = os.path.basename(content_path)\n",
    "                display(Image(filename=local_img_path))\n",
    "\n",
    "            logger.info(f\"Going to answer the question: \\\"{question}\\\" using the context: \\\"{content_path}\\\"\")\n",
    "\n",
    "            # Now getting a response from the text or image index\n",
    "            if index_name == outputs['OpenSearchImgIndexName']:\n",
    "                # If the response is not given in the extracted text, search the image directly\n",
    "                search_in_img_prompt: str = direct_image_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_response = get_nearest_img_search_response(content_path, search_in_img_prompt, config['bedrock_model_info']['claude_sonnet_model_id'])\n",
    "                sanitized_response = sanitize_llm_response(direct_response)\n",
    "                logger.info(f\"sanitized response: {sanitized_response}\")\n",
    "                logger.info(f\"response from the image index: {direct_response}\")\n",
    "                # If the answer is not contained in the image description, then \n",
    "                # add the llm response to that specific chosen image to the file_text/context.\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response found from the image index\")\n",
    "                    # Update the context sources if the answer is given directly from the image as context to give the final answer\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "            elif index_name == outputs['OpenSearchTextIndexName']:\n",
    "                search_in_txt_prompt: str = direct_text_answer_retrieval_prompt.format(context=extracted_text, question=question)\n",
    "                direct_response = response_from_text_extracted(bedrock, search_in_txt_prompt)\n",
    "                sanitized_response = sanitize_llm_response(direct_response)\n",
    "                logger.info(f\"sanitized response: {sanitized_response}\")\n",
    "                logger.info(f\"response from the text index: {direct_response}\")\n",
    "                # If the answer is not contained in the text description, then \n",
    "                # add the llm response to that specific chosen text to the file_text/context.\n",
    "                if sanitized_response != \"not found\":\n",
    "                    logger.info(f\"Response found from the text index\")\n",
    "                    # Update the context sources if the answer is given directly from the text as context to give the final answer\n",
    "                    file_text += sanitized_response\n",
    "                    index_llm_response_and_context['source'] += file_text\n",
    "\n",
    "        logger.info(f\"Summary provided to the llm: {index_llm_response_and_context['source']}\")\n",
    "        logger.info(f\"question provided to the llm: {question}\")\n",
    "        index_llm_response = get_llm_response(bedrock, question, index_llm_response_and_context['source'], model_id)\n",
    "        index_llm_response_and_context['response'] = index_llm_response\n",
    "        logger.info(f\"response from the llm: {index_llm_response}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not get a response: {e}\")\n",
    "        index_llm_response_and_context['response'] = None\n",
    "        index_llm_response_and_context['source'] = None\n",
    "    return index_llm_response_and_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 - Combined Response (Both Image and Text Indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, get the response from the text index using the text index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question: str = \"<enter your example question here>\"\n",
    "# # index_clients: List[Tuple] = [(img_os_client, img_index_name)]\n",
    "# index_clients: List[Tuple] = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "# get_index_response(question, 1, index_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question: str = \"What is the Cisco recommendation based on the ratings?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name)]\n",
    "index_clients: List[Tuple] = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question: str = \"What was the total Debt/Equity ratio for Amazon in 2022 ?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name)]\n",
    "index_clients: List[Tuple] = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question: str = \"What is the Total Debt/Capital Ratio for Amazon?\"\n",
    "# index_clients: List[Tuple] = [(img_os_client, img_index_name)]\n",
    "index_clients: List[Tuple] = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "get_index_response(question, config['k_count_retrieval'], index_clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Dataset Comparison\n",
    "---\n",
    "\n",
    "In this section of the notebook we do as follows:\n",
    "\n",
    "1. Iterate through each question provided in the dataset, and call the `get_index_response` function\n",
    "\n",
    "1. Record responses from the text response, the image response and combined responses\n",
    "\n",
    "1. Update the df and store the result in the `eval directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_files: Optional[List[str]] = None\n",
    "eval_content = []\n",
    "# if the evaluation dataset is given, get those files and create a dataframe to work with\n",
    "if config['eval_qna_dataset_info']['is_given'] is True:\n",
    "    eval_dir: str = config['eval_qna_dataset_info']['dir_name']\n",
    "    fpath = os.path.join(eval_dir, \"*.csv\")\n",
    "    eval_files = glob.glob(fpath, recursive=True)\n",
    "    for eval_file in eval_files:\n",
    "        logger.info(f\"eval files: {eval_file}\")\n",
    "        eval_df = pd.read_csv(eval_file)\n",
    "        eval_df = eval_df.drop(columns=['Unnamed: 3'])\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if eval_df is not None:\n",
    "    index_clients_both = [(text_os_client, text_index_name), (img_os_client, img_index_name)]\n",
    "    text_index_client = [(text_os_client, text_index_name)]\n",
    "    img_index_client = [(img_os_client, img_index_name)]\n",
    "    for i, question in enumerate(eval_df[config['eval_qna_dataset_info']['question_key']]):\n",
    "        combined_response = get_index_response(question, config['k_count_retrieval'], index_clients_both)\n",
    "        eval_df.at[i, 'combined_response'] = combined_response['response']\n",
    "        logger.info(f\"combined_response['response']: {combined_response['response']}\")\n",
    "        eval_df.at[i, 'image_and_text_source'] = combined_response['source']    \n",
    "        text_response = get_index_response(question, config['k_count_retrieval'], text_index_client)\n",
    "        eval_df.at[i, 'text_response'] = text_response['response']\n",
    "        eval_df.at[i, 'text_source'] = text_response['source']\n",
    "        image_response = get_index_response(question, config['k_count_retrieval'], img_index_client)\n",
    "        eval_df.at[i, 'img_response'] = image_response['response']\n",
    "        eval_df.at[i, 'img_source'] = image_response['source']\n",
    "    print(eval_df.head(10))\n",
    "    metrics_dir = config['metrics_dir']['dir_name']\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    side_view_eval_file = os.path.join(metrics_dir, config['eval_qna_dataset_info']['updated_eval_file'])\n",
    "    eval_df.to_csv(side_view_eval_file, index=True)\n",
    "else:\n",
    "    logger.info(f\"Evaluation dataset not provided. Provide a data set in the eval directory and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
