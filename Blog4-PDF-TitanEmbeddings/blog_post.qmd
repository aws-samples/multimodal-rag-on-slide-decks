---
title: "Talk to your slide deck using multimodal foundation models hosted on Amazon Bedrock and Amazon SageMaker – Part 2"
format:
  html:
    embed-resources: true
    output-file: blog_post.html
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  docx:
    embed-resources: true
    output-file: blog_post.docx
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  gfm: 
    output-file: README.md
---

_Amit Arora_, _Archana Inapudi_, _Manju Prasad_, _Antara Raisa_

In [Part 1](https://aws.amazon.com/blogs/machine-learning/talk-to-your-slide-deck-using-multimodal-foundation-models-hosted-on-amazon-bedrock-and-amazon-sagemaker-part-1/) of this series, we presented a solution that used the [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) model to convert individual slides from a slide deck into embeddings. We stored the embeddings in a vector database and then used the [Large Language-and-Vision Assistant (LLaVA 1.5-7b)](https://llava-vl.github.io/) model to generate text responses to user questions based on the most similar slide retrieved from the vector database. We used AWS services including [Amazon Bedrock](https://aws.amazon.com/bedrock/), [Amazon SageMaker](https://aws.amazon.com/sagemaker/), and [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) in this solution.


In this post, we demonstrate a different approach. We use the [Anthropic Claude 3 Sonnet](https://aws.amazon.com/bedrock/claude/) model to generate text descriptions for each slide in the slide deck. These descriptions are then converted into text embeddings using the [Amazon Titan Text Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) model and stored in a vector database. Then we use the Claude 3 Sonnet model to generate answers to user questions based on the most relevant text description retrieved from the vector database.

You can test both approaches for your dataset and evaluate the results to see which approach gives you the best results. In Part 3 of this series, we evaluate the results of both methods.

#### Solution overview

The solution provides an implementation for answering questions using information contained in text and visual elements of a slide deck. The design relies on the concept of Retrieval Augmented Generation (RAG). Traditionally, RAG has been associated with textual data that can be processed by large language models (LLMs). In this series, we extend RAG to include images as well. This provides a powerful search capability to extract contextually relevant content from visual elements like tables and graphs along with text.

This solution includes the following components:

- Amazon Titan Text Embeddings is a text embeddings model that converts natural language text, including single words, phrases, or even large documents, into numerical representations that can be used to power use cases such as search, personalization, and clustering based on semantic similarity.
- Claude 3 Sonnet is the next generation of state-of-the-art models from Anthropic. Sonnet is a versatile tool that can handle a wide range of tasks, from complex reasoning and analysis to rapid outputs, as well as efficient search and retrieval across vast amounts of information.
- OpenSearch Service Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. We use OpenSearch Serverless as a vector database for storing embeddings generated by the Amazon Titan Multimodal Embeddings model. An index created in the OpenSearch Serverless collection serves as the vector store for our RAG solution. 
- [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) (OSI) is a fully managed, serverless data collector that delivers data to OpenSearch Service domains and OpenSearch Serverless collections. In this post, we use an OSI pipeline API to deliver data to the OpenSearch Serverless vector store.


The solution design consists of two parts: ingestion and user interaction. During ingestion, we process the input slide deck by converting each slide into an image, generating descriptions and text embeddings for each image. We then populate the vector data store with the embeddings and text description for each slide. These steps are completed prior to the user interaction steps.

In the user interaction phase, a question from the user is converted into text embeddings. A similarity search is run on the vector database to find a text description corresponding to a slide that could potentially contain answers to the user question. We then provide the slide description and the user question to the Claude 3 Sonnet model to generate an answer to the query. All the code for this post is available in the [GitHub](https://github.com/aws-samples/multimodal-rag-on-slide-decks/tree/main/Blog2-LVM-TitanEmbeddings) repo.

#### Ingestion steps:
The following diagram illustrates the ingestion architecture.
![Ingestion architecture](images/ML-16123-2-ingestion-design.png)

The workflow consists of the following steps:

1.	Slides are converted to image files (one per slide) in the JPG format and passed to the Claude 3 Sonnet model to generate text description.
2.	The data is sent to the Amazon Titan Text Embeddings model to generate embeddings. In this series, we use the slide deck [Train and deploy Stable Diffusion using AWS Trainium & AWS Inferentia](https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf) from the AWS Summit in Toronto, June 2023 to demonstrate the solution. The sample deck has 31 slides, therefore we generate 31 sets of vector embeddings, each with 1536 dimensions. We add additional metadata fields to perform rich search queries using OpenSearch’s powerful search capabilities.
3.	The embeddings are ingested into an OSI pipeline via an API call. 
4.	The OSI pipeline ingests the data as documents into an OpenSearch Serverless index. The index is configured as the sink for this pipeline and is created as part of the OpenSearch Serverless collection.


#### User interaction steps:
 The following diagram illustrates the user interaction architecture.
![User interaction architecture](images/ML-16123-2-user-interaction-design.png)

The workflow consists of the following steps:

1.	A user submits a question related to the slide deck that has been ingested.
2.	The user input is converted into embeddings using the Amazon Titan Text Embeddings model accessed via Amazon Bedrock. An OpenSearch Service vector search is performed using these embeddings. We perform a k-nearest neighbor (k-NN) search to retrieve the most relevant embeddings matching the user query.
3.	The metadata of the response from OpenSearch Serverless contains a path to the image and description corresponding to the most relevant slide.
4.	A prompt is created by combining the user question and the image description. The prompt is provided to Claude 3 Sonnet hosted on Amazon Bedrock.
5.	The result of this inference is returned to the user.


We discuss the steps for both stages in the following sections, and include details about the output.

#### Prerequisites

To implement the solution provided in this post, you should have an [AWS account](https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&client_id=signup) and familiarity with FMs, Amazon Bedrock, SageMaker, and OpenSearch Service.

This solution uses the Claude 3 Sonnet and Amazon Titan Text Embeddings models hosted on Amazon Bedrock. Make sure that these models are enabled for use by navigating to the **Model access** page on the Amazon Bedrock console. 

If models are enabled, the **Access status** will state **Access granted**.

![Model access](images/ML-16123-2-request-model-access.png)

If the models are not available, enable access by choosing **Manage model access**, selecting the models, and choosing **Request model access**. The models are enabled for use immediately.

#### Use AWS CloudFormation template to create the solution stack

You can use AWS CloudFormation to create the solution stack. If you have created the solution for Part 1 in the same AWS account, be sure to delete that before creating this stack.

|AWS Region                |     Link    |
|:------------------------:|:-----------:|
|us-east-1  | [<img src="./images/ML-16123-2-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=multimodal-blog2-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123-2/template.yml)|
|us-west-2          | [<img src="./images/ML-16123-2-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=multimodal-blog2-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123-2/template.yml) |


After the stack is created successfully, navigate to the stack's **Outputs** tab on the AWS CloudFormation console and note the values for `MultimodalCollectionEndpoint` and `OpenSearchPipelineEndpoint`, You use these it in the subsequent steps.

![CloudFormation stack outputs](images/ML-16123-2-cloudformation-outputs.png)

The CloudFormation template creates the following resources:

- **IAM roles** – The following [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM) roles are created. Update these roles to apply least-privilege permissions, as discussed in  [Security best practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege).
  - `SMExecutionRole` with [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3), SageMaker, OpenSearch Service, and Bedrock full access.
  - `OSPipelineExecutionRole` with access to the S3 bucket and OSI actions.
- **SageMaker notebook**- All code for this post is run via this notebook.
- **OpenSearch Service Serverless collection**– This is the vector database for storing and retrieving embeddings.
- **OSI pipeline**– This is the pipeline for ingesting data into OpenSearch Serverless.
- **S3 bucket**– All data for this post is stored in this bucket.

The CloudFormation template sets up the pipeline configuration required to configure the OSI pipeline with HTTP as source and the OpenSearch Serverless index as sink. The SageMaker notebook `2_data_ingestion.ipynb` displays how to ingest data into the pipeline using the [Requests](https://pypi.org/project/requests/) HTTP library. 

The CloudFormation template also creates [network](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-network.html), [encryption](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-encryption.html) and [data access](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html) policies required for your OpenSearch Serverless collection. Update these policies to apply least-privilege permissions.

The CloudFormation template name and OpenSearch Service index name are referenced in the SageMaker notebook  [`3_rag_inference.ipynb`](./notebooks/3_rag_inference.ipynb). If you change the default names, make sure you update them in the notebook.

#### Testing the solution 
After you have created the CloudFormation stack, you can test the solution. Complete the following steps:

1.	On the SageMaker console, choose **Notebooks** in the navigation pane.
2.	Select `MultimodalNotebookInstance` and choose **Open JupyterLab.**

![SageMaker Notebooks](images/ML-16123-2-open-jl.png)

3.	In **File Browser**, traverse to the notebooks folder to see notebooks and supporting files. 

The notebooks are numbered in the sequence in which they run. Instructions and comments in each notebook describe the actions performed by that notebook. We run these notebooks one by one.

4.	Choose `1_data_prep.ipynb` to open it in JupyterLab. 
5.	On the **Run** menu, choose **Run All Cells** to run the code in this notebook. 

This notebook will download a publicly available [slide deck]((https://d1.awsstatic.com/events/Summits/torsummit2023/CMP301_TrainDeploy_E1_20230607_SPEdited.pdf)), convert each slide into the JPG file format, and upload these to the S3 bucket.

6.	Choose `2_data_ingestion.ipynb` to open it in JupyterLab. 
7.	On the **Run** menu, choose **Run All Cells** to run the code in this notebook. 

In this notebook, you create an index in the OpenSearch Serverless collection. This index stores the embeddings data for the slide deck. See the following code:

  ```{.python}
  session = boto3.Session()
  credentials = session.get_credentials()
  auth = AWSV4SignerAuth(credentials, g.AWS_REGION, g.OS_SERVICE)

  os_client = OpenSearch(
    hosts = [{'host': host, 'port': 443}],
    http_auth = auth,
    use_ssl = True,
    verify_certs = True,
    connection_class = RequestsHttpConnection,
    pool_maxsize = 20
  )

  index_body = """
  {
    "settings": {
      "index.knn": true
    },
    "mappings": {
      "properties": {
        "vector_embedding": {
          "type": "knn_vector",
          "dimension": 1536,
          "method": {
            "name": "hnsw",
            "engine": "nmslib",
            "parameters": {}
          }
        },
        "image_path": {
          "type": "text"
        },
        "slide_text": {
          "type": "text"
        },
        "slide_number": {
          "type": "text"
        },
        "metadata": { 
          "properties" :
            {
              "filename" : {
                "type" : "text"
              },
              "desc":{
                "type": "text"
              }
            }
        }
      }
    }
  }
  """
  index_body = json.loads(index_body)
  try:
    response = os_client.indices.create(index_name, body=index_body)
    logger.info(f"response received for the create index -> {response}")
  except Exception as e:
    logger.error(f"error in creating index={index_name}, exception={e}")
  ```

  You use the Claude 3 Sonnet and Amazon Titan Text Embeddings models to convert the JPG images created in the previous notebook into vector embeddings. These embeddings and additional metadata (such as the S3 path and description of the image file) are stored in the index along with the embeddings. The following code snippet shows how Claude 3 Sonnet generates image descriptions:

  ```{.python}
  def get_img_desc(image_file_path: str, prompt: str):
  # read the file, MAX image size supported is 2048 * 2048 pixels
  with open(image_file_path, "rb") as image_file:
      input_image_b64 = image_file.read().decode('utf-8')

  body = json.dumps(
      {
          "anthropic_version": "bedrock-2023-05-31",
          "max_tokens": 1000,
          "messages": [
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "image",
                          "source": {
                              "type": "base64",
                              "media_type": "image/jpeg",
                              "data": input_image_b64
                          },
                      },
                      {"type": "text", "text": prompt},
                  ],
              }
          ],
      }
  )
  
  response = bedrock.invoke_model(
      modelId=g.CLAUDE_MODEL_ID,
      body=body
  )

  resp_body = json.loads(response['body'].read().decode("utf-8"))
  resp_text = resp_body['content'][0]['text'].replace('"', "'")

  return resp_text
  ```

  The image descriptions are passed to the Amazon Titan Text Embeddings model to generate vector embeddings. These embeddings and additional metadata (such as the S3 path and description of the image file) are stored in the index along with the embeddings. The following code snippet shows the call to the Amazon Titan Text Embeddings model:

  ```{.python}
  def get_text_embedding(bedrock: botocore.client, prompt_data: str) -> np.ndarray:
  body = json.dumps({
      "inputText": prompt_data,
  })    
  try:
      response = bedrock.invoke_model(
          body=body, modelId=g.TITAN_MODEL_ID, accept=g.ACCEPT_ENCODING, contentType=g.CONTENT_ENCODING
      )
      response_body = json.loads(response['body'].read())
      embedding = response_body.get('embedding')
  except Exception as e:
      logger.error(f"exception={e}")
      embedding = None

  return embedding
  ```

  The data is ingested into the OpenSearch Serverless index by making an API call to the OSI pipeline. The following code snippet shows the call made via the Requests HTTP library:

  ```{.python}
  data = json.dumps([{
      "image_path": input_image_s3, 
      "slide_text": resp_text, 
      "slide_number": slide_number, 
      "metadata": {
          "filename": obj_name, 
          "desc": "" 
      }, 
      "vector_embedding": embedding
  }])

  r = requests.request(
      method='POST', 
      url=osi_endpoint, 
      data=data,
      auth=AWSSigV4('osis'))
  ```
8.	Choose `3_rag_inference.ipynb` to open it in JupyterLab. 
9.	On the **Run** menu, choose **Run All Cells** to run the code in this notebook. 


This notebook implements the RAG solution: you convert the user question into embeddings, find a similar image description from the vector database, and provide the retrieved description to Claude 3 Sonnet to generate an answer to the user question. You use the following prompt template:

  ```{.python}
    llm_prompt: str = """

    Human: Use the summary to provide a concise answer to the question to the best of your abilities. If you cannot answer the question from the context then say I do not know, do not make up an answer.
    <question>
    {question}
    </question>

    <summary>
    {summary}
    </summary>

    Assistant:"""
  ```

  The following code snippet provides the RAG workflow:

  ```{.python}
  def get_llm_response(bedrock: botocore.client, question: str, summary: str) -> str:
    prompt = llm_prompt.format(question=question, summary=summary)
    
    body = json.dumps(
    {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1000,
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                ],
            }
        ],
    })
        
    try:
        response = bedrock.invoke_model(
        modelId=g.CLAUDE_MODEL_ID,
        body=body)

        response_body = json.loads(response['body'].read().decode("utf-8"))
        llm_response = response_body['content'][0]['text'].replace('"', "'")
        
    except Exception as e:
        logger.error(f"exception while slide_text={summary[:10]}, exception={e}")
        llm_response = None

    return llm_response

  # create prompt and convert to embeddings
  question: str = "How does Inf2 compare in performance to comparable EC2 instances? I need numbers."
  text_embedding = get_text_embedding(bedrock, question)

  # vector db search
  vector_db_response: Dict = find_similar_data(text_embeddings)

  # download image for local notebook display
  s3_img_path = vector_db_response.get('hits', {}).get('hits')[0].get('_source').get('image_path')
  logger.info(f"going to answer the question=\"{question}\" using the image \"{s3_img_path}\"")

  !aws s3 cp {s3_img_path} .
  local_img_path = os.path.basename(s3_img_path)
  display(filename=local_img_path) 

  # Ask Claude 3 Sonnet
  slide_text = vector_db_response.get('hits', {}).get('hits')[0].get('_source').get('slide_text')

  llm_response = get_llm_response(bedrock, question, slide_text)
  print(llm_response)
  ```

#### Results 
The following table contains some user questions and responses generated by our implementation. The **Question** column captures the user question, and the **Answer** column is the textual response generated by Claude 3 Sonnet. The **Image** column shows the k-NN slide match returned by the OpenSearch Serverless vector search. This slide is interpreted by Claude 3 Sonnet to provide the answer.

|Question                |     Answer    |  Image |
|------------------------|-----------|-----------|
|How does Inf2 compare in performance to comparable EC2 instances? I need numbers.  |Based on the information provided in the summary, the key performance comparison numbers given for the Amazon EC2 Inf2 instances compared to other instances are: 1\. Up to 4x higher throughput. 2\. Up to 10x lower latency. The summary does not provide specific numbers comparing the Inf2 instances to other EC2 instance types. However, it highlights the throughput and latency improvements as the main performance advantages of the Inf2 instances over comparable instances for running generative AI models.| ![](./images/ML-16123-2-slide-1.png)|
|As per the AI/ML flywheel, what do the AWS AI/ML services provide?  |Based on the summary, the AWS AI/ML services provide a range of capabilities that fuel an AI/ML flywheel. This flywheel consists of innovation in developing new and better algorithms, enabling more use-cases and applications, driving wider adoption and demand, which in turn leads to further investment in research and optimization to drive more innovation. The AWS AI/ML services seem to offer the tools, resources, and infrastructure to support this continuous cycle of innovation, application development, adoption, and reinvestment in the field of artificial intelligence and machine learning.| ![](./images/ML-16123-2-slide-2.png)|
|Compared to GPT-2, how many more parameters does GPT-3 have? What is the numerical difference between the parameter size of GPT-2 and GPT-3? |According to the information provided in the summary, GPT-3 from 2020 had 175B (175 billion) parameters, while GPT-2 from 2019 had 1.5B (1.5 billion) parameters. Therefore, the numerical difference between the parameter size of GPT-2 and GPT-3 is 173.5 billion parameters. GPT-3 has approximately 115 times more parameters than GPT-2.| ![](./images/ML-16123-2-slide-3.png)|
|What are quarks in particle physics?	|Unfortunately, the given summary does not contain any information about quarks in particle physics. The summary describes an image related to the progression of natural language processing and generative AI technologies, but it does not mention anything about particle physics or the concept of quarks.| ![](./images/ML-16123-2-slide-4.png)|

: Multimodal RAG results {tbl-colwidths="[25,25,50]"}

#### Query your index

You can use OpenSearch Dashboards to interact with the OpenSearch API to run quick tests on your index and ingested data.

![OpenSearch dashboard GET example](images/ML-os-1.png)
 
#### Cleanup

To avoid incurring future charges, delete the resources. You can do this by deleting the stack via the AWS CloudFormation console.

![Delete CloudFormation Stack](images/ML-16123-2-cloudformation-delete-stack.png){#fig-delete-cft} 

#### Conclusion

Enterprises generate new content all the time, and slide decks are a common way to share and disseminate information internally within the organization and externally with customers or at conferences. Over time, rich information can remain buried and hidden in non-text modalities like graphs and tables in these slide decks.

You can use this solution and the power of multimodal FMs such as the Amazon Titan Text Embeddings and Claude 3 Sonnet models to discover new information or uncover new perspectives on content in slide decks. You can try different Claude models available on Amazon Bedrock by updating the `CLAUDE_MODEL_ID` in the `globals.py` file.

This is Part 2 of a three-part series. We used the Amazon Titan Multimodal Embeddings and LLaVA models in Part 1. In Part 3, we will compare the approaches from Part 1 and Part 2.


Portions of this code are released under the [Apache 2.0 License](https://aws.amazon.com/apache-2-0/)

 *  *  *  *  *

## Author bio

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Amit.jpg"> <b>Amit Arora</b> is an AI and ML Specialist Architect at Amazon Web Services, helping enterprise customers use cloud-based machine learning services to rapidly scale their innovations. He is also an adjunct lecturer in the MS data science and analytics program at Georgetown University in Washington D.C.
<br><br>

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Manju.jpg"> <b>Manju Prasad</b> is a Senior Solutions Architect at Amazon Web Services. She focuses on providing technical guidance in a variety of technical domains, including AI/ML. Prior to joining AWS, she designed and built solutions for companies in the financial services sector and also for a startup. She has worked in all layers of the software stack, ranging from webdev to databases, and has experience in all levels of the software development lifecycle. She is passionate about sharing knowledge and fostering interest in emerging talent.
<br><br>

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Archana.jpg"> <b>Archana Inapudi</b> is a Senior Solutions Architect at AWS, supporting a strategic customer. She has over a decade of cross-industry expertise leading strategic technical initiatives. Archana is an aspiring member of the AI/ML technical field community at AWS. Prior to joining AWS, Archana led a migration from traditional siloed data sources to Hadoop at a healthcare company. She is passionate about using technology to accelerate growth, provide value to customers, and achieve business outcomes.
<br><br>

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Antara.jpg"> <b>Antara Raisa</b> is an AI and ML Solutions Architect at Amazon Web Services, supporting strategic customers based out of Dallas, Texas. She also has previous experience working with large enterprise partners at AWS, where she worked as a Partner Success Solutions Architect for digital-centered customers.


