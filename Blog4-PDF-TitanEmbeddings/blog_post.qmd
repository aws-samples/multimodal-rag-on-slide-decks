---
title: "Talk to your PDF files (Enhanced Multimodal RAG) using foundation models (FMs) hosted on Amazon Bedrock – Part 4"
format:
  html:
    embed-resources: true
    output-file: blog_post.html
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  docx:
    embed-resources: true
    output-file: blog_post.docx
    theme: cosmo
    code-copy: true
    code-line-numbers: true
    highlight-style: github
  gfm: 
    output-file: README.md
---

_Madhur Prashant_, _Amit Arora_, _Archana Inapudi_, _Manju Prasad_

For readers that followed us through our blog series, the first 3 parts of the blog focused on using slide decks as the data source.  In this blog, Part 4, we extend the RAG solution to PDFs and also talk about strategies for performing hybrid searches, optimizing search results, and evaluating quality of responses to determine the approach best suited for your use case

PDFs are one of the most commonly used data formats in the industry. They are versatile with document sharing, distribution, print-ready, secure sharing, and form features, making them an indispensable tool across a wide range of industries, from finance and healthcare to manufacturing and creative services. 

This blog walks readers through ingestion of PDFs into a data repository and demonstrates search capabilities on content like text, images, graph, charts, etc. embedded in that PDF. It introduces readers to strategies you can adopt to improve search results.


#### Solution Overview

The solution includes the following components:
- Amazon Titan Text Embeddings is a text embeddings model that converts natural language text, including single words, phrases, or even large documents, into numerical representations that can be used to power use cases such as search, personalization, and clustering based on semantic similarity. (add link) 

- [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) model: this FM is used to generate embeddings for the content in the slide deck used in this blog. As a multimodal model, this Titan model can process text, image or a combination as input and generate 
embeddings. The Titan Multimodal Embeddings model generates vectors (embeddings) of dimension 1024 and is accessed via the Amazon Bedrock service.

- [Amazon OpenSearch Service Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html): OpenSearch Service Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. We use OpenSearch Service Serverless as a vector database for storing embeddings generated by the Titan Multimodal Embeddings model. An index created in the OpenSearch Service Serverless collection serves as the vector store for our RAG solution. 

- [Amazon OpenSearch Ingestion](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/ingestion.html) (OSI): OSI is a fully managed, serverless data collector that delivers data to Amazon OpenSearch Service domains and OpenSearch Serverless collections. In this blog, we are using an OSI pipeline to deliver data to the OpenSearch Serverless vector store. 

- Claude 3 Sonnet is the next generation of state-of-the-art models from Anthropic. Sonnet is a versatile tool that can handle a wide range of tasks, from complex reasoning and analysis to rapid outputs, as well as efficient search and retrieval across vast amounts of information, including visual elements like tables and charts. (correct model and add link) 

In addition to the above components, we have made use of the following open source projects to enhance the developer experience and performance of the solution:
- [LiteLLM](https://github.com/BerriAI/litellm) for a standardized interface to call inference APIs in Bedrock 
- [Ray](https://github.com/ray-project/ray) for concurrent and asynchronous execution of Bedrock inference calls.
- [NLTK](https://github.com/nltk/nltk) for entity extraction to support pre-filtering for hybrid searches. 


As with any RAG solution, this solution consists of two steps: Ingestion of data to build the data store and User Interaction to return responses from the data store to user queries. We will dive into details of these steps below. The repo for this solution can be found on [GitHub](https://github.com/aws-samples/multimodal-rag-on-slide-decks/tree/main/Blog4-PDF-TitanEmbeddings).*Hint* The repo is parameterized for includes config*.yml files to manage inputs.    

#### Ingestion steps:
![](images/pdf_Ingestion.drawio.png)

1.	Data Preparation beings with content extract. For each PDF page, we perform two steps: 1. Extract all text from the page and 2. Convert the page to an JPG image file. Both outputs are stored in S3. *Hint* The image file is saved at 3x scale to improve model inference. Check config_full.yml for parameters to control 2-way horizontal and vertical split of images to further improve model inference. 

2.  Data ingestion for extracted text consists of: 1. Use NLTK to retrieve entities from extracted text, 2. Use Titan Text embeddings model to generate embeddings for extracted text, 3. Create a JSON file with generated embeddings, retrieved entities and other pertinent metadata fields like page number 4. Ingest the JSON file to an OpenSearch Serverless index via OpenSearch Ingestion service.  *Hint* NLTK is used to extract entities from text to support hybrid search during User Interaction. *Hint* Additional metadata fields added to the index along with generated vector embeddings, can be used to perform rich search queries using OpenSearch’s powerful search capabilities.

3.  Data Ingestion for JPG image file (one per PDF page) consists of: 1. Pass JPG file to Claude Sonnet for inference of visual content, 2. Using a different prompt with Claude Sonnet, extract entities from inference results, 3. Use Titan Text embeddings model to generate embeddings for JPG inference, 4. Create a JSON file with generated embeddings, extracted entities and other pertinent metadata fields like page number 4. Ingest the JSON file to an OpenSearch Serverless index via OpenSearch Ingestion service.  *Hint* Entities info is generated to support hybrid search during User Interaction. *Hint* Additional metadata fields added to the index along with generated vector embeddings, can be used to perform rich search queries using OpenSearch’s powerful search capabilities.  


#### User Interaction steps:
![](images/architecture_diagram.jpg)

To fix - extract entities from user question, match min 2 and return all that match.  If none match, return none. Current knn is 4
*hint* entity match count is configurable. knn is configurable 

1.	A user submits a question related to the PDF that has been ingested.
2.	The user input is converted into embeddings using the Amazon Titan Text Embeddings model accessed via Amazon Bedrock. 
3.  An OpenSearch Service vector search is performed using these embeddings. We perform a k-nearest neighbor (k-NN) search to retrieve the most relevant embeddings matching the user query. (add the ladder logic) *Hint* Search both vector stores for (cost vs accuracy)
4.	Responses from OpenSearch search are synthesized by Claude Sonnet and result returned to the user. 


#### Prerequisites

To implement the solution provided in this post, you should have an [AWS account](https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fportal.aws.amazon.com%2Fbilling%2Fsignup%2Fresume&client_id=signup) and familiarity with FMs, Amazon Bedrock, and OpenSearch Service.

This solution uses the Claude 3 Sonnet and Amazon Titan Text Embeddings models hosted on Amazon Bedrock. Make sure that these models are enabled for use by navigating to the **Model access** page on the Amazon Bedrock console. 

If models are enabled, the **Access status** will state **Access granted**.


#### Use AWS CloudFormation template to create the solution stack

You can use AWS CloudFormation to create the solution stack. If you have created the solution for Part 1 in the same AWS account, be sure to delete that before creating this stack.

|AWS Region                |     Link    |
|:------------------------:|:-----------:|
|us-east-1  | [<img src="./images/ML-16123-2-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=multimodal-blog2-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123-2/template.yml)|
|us-west-2          | [<img src="./images/ML-16123-2-cloudformation-launch-stack.png">](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=multimodal-blog2-stack&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-16123-2/template.yml) |


After the stack is created successfully, navigate to the stack's **Outputs** tab on the AWS CloudFormation console and note the values for `MultimodalCollectionEndpoint` and `OpenSearchPipelineEndpoint`, You use these it in the subsequent steps.


- **IAM roles** – The following [AWS Identity and Access Management](https://aws.amazon.com/iam/) (IAM) roles are created. Update these roles to apply least-privilege permissions, as discussed in  [Security best practices](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#grant-least-privilege).
  - `SMExecutionRole` with [Amazon Simple Storage Service](http://aws.amazon.com/s3) (Amazon S3), SageMaker, OpenSearch Service, and Bedrock full access.
  - `OSPipelineExecutionRole` with access to the S3 bucket and OSI actions.
- **SageMaker notebook**- All code for this post is run via this notebook.
- **OpenSearch Service Serverless collection**– This is the vector database for storing and retrieving embeddings.
- **OSI pipeline**– This is the pipeline for ingesting data into OpenSearch Serverless.
- **S3 bucket**– All data for this post is stored in this bucket.

The CloudFormation template sets up the pipeline configuration required to configure the OSI pipeline with HTTP as source and the OpenSearch Serverless index as sink. The SageMaker notebook `2_data_ingestion.ipynb` displays how to ingest data into the pipeline using the [Requests](https://pypi.org/project/requests/) HTTP library. 

The CloudFormation template also creates [network](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-network.html), [encryption](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-encryption.html) and [data access](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html) policies required for your OpenSearch Serverless collection. Update these policies to apply least-privilege permissions.

The CloudFormation template name and OpenSearch Service index name are referenced in the SageMaker notebook  [`3_rag_inference.ipynb`](./notebooks/3_rag_inference.ipynb). If you change the default names, make sure you update them in the notebook.


#### Testing the solution 
After you have created the CloudFormation stack, you can test the solution. Complete the following steps:

1.	On the SageMaker console, choose **Notebooks** in the navigation pane.
2.	Select `MultimodalNotebookInstance` and choose **Open JupyterLab.**

![SageMaker Notebooks](images/ML-16123-2-open-jl.png)

3.	In **File Browser**, traverse to the notebooks folder to see notebooks and supporting files. 

The notebooks are numbered in the sequence in which they run. Instructions and comments in each notebook describe the actions performed by that notebook. Run these notebooks in sequence.


#### Results 
The following table contains some user questions and responses generated by our implementation. The **Question** column captures the user question, and the **Answer** column is the textual response generated by Claude 3 Sonnet. The **Image** column shows the k-NN slide match returned by the OpenSearch Serverless vector search. This slide is interpreted by Claude 3 Sonnet to provide the answer.

|Question                |     Answer    |  Image |
|------------------------|-----------|-----------|
|How does Inf2 compare in performance to comparable EC2 instances? I need numbers.  |Based on the information provided in the summary, the key performance comparison numbers given for the Amazon EC2 Inf2 instances compared to other instances are: 1\. Up to 4x higher throughput. 2\. Up to 10x lower latency. The summary does not provide specific numbers comparing the Inf2 instances to other EC2 instance types. However, it highlights the throughput and latency improvements as the main performance advantages of the Inf2 instances over comparable instances for running generative AI models.| ![](./images/ML-16123-2-slide-1.png)|
|As per the AI/ML flywheel, what do the AWS AI/ML services provide?  |Based on the summary, the AWS AI/ML services provide a range of capabilities that fuel an AI/ML flywheel. This flywheel consists of innovation in developing new and better algorithms, enabling more use-cases and applications, driving wider adoption and demand, which in turn leads to further investment in research and optimization to drive more innovation. The AWS AI/ML services seem to offer the tools, resources, and infrastructure to support this continuous cycle of innovation, application development, adoption, and reinvestment in the field of artificial intelligence and machine learning.| ![](./images/ML-16123-2-slide-2.png)|
|Compared to GPT-2, how many more parameters does GPT-3 have? What is the numerical difference between the parameter size of GPT-2 and GPT-3? |According to the information provided in the summary, GPT-3 from 2020 had 175B (175 billion) parameters, while GPT-2 from 2019 had 1.5B (1.5 billion) parameters. Therefore, the numerical difference between the parameter size of GPT-2 and GPT-3 is 173.5 billion parameters. GPT-3 has approximately 115 times more parameters than GPT-2.| ![](./images/ML-16123-2-slide-3.png)|
|What are quarks in particle physics?	|Unfortunately, the given summary does not contain any information about quarks in particle physics. The summary describes an image related to the progression of natural language processing and generative AI technologies, but it does not mention anything about particle physics or the concept of quarks.| ![](./images/ML-16123-2-slide-4.png)|

: Multimodal RAG results {tbl-colwidths="[25,25,50]"}


#### Enhancements to RAG 
* Entity extraction - This post uses NLTK and Claude to extract entities information from PDF. This info is stored with emebeddings of the PDF text and images. Our tests have shown improved search results when prefiltering search results using entities (need info) 
* Evaluation - options for cost vs best results 
* Ladder search of top hits (needs diagram) - 

* Parameterization: The code repo is highly parameterized using config files. This allows users to adapt the solution to their setup and requirements. Inputs such as inference model, input PDF file, split strategy and scaling values for PDFs, among others are configurable values and require no code modification. After an update, just re-run the notebooks. *Hint* See config.yml for commonly used inputs, and config-full.yml for the full list 


#### Cleanup

To avoid incurring future charges, delete the resources. You can do this by deleting the stack via the AWS CloudFormation console.

![Delete CloudFormation Stack](images/ML-16123-2-cloudformation-delete-stack.png){#fig-delete-cft} 

#### Conclusion

Try out this solution by extending it to your own PDFs. Use the enhancements available to adjust the implementation. Cost conscious users may choose to use only one vector store and limit searches to that vector store. Users chasing search accuracy may consider adding a 3rd index with embeddings of the PDF JPGs (generated using Titan Multimodal Embeddings model) and explore ways to improve results. Consider adding a Streamlit UI (tbcompleted)

Let us know what you think about this solution. 

Portions of this code are released under the [Apache 2.0 License](https://aws.amazon.com/apache-2-0/)



 *  *  *  *  *

## Author bio (to fix) x

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Amit.jpg"> <b>Amit Arora</b> is an AI and ML Specialist Architect at Amazon Web Services, helping enterprise customers use cloud-based machine learning services to rapidly scale their innovations. He is also an adjunct lecturer in the MS data science and analytics program at Georgetown University in Washington D.C.
<br><br>

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Manju.jpg"> <b>Manju Prasad</b> is a Senior Solutions Architect at Amazon Web Services. She focuses on providing technical guidance in a variety of technical domains, including AI/ML. Prior to joining AWS, she designed and built solutions for companies in the financial services sector and also for a startup. She has worked in all layers of the software stack, ranging from webdev to databases, and has experience in all levels of the software development lifecycle. She is passionate about sharing knowledge and fostering interest in emerging talent.
<br><br>

<img style="float: left; margin: 0 10px 0 0;" src="images/ML-16123-Archana.jpg"> <b>Archana Inapudi</b> is a Senior Solutions Architect at AWS, supporting a strategic customer. She has over a decade of cross-industry expertise leading strategic technical initiatives. Archana is an aspiring member of the AI/ML technical field community at AWS. Prior to joining AWS, Archana led a migration from traditional siloed data sources to Hadoop at a healthcare company. She is passionate about using technology to accelerate growth, provide value to customers, and achieve business outcomes.
<br><br>






store it in S3. Next, we convert the page to an image file and store s, all images and  PDF content is extracted in 3 ways. All textual content is extracted and stored in S3. Each PDF page is exported as 
(find out from Madhur - if this is correct
for each page in the pdf{
 -extract text, saving it to a file on disk
 -create jpg of page, saving it to a file on disk (5 images for every page ... 2 hori, 2 vert and 1 full -- parameterized in config_full.yml) default behavior 3x of 1 full image
}
If yes, how many folders?  2 folders (extract_data/text and extract_data/imgs folder  and says in path the hori/vert/full, scale whole image 3x then it crops hori and vert)
when is the scale (zoom in) done? and vertical or hori split?
when is it written to S3? (saved to S3 after prep notebook, mm/imgs/text folders in S3), in second nb, retrieving from S3 & conversion to B64 & send to embed model,  
)
All extracted content is stored in S3
extract entties, describe img, store in embeddings and JSON (which has file type, page num, entities ext, img desc and file name) (This is in ingestion notebook)

2.  Why are embedding models not called as soon as content is extracted from PDF?  Is there a cost saving move here?
Text data is passed to the Titan Text embeddings model.  Generated embeddings 
Slides are converted to image files (one per slide) in the JPG format and passed to the Claude 3 Sonnet model to generate text description
